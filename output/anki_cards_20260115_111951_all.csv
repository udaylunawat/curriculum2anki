What is EDA,"Exploratory Data Analysis (EDA) is the process of analyzing data to understand its structure, patterns, and relationships using statistics, linear algebra, and plotting tools.<br><br><b>ELI5:</b> Like a detective examining clues to understand a mystery, EDA helps uncover insights in data.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is the IRIS dataset,"The IRIS dataset is a classic dataset in data science, collected in 1936, used to classify flowers into 3 species based on 4 features: Sepal Length, Sepal Width, Petal Length, and Petal Width.<br><br><b>ELI5:</b> Think of it as a plant catalog where each entry describes a flower's measurements to identify its type.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does a 2D scatter plot work in EDA,"A 2D scatter plot visualizes the relationship between two features (e.g., Sepal Length vs. Sepal Width) by plotting data points on a 2D plane. It helps identify patterns, clusters, or separability between classes.<br><br><b>ELI5:</b> Like plotting dots on a graph to see if certain flower types group together based on their measurements.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does a pair plot extend visualization,"A pair plot creates a grid of scatter plots for all pairs of features, allowing visualization of up to 6D or 10D data. It helps identify correlations and separability across multiple dimensions.<br><br><b>ELI5:</b> Like looking at every possible combination of flower measurements side-by-side to spot trends.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does class balance affect EDA,"Class balance, checked via .value_counts(), reveals if one class dominates the dataset. Imbalance can skew analysis and model performance.<br><br><b>ELI5:</b> If one flower type has way more samples than others, your analysis might favor it, like a biased referee.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does dimensionality limit visualization,"Visualizing beyond 3D requires tools like pair plots or dimensionality reduction (e.g., PCA) because human perception is limited to 3D space.<br><br><b>ELI5:</b> Trying to visualize 4D data is like trying to draw a 4D cube on paper—you need tricks to flatten it.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does hue in plots improve EDA,"Using hue (e.g., seaborn's hue='species') colors data points by class, making separability and patterns between classes visually apparent.<br><br><b>ELI5:</b> Like coloring each flower type differently on a graph to see which ones cluster together.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a pair plot,"A pair plot is a matrix of scatter plots showing pairwise relationships between variables. It is useful for low-dimensional data (typically < 10 features).<br><br><b>ELI5:</b> Imagine a grid where each cell shows how two variables interact, like plotting height vs weight for every possible pair of measurements.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a histogram,A histogram is a bar plot representing the frequency distribution of a single variable. It divides the data into bins and counts observations in each bin.<br><br><b>ELI5:</b> Like sorting marbles into buckets by size and counting how many fit in each bucket.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a PDF,"A Probability Density Function (PDF) is a smoothed version of a histogram using Kernel Density Estimation (KDE). It estimates the underlying probability distribution of a variable.<br><br><b>ELI5:</b> Instead of rigid buckets, imagine a smooth curve that shows where data points are most likely to appear.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does KDE smooth a histogram,"Kernel Density Estimation (KDE) replaces each data point with a small smooth kernel (e.g., Gaussian) and sums them to create a continuous curve. The bandwidth parameter controls smoothness.<br><br><b>ELI5:</b> Like blurring a pixelated image to see the bigger picture without jagged edges.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does bandwidth affect KDE,"A small bandwidth makes the PDF more rigid (high variance, noisy), while a large bandwidth makes it smoother (high bias, oversimplified).<br><br><b>ELI5:</b> Like adjusting the focus on a camera: too sharp shows noise, too blurry loses details.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does univariate analysis use PDFs,"Univariate analysis uses PDFs to visualize the distribution of individual variables, often when the number of features is small (e.g., ≤ 4).<br><br><b>ELI5:</b> Like examining one ingredient at a time to understand its role in a recipe.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is CDF,CDF (Cumulative Distribution Function) is the percentage of data points below a given x value. It is obtained by integrating the PDF (Probability Density Function).<br><br><b>ELI5:</b> Imagine stacking all data points from left to right; CDF tells you how much of the stack is below a certain height.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is PDF,"PDF (Probability Density Function) is the percentage of data points between two x values. It is obtained by differentiating the CDF.<br><br><b>ELI5:</b> PDF shows how densely packed data points are in a specific range, like how many people are in a certain height range.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does CDF relate to PDF,"CDF is the integral of PDF, and PDF is the derivative of CDF. CDF gives cumulative probabilities, while PDF gives probabilities over intervals.<br><br><b>ELI5:</b> CDF is like a running total of data points, while PDF is like the speed at which data points accumulate.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is mean,"Mean is the sum of data values divided by the number of values. It represents the central value but is highly sensitive to outliers.<br><br><b>ELI5:</b> Mean is the average value, but if one value is extremely high or low, it can pull the mean away from the center.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is variance,"Variance is the average of the squared distances of data points from the mean. It measures the spread of data.<br><br><b>ELI5:</b> Variance tells you how far apart the data points are from the average, like how spread out a group of people are from the center of a room.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is standard deviation,"Standard deviation is the square root of variance. It measures the width of the spread of data values on an axis.<br><br><b>ELI5:</b> Standard deviation tells you how much the data points deviate from the average, like how much the heights of people vary from the average height.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does median differ from mean,"Median is the middle value of a sorted list and is not affected by outliers, making it a better measure of central tendency when outliers are present.<br><br><b>ELI5:</b> Median is like the middle person in a line of people sorted by height, while mean is the average height, which can be skewed by a very tall or short person.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a percentile,"A percentile is the value below which a given percentage of observations fall. For example, the 50th percentile is the value at which 50% of the data is less than this value.<br><br><b>ELI5:</b> Imagine 100 people lined up by height. The 50th percentile is the height of the person in the middle.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What are quantiles,"Quantiles are specific percentiles that divide the data into equal parts. Common quantiles are the 25th, 50th, and 75th percentiles.<br><br><b>ELI5:</b> Think of dividing a pizza into 4 equal slices. The 25th, 50th, and 75th percentiles are like the cuts that divide the pizza into quarters.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is the Inter Quartile Range (IQR),"The IQR is the range between the 25th percentile (Q1) and the 75th percentile (Q3). It measures the spread of the middle 50% of the data.<br><br><b>ELI5:</b> If you have 100 people lined up by height, the IQR is the height difference between the 25th and 75th person in line.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is the Median Absolute Deviation (MAD),"MAD is a measure of variability calculated as the median of the absolute deviations from the data's median. It is robust to outliers.<br><br><b>ELI5:</b> Imagine the median height of a group. MAD tells you how much, on average, people's heights differ from this median height.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a box plot,"A box plot is a visualization of the five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It shows the distribution of data and highlights outliers.<br><br><b>ELI5:</b> Think of a box plot as a summary of a group's heights. The box shows where most people's heights fall, and the whiskers show the range of the shortest and tallest people.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is a violin plot,"A violin plot combines a box plot with a rotated kernel density plot on each side, showing the probability density of the data at different values.<br><br><b>ELI5:</b> Imagine a box plot with a 'mirrored' density curve on each side, showing how data is distributed.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does a violin plot differ from a box plot,"A violin plot adds a kernel density estimate to a box plot, showing the full distribution shape, not just summary statistics.<br><br><b>ELI5:</b> A box plot is a skeleton; a violin plot adds 'muscle' by showing the full shape of the data.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is univariate analysis,"Univariate analysis examines one variable at a time using tools like PDF, CDF, box plots, or violin plots.<br><br><b>ELI5:</b> Studying one thing (e.g., height) without comparing it to anything else.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is bivariate analysis,"Bivariate analysis explores relationships between two variables using pair plots or scatter plots.<br><br><b>ELI5:</b> Comparing two things (e.g., height vs. weight) to see if they relate.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is multivariate analysis,"Multivariate analysis examines relationships among three or more variables, often using 3D plots.<br><br><b>ELI5:</b> Studying multiple factors (e.g., height, weight, age) together to find patterns.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What does a contour plot show,"A contour plot visualizes multivariate probability density, with darker regions indicating higher density, like hills on a map.<br><br><b>ELI5:</b> A topographic map where 'peaks' show where data points cluster most densely.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is the projection of a onto b,The projection of vector a onto vector b is given by the scalar d = a·b / ||b||. This represents the length of the shadow of a along b.<br><br><b>ELI5:</b> Imagine shining a light perpendicular to b; the projection is the length of a's shadow on b.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a unit vector,"A unit vector â is a vector in the same direction as a with a magnitude of 1, defined as â = a / ||a||.<br><br><b>ELI5:</b> Think of it as shrinking or stretching a vector until its length is exactly 1, like a compass needle pointing in the same direction.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How is a hyperplane defined in n dimensions,"A hyperplane in n dimensions is defined by the equation Wᵀx + W₀ = 0, where W is a normal vector and W₀ is the intercept. For a hyperplane passing through the origin, W₀ = 0.<br><br><b>ELI5:</b> It's like a flat sheet of paper in higher dimensions, dividing space into two halves.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does the normal vector relate to a hyperplane,"The normal vector W is perpendicular to the hyperplane. The angle θ between W and any vector x on the hyperplane is 90°, so Wᵀx = 0.<br><br><b>ELI5:</b> The normal vector is like a pole sticking straight out of the hyperplane, pointing in the direction of the 'steepest climb.'",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How is the distance of a point from a hyperplane calculated,"The distance d of a point P from the hyperplane Wᵀx = 0 is given by d = |WᵀP| / ||W||. This measures the shortest distance from P to the hyperplane.<br><br><b>ELI5:</b> It's like measuring how far a point is from a wall, using the perpendicular distance.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does a hyperplane divide space,"A hyperplane divides the space into two half-spaces. If WᵀP > 0, the point P lies in the direction of W; otherwise, it lies in the opposite direction.<br><br><b>ELI5:</b> Think of a hyperplane as a fence; points on one side are in the direction the fence is facing, and points on the other side are behind it.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a hypersphere,"A hypersphere is the set of all points in n-dimensional space that are at a fixed distance (radius) from a central point. The boundary is defined by the sum of squared coordinates equaling the squared radius.
\[
\sum_{i=1}^{n} x_i^2 = r^2
\]
Here, \(x_i\) are the coordinates and \(r\) is the radius.
<br><br><b>ELI5:</b> Like a balloon expanding in any number of dimensions; every point on the surface is exactly one stretch length away from the center.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does a point relate to a hypersphere,"The position of a point relative to a hypersphere is determined by comparing the sum of its squared coordinates to the squared radius.
\[
\sum_{i=1}^{n} x_i^2 \begin{cases} < r^2 & \text{inside} \\ = r^2 & \text{on boundary} \\ > r^2 & \text{outside} \end{cases}
\]
This calculates the squared Euclidean distance from the origin.
<br><br><b>ELI5:</b> If the point's total squared distance is less than the radius squared, it's inside the balloon.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a hyperellipsoid,"A hyperellipsoid is the n-dimensional generalization of an ellipse. It is defined by the sum of squared coordinates scaled by different radii equaling 1.
\[
\sum_{i=1}^{n} \left( \frac{x_i}{a_i} \right)^2 = 1
\]
The parameters \(a_i\) represent the semi-axis lengths along each dimension.
<br><br><b>ELI5:</b> A sphere that has been stretched or squashed by different amounts along each axis.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does scaling affect a hyperellipsoid,"The parameters \(a_i\) control the shape by scaling the axes. A larger \(a_i\) stretches the shape along that dimension, while a smaller \(a_i\) compresses it. If all \(a_i\) are equal, the hyperellipsoid becomes a hypersphere.
<br><br><b>ELI5:</b> Pulling on rubber bands in different directions to change the shape of a blob.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What defines an axis parallel rectangle,"A point \(P(x_1, y_1)\) lies inside an axis parallel rectangle defined by corners \((a_1, b_1)\) and \((a_2, b_2)\) if it satisfies two inequalities:
\[
a_1 < x_1 < a_2 \quad \text{and} \quad b_1 < y_1 < b_2
\]
A square is a special case where the side lengths are equal: \(a_2 - a_1 = b_2 - b_1\).
<br><br><b>ELI5:</b> The point is inside if its x-coordinate is between the left and right edges, and its y-coordinate is between the bottom and top edges.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a random variable,"A variable that can take multiple values in a random manner. It maps outcomes of a random process to numerical values.<br><br>\[ X \in \{\text{outcomes}\} \]<br><br><b>ELI5:</b> Like rolling a die: the result is random, but we can assign numbers to it.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a discrete random variable,"A random variable that can only take specific values from a finite or countable set.<br><br>Example: Dice roll \( X \in \{1, 2, 3, 4, 5, 6\} \)<br><br><b>ELI5:</b> Like counting apples—you can have 1, 2, or 3 apples, but not 2.7 apples.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a continuous random variable,"A random variable that can take any value within a continuous range of real numbers.<br><br>Example: Height \( Y \in [120, 190] \) cm<br><br><b>ELI5:</b> Like measuring height—you can be 150.5 cm or 150.6 cm, infinite possibilities.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is an outlier,"Data points that deviate significantly from the general trend of the population, potentially due to measurement error or genuine anomalies.<br><br>Example: Heights \( \{122.2, 146.4, 12.26, 92.6\} \) where 12.26 and 92.6 are outliers<br><br><b>ELI5:</b> Like finding a 3-foot-tall adult in a group of average-height people.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Gaussian distribution,"A continuous probability distribution characterized by a symmetric bell-shaped curve, defined by mean \( \mu \) and variance \( \sigma^2 \).<br><br>\[ X \sim N(\mu, \sigma^2) \]<br><br><b>ELI5:</b> Like the natural spread of heights—most people are around average, with fewer very tall or very short people.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Gaussian PDF formula,"The probability density function for a normal distribution with mean \( \mu \) and standard deviation \( \sigma \):<br><br>\[ f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \]<br><br>As \( x \) moves away from \( \mu \), the PDF value decreases.<br><br><b>ELI5:</b> Like a hill shape—highest at the center (average), sloping down on both sides.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does variance affect Gaussian distribution,"Variance \( \sigma^2 \) controls the spread of the bell curve. Larger variance = wider, flatter curve. Smaller variance = narrower, taller curve.<br><br>\[ \sigma^2 \uparrow \implies \text{spread} \uparrow \]<br><br><b>ELI5:</b> Like stretching a rubber band—more stretch means data is more spread out.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does mean affect Gaussian distribution,"Mean \( \mu \) shifts the entire bell curve left or right along the x-axis without changing its shape.<br><br>\[ \mu \uparrow \implies \text{curve shifts right} \]<br><br><b>ELI5:</b> Like sliding a bell along a table—the bell shape stays the same, but its center moves.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the relationship between population and sample,Population is the complete set of all possible observations. Sample is a subset used to estimate population parameters when full data is unavailable.<br><br>\[ \text{Sample Mean } \bar{x} \approx \text{Population Mean } \mu \]<br><br><b>ELI5:</b> Like tasting soup—you don't need to eat the whole pot to know if it's salty.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the CDF of a Gaussian,"The Cumulative Distribution Function for a normal distribution N(µ, σ²) gives the probability that a random variable X is less than or equal to a value x. As variance decreases, the CDF approaches a step function.<br><br>\[ F(x) = \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{(t-\mu)^2}{2\sigma^2}} dt \]<br><br><b>ELI5:</b> Like a running tally of how many people are shorter than a certain height in a population; as everyone becomes the same height, the tally jumps instantly from 0 to 100%.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the 68 95 99.7 rule,"For a normal distribution, approximately 68% of data falls within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3. This quantifies the concentration of data around the mean.<br><br><b>ELI5:</b> If you measure the height of people, almost everyone is within 3 inches of the average; very few giants or ants exist.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What defines a symmetric distribution,A distribution is symmetric about a mean µ if the probability density is identical on both sides. Mathematically: F(µ - h) = F(µ + h) for any distance h. The left side is a mirror image of the right.<br><br><b>ELI5:</b> Like a perfectly balanced seesaw; the shape on the left is identical to the shape on the right.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What does skewness measure,"Skewness measures the asymmetry of a distribution around its mean. A positive skew indicates a longer tail to the right; negative skew indicates a longer tail to the left. It quantifies deviation from symmetry.<br><br><b>ELI5:</b> Like a bell curve that is pulled to one side, dragging a long tail of outliers behind it.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is excess kurtosis,"Excess kurtosis is kurtosis minus 3. It compares a distribution's tailedness to a Gaussian distribution (which has kurtosis = 3). It measures the prevalence of outliers, not peak height.<br><br><b>ELI5:</b> Checking if the data has more extreme outliers than a standard normal curve; lower values mean fewer surprises.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does kurtosis relate to outliers,"Kurtosis is a measure of tailedness, not peakedness. High kurtosis indicates heavy tails and a higher probability of extreme outliers. Low kurtosis indicates light tails and fewer outliers.<br><br><b>ELI5:</b> Like checking how thick the ends of the bell curve are; thicker ends mean more chance of finding a giant or an ant.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a standard normal variate Z,"A random variable Z follows the standard normal distribution Z ~ N(0, 1), meaning it has a mean of 0 and a variance of 1. It is used to standardize other normal variables.<br><br><b>ELI5:</b> A universal ruler where the average is 0 and almost all values fall between -3 and +3.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How do you standardize a normal variable X,"To convert X ~ N(µ, σ²) to Z ~ N(0, 1), apply mean centering and scaling:<br><br>\[ Z = \frac{X - \mu}{\sigma} \]<br><br>Subtract the mean to center at 0, divide by standard deviation to scale variance to 1.<br><br><b>ELI5:</b> Like shifting a graph so the average is at zero and stretching or shrinking it so the spread matches a standard bell curve.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is kernel density estimation,Kernel density estimation (KDE) computes a probability density function (PDF) by placing a Gaussian kernel (bell curve) at each observation and summing their contributions. The variance of the kernel is called bandwidth.<br><br><b>ELI5:</b> Imagine smoothing a histogram by drawing a small bell curve at each data point and adding them up to get a smooth curve.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does bandwidth affect kernel density estimation,"Bandwidth controls the smoothness of the KDE. A small bandwidth makes the estimate more rigid (high variance), while a large bandwidth makes it smoother (high bias).<br><br><b>ELI5:</b> Think of bandwidth as the width of a paintbrush. A narrow brush captures fine details but may overfit, while a wide brush smooths out noise but may miss details.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the sampling distribution of the sample mean,The sampling distribution of the sample mean is the distribution of means from multiple random samples of size n drawn from a population. It has a mean equal to the population mean and variance equal to the population variance divided by n.<br><br><b>ELI5:</b> Imagine taking many small groups of people and calculating their average height. The distribution of these averages will cluster around the true average height of the entire population.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What does the Central Limit Theorem state,"The Central Limit Theorem states that the sampling distribution of the sample mean will approximate a Gaussian distribution as the sample size n increases, regardless of the original population distribution, provided the population has finite mean and variance.<br><br><b>ELI5:</b> No matter the shape of the original data, the average of many small samples will form a bell curve.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sample size affect the Central Limit Theorem,"As sample size n increases, the sampling distribution of the sample mean becomes more Gaussian. Typically, n ≈ 30 is sufficient for the approximation to hold.<br><br><b>ELI5:</b> The larger your sample, the more the distribution of averages looks like a bell curve, even if the original data is skewed.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a Q-Q plot,"A Q-Q plot is a graphical tool to compare two distributions by plotting their quantiles against each other. If the points lie on a straight line, the distributions are similar.<br><br><b>ELI5:</b> Imagine lining up two groups of people by height and comparing their positions. If the line is straight, the height distributions are similar.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does a Q-Q plot test for normality,"To test for normality, sort the data and plot its quantiles against the quantiles of a standard normal distribution. A straight line indicates normality.<br><br><b>ELI5:</b> If your data points follow a straight line when compared to a perfect bell curve, your data is likely normally distributed.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sample size affect Q-Q plot interpretation,"Small sample sizes make Q-Q plots harder to interpret because there are fewer points to assess the linearity of the plot.<br><br><b>ELI5:</b> With few data points, it's like trying to draw a straight line with only a couple of dots—it's unclear if the line is truly straight.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the purpose of distributions in data analysis,"Distributions provide a theoretical model of a random variable, allowing us to understand its properties and make predictions about unseen data points.<br><br>\[ \text{Model} \xrightarrow{\text{properties}} \text{Predictions} \]<br><br><b>ELI5:</b> Like knowing the size distribution of 500 employees lets you order the right number of XL shirts for 100,000 employees.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is Chebyshev's inequality,"A bound on the proportion of values that lie within a certain distance from the mean, requiring only the mean and standard deviation.<br><br>\[ P(|x - \mu| \geq k\sigma) \leq \frac{1}{k^2} \]<br>where \( \mu \) is the mean, \( \sigma \) is the standard deviation, and \( k \) is the number of standard deviations.<br><br><b>ELI5:</b> Like estimating how many salaries are far from the average without knowing the exact salary distribution.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does the discrete uniform distribution work,A distribution where \( n = b - a + 1 \) outcomes are equally likely.<br><br>\[ \text{PMF} = \frac{1}{n} \quad \text{Mean} = \frac{a+b}{2} \quad \text{Variance} = \frac{n^2 - 1}{12} \]<br>where \( a \) is the minimum value and \( b \) is the maximum value.<br><br><b>ELI5:</b> Rolling a fair die where every face has probability \( 1/6 \).,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does the continuous uniform distribution work,"A distribution where probability density is constant over the interval \( [a, b] \).<br><br>\[ \text{PDF} = \frac{1}{b-a} \quad \text{Mean} = \frac{a+b}{2} \quad \text{Variance} = \frac{(b-a)^2}{12} \]<br>where \( a \) is the lower bound and \( b \) is the upper bound.<br><br><b>ELI5:</b> Like picking a random point uniformly between 0 and 1.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is uniform distribution,A distribution where every value has equal probability of being selected.<br><br><b>ELI5:</b> Like picking a random card from a shuffled deck where every card has the same chance.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does Bernoulli distribution work,"Models a single binary outcome (e.g., coin toss) with parameter p (success probability).<br><br><b>ELI5:</b> Like flipping a coin where heads has probability p.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does Binomial distribution work,Models the number of successes in n independent Bernoulli trials with success probability p.<br><br><b>ELI5:</b> Like counting heads in 10 coin flips where each flip has probability p of heads.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is log normal distribution,A distribution where the natural logarithm of the variable is normally distributed.<br><br><b>ELI5:</b> Like measuring the length of comments where most are short but a few are very long.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How to check log normality,"Use a QQ plot to compare the log of the data to a normal distribution.<br><br><b>ELI5:</b> Like checking if the lengths of comments, when logged, follow a bell curve.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a power law distribution,"A distribution where a small number of items have very high values, and a large number of items have very low values. It is characterized by the Pareto principle (80/20 rule).<br><br>\[ P(X > x) \propto x^{-\alpha} \]<br>\(\alpha\) is the shape parameter determining the tail heaviness.<br><br><b>ELI5:</b> Like wealth distribution: a few people own most of the money, while most people own very little.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Pareto principle,"A specific power law relationship where roughly 80% of consequences come from 20% of the causes. In a Pareto distribution, this manifests as 80% of the mass being concentrated in the bottom 20% of values (or vice versa depending on perspective).<br><br><b>ELI5:</b> In a messy room, 80% of the trash is found in 20% of the corners.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Box Cox transform,A power transformation method used to stabilize variance and make data approximate a Gaussian distribution. It parameterizes a family of transformations using a parameter \(\lambda\).<br><br>\[ y_i = \begin{cases} \frac{x_i^{\lambda} - 1}{\lambda} & \text{if } \lambda \neq 0 \\ \ln(x_i) & \text{if } \lambda = 0 \end{cases} \]<br><br><b>ELI5:</b> A mathematical 'mold' that reshapes skewed data (like Pareto) into a bell curve so algorithms work better.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does lambda affect Box Cox,\(\lambda\) controls the shape of the transformation. <br>- \(\lambda = 0\) implies a log transform (handles multiplicative processes).<br>- \(\lambda = 1\) implies a linear shift (no change).<br>- Negative \(\lambda\) compresses large values.<br><br><b>ELI5:</b> Turning a dial to find the exact angle that straightens out a curved graph.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
When does Box Cox fail,It fails if the data contains negative values or zeros (since powers of non-positive numbers are problematic). It also fails if the data does not strictly follow a power law or log-normal structure. Use QQ-plots to validate the result.<br><br><b>ELI5:</b> You can't use a hammer on a screw; check if the data actually fits the tool.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Weibull distribution used for,"Modeling extreme events and reliability, specifically 'time-to-failure' or extreme value analysis. It is used for rainfall, river discharges, and component lifespans.<br><br>\[ P(X > x) = e^{-(x/\lambda)^k} \]<br><br><b>ELI5:</b> Predicting how long a dam will survive a 100-year storm or when a machine part will break.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How to verify Box Cox transformation results,"Use a QQ-plot (Quantile-Quantile plot) comparing the transformed data against a theoretical Gaussian distribution. If the points lie on the diagonal line, the transformation was successful.<br><br><b>ELI5:</b> Placing the reshaped data against a perfect bell curve to see if they match up perfectly.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is co-variance,"Co-variance measures the joint variability of two random variables X and Y. It indicates the direction of their linear relationship.<br><br>\[ \text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu_x)(y_i - \mu_y) \]<br>where \(x_i, y_i\) are data points and \(\mu_x, \mu_y\) are means.<br><br><b>ELI5:</b> Like checking if two friends walk in the same direction: if one speeds up, does the other also speed up (positive) or slow down (negative)?",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sign of co-variance indicate relationship,"The sign of co-variance reveals the direction of the linear relationship between variables.<br><br>\[ \text{Cov}(X, Y) > 0 \implies \text{As } X \uparrow, Y \uparrow \]<br>\[ \text{Cov}(X, Y) < 0 \implies \text{As } X \uparrow, Y \downarrow \]<br><br><b>ELI5:</b> Positive is like two cars on a highway speeding up together; negative is like a seesaw where one side goes up while the other goes down.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is Pearson correlation coefficient,"Pearson correlation coefficient (\(\rho\)) measures the strength and direction of a LINEAR relationship between two variables, normalized by their standard deviations.<br><br>\[ \rho_{x,y} = \frac{\text{Cov}(X, Y)}{\sigma_x \sigma_y} \]<br>Range: \([-1, 1]\). \(\rho = 0\) implies no linear relation.<br><br><b>ELI5:</b> Like measuring how perfectly two dancers move in sync along a straight line.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is limitation of Pearson correlation,"Pearson correlation only detects linear relationships. It fails to capture non-linear patterns.<br><br>\[ \rho \approx 0 \nRightarrow \text{No relationship} \]<br>\(\rho\) can be zero even if variables have a strong non-linear relationship (e.g., parabolic).<br><br><b>ELI5:</b> Like judging friendship only by how often people stand in a straight line together, missing complex interactions.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is Spearman rank correlation,"Spearman rank correlation measures monotonic relationships by applying Pearson correlation to the RANKS of the data, not raw values. It is robust to outliers.<br><br>\[ \rho_{\text{Spearman}} = \text{Corr}(\text{rank}(X), \text{rank}(Y)) \]<br><br><b>ELI5:</b> Like ranking students by height and weight instead of using exact measurements; outliers (e.g., one giant student) don't skew the result.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does Spearman differ from Pearson,"Spearman captures monotonic (consistently increasing/decreasing) relationships, while Pearson captures only linear ones. Spearman is robust to outliers; Pearson is sensitive to them.<br><br><b>ELI5:</b> Pearson is a ruler for straight lines; Spearman is a flexible rubber band that follows the curve's general direction.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is correlation vs causation,"Correlation measures statistical relationship between variables (e.g., rank correlation coefficient ~0.8). Causation implies direct influence, requiring causal models. Correlation does not imply causation.<br><br><b>ELI5:</b> Eating more chocolate doesn't make you win a Nobel Prize, even if data shows both increase together.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does correlation help in real estate,"Correlation between salary and home sq footage helps real estate agents predict pricing trends and client preferences.<br><br><b>ELI5:</b> If bigger houses usually mean higher salaries, agents can target wealthier buyers.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does correlation help in education policy,"Correlation between years of education and income guides policies to incentivize longer study durations.<br><br><b>ELI5:</b> More school years often mean higher pay, so governments may push for longer education.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does correlation help in ecommerce,"Correlation between time spent on a webpage and future spending helps ecommerce sites optimize engagement strategies.<br><br><b>ELI5:</b> If users who linger buy more, sites may design features to keep visitors longer.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a confidence interval,"A confidence interval (C.I.) is a range where the population parameter (e.g., mean μ) is expected to lie with a specified probability (e.g., 95%). It is derived from sample data and reflects uncertainty in estimation.<br><br><b>ELI5:</b> If 95% of samples give a mean height between 165–175 cm, we're 95% confident the true average is in that range.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sample size affect confidence interval,"As sample size (n) increases, the sample mean converges to the population mean (Law of Large Numbers), narrowing the confidence interval.<br><br><b>ELI5:</b> More data means less guesswork—like averaging more opinions to get closer to the truth.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does confidence interval relate to probability,"A 95% C.I. means 95% of sampled intervals will contain the true parameter, not that the parameter has a 95% chance of being in the interval.<br><br><b>ELI5:</b> It's like saying 95% of nets thrown will catch the fish, not that the fish has a 95% chance to be in one net.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is hypothesis testing,A statistical method to determine if there is enough evidence to reject a null hypothesis about a population parameter.<br><br><b>ELI5:</b> Like a court trial where the null hypothesis is 'innocent until proven guilty' and data is the evidence.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the null hypothesis,"A default assumption that there is no effect or no difference (e.g., μ₂ - μ₁ = 0).<br><br><b>ELI5:</b> The starting point that says 'nothing interesting is happening here.'",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a p-value,The probability of observing the test statistic (or more extreme) if the null hypothesis is true.<br><br><b>ELI5:</b> How likely your data would occur if nothing special was happening.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does p-value relate to hypothesis testing,"If p-value < threshold (e.g., 0.05), reject the null hypothesis. It does not prove the alternative hypothesis.<br><br><b>ELI5:</b> Like a smoke detector: a low p-value is a loud alarm, but it doesn't tell you what caused the fire.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sample size affect p-value,"Larger samples reduce variability, making it easier to detect true differences (lower p-values for same effect size).<br><br><b>ELI5:</b> More data = clearer picture, like zooming in on a blurry photo.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a permutation test,A resampling method where data is shuffled to simulate the null hypothesis distribution.<br><br><b>ELI5:</b> Like shuffling a deck of cards to see if your hand is truly unusual.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does permutation test compute p-value,1. Shuffle data and compute test statistic repeatedly.<br>2. Compare observed statistic to shuffled distribution.<br>3. p-value = fraction of shuffled stats ≥ observed.<br><br><b>ELI5:</b> Like seeing how often random shuffles give a result as extreme as yours.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does p-value threshold vary by domain,"Medical studies often use 1% (more stringent), while general research uses 5%.<br><br><b>ELI5:</b> Like setting a higher bar for life-or-death decisions vs. everyday choices.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the Kolmogorov-Smirnov test,The Kolmogorov-Smirnov test is a non-parametric test to determine if two random variables follow the same distribution. It compares the empirical CDFs of the two samples and calculates the maximum distance (D) between them.<br><br><b>ELI5:</b> Imagine two lines representing how data accumulates for two groups. The K-S test measures the biggest gap between these lines to see if they're from the same source.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does the K-S test work,"The K-S test works by plotting the CDFs of two random variables and computing the supremum (maximum distance) D between them. The null hypothesis is rejected if D exceeds a critical value based on sample sizes and significance level α.<br><br><b>ELI5:</b> It's like comparing two growth charts for plants. If the biggest difference between them is too large, they're probably different species.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is the test statistic in K-S test,"The test statistic D is the maximum absolute difference between the CDFs of two samples: D = sup |CDF(X₁) - CDF(X₂)|. The null hypothesis is rejected when D > c(α) * sqrt((n+m)/nm), where c(α) = sqrt(-0.5*ln(α/2)).<br><br><b>ELI5:</b> It's the largest gap between two cumulative distribution curves. If this gap is too wide, the distributions are considered different.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does sample size affect K-S test,"Larger sample sizes make the K-S test more sensitive to small differences. The critical value D depends on sample sizes n and m, with larger samples requiring smaller D to reject the null hypothesis.<br><br><b>ELI5:</b> With more data points, even small differences become noticeable, like seeing tiny cracks in a wall when you look closely.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is hypothesis testing for difference of means,"Hypothesis testing for difference of means compares sample means from two populations. The null hypothesis states no difference (μ₁ - μ₂ = 0), while the alternative suggests a difference exists.<br><br><b>ELI5:</b> It's like comparing average heights of two cities. If the difference is too large to be random, we conclude the cities have different average heights.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does p-value determine hypothesis rejection,"The p-value represents the probability of observing the test statistic under the null hypothesis. If p-value < α (e.g., 0.05), reject the null hypothesis; otherwise, fail to reject it.<br><br><b>ELI5:</b> If there's only a 3% chance of seeing your result by random luck, you'd bet it's not just luck.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How does K-S test relate to hypothesis testing,"The K-S test is a specific type of hypothesis test that compares entire distributions rather than just means. It uses the maximum CDF difference as its test statistic.<br><br><b>ELI5:</b> While some tests just compare averages, the K-S test compares the entire shape of the data distributions.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is a permutation test,"A non-parametric method to test if two groups come from the same distribution by shuffling group labels and recomputing the test statistic many times.<br><br>\[ \text{Step 1: } S = C_1 \cup C_2 \]
\[ \text{Step 2: } S_1, S_2 \sim \text{RandomSplit}(S, 50, 50) \]
\[ \text{Step 3: } \Delta = \mu_2 - \mu_1 \]
\[ \text{Repeat } k \text{ times} \]<br><br><b>ELI5:</b> Like shuffling a deck of cards and dealing new hands to see if your original hand was unusually good.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How is p-value computed via permutation,"Simulate the null distribution by resampling, then calculate the proportion of simulated statistics exceeding the observed statistic.
\[ p = P(\Delta_{sim} \geq \Delta_{obs} \mid H_0) \]
\[ \Delta_{obs} = \mu_B - \mu_A = 5 \]<br><br><b>ELI5:</b> Count how often random shuffles produce a result as extreme as yours.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How to interpret p-value threshold,"Compare p-value to significance level α. If p > α, accept H0; if p ≤ α, reject H0.
\[ \alpha \in \{0.001, 0.01, 0.03, 0.05\} \]
Medical: α = 0.001–0.01<br>E-commerce: α = 0.03–0.05<br><br><b>ELI5:</b> If the chance of seeing your result by luck is smaller than your risk tolerance, you call it significant.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is proportional sampling,"Selecting items with probability proportional to their value, not uniformly.
\[ d = [2.0, 6.0, 1.2, 5.8, 20.0] \]
\[ p_i = \frac{d_i}{\sum d} \]
\[ C_{list} = \text{cumsum}(p) \]<br><br><b>ELI5:</b> Bigger values get a bigger slice of the probability pie.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
How to perform proportional sampling,"1. Normalize values to probabilities \( p_i = d_i / \sum d \)<br>2. Compute cumulative sum \( C_{list} \)<br>3. Draw \( r \sim U(0,1) \)<br>4. Return \( d_i \) where \( C_{list}[i-1] < r \leq C_{list}[i] \)<br><br><b>ELI5:</b> Line up values by size, throw a dart, and see which section it hits.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::probability_and_statistics
What is dimensionality reduction,Transforms high-dimensional data into fewer features while preserving variance to minimize information loss.<br><br>\[ \text{High Dimensions } \xrightarrow{\text{reduce}} \text{Lower Dimensions} \]<br>\[ \text{Preserve Variance } \approx \text{Preserve Information} \]<br><br><b>ELI5:</b> Like summarizing a long book into key bullet points - you keep the main ideas but remove the fluff.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
When to use dimensionality reduction,Use when n > 6 dimensions because humans cannot visualize beyond 3D.<br>2D/3D: Scatter plots<br>Up to 6D: Pair plots<br>More than 6D: Dimensionality reduction required<br><br><b>ELI5:</b> Like trying to draw a 10-dimensional object - you must project it onto 2D paper to see its shape.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How to represent a dataset mathematically,"Dataset D = {x_i, y_i} where:<br>x_i ∈ ℝ^d (data point as d-dimensional vector)<br>y_i ∈ ℝ^1 (class label)<br>i = [1 to n] (n data points)<br><br><b>ELI5:</b> Each data point is a coordinate in d-dimensional space, with a label attached.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How to represent dataset as a matrix,Matrix representation (preferred):<br>Each row = one data point x_i<br>Each column = one feature<br>Y = column vector of class labels<br><br>\[ X = \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{bmatrix} \in \mathbb{R}^{n \times d} \]<br><br><b>ELI5:</b> Spreadsheet where rows are observations and columns are measurements.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is the objective of dimensionality reduction,"Minimize information loss while reducing features:<br>\[ \min_{\text{new features}} \left( \text{Information Lost} \right) \]<br>Subject to: \[ \text{Dimensionality} < \text{Original} \]<br>Goal: Preserve maximum variance<br><br><b>ELI5:</b> Like compressing a photo - keep the important details, remove the pixels nobody notices.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is column normalization,"Transforms each feature to range [0, 1] using min-max scaling:
\[ a_i' = \frac{a_i - a_{min}}{a_{max} - a_{min}} \]
Squishes data into unit hypercube. Removes feature scale differences.
<br><br><b>ELI5:</b> Like resizing photos to same dimensions so no single feature dominates.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
Why normalize features before modeling,"Prevents scale dominance. Features with large ranges (1M-2M) overpower small ranges (0-10) in distance calculations and gradient descent. Normalization ensures equal contribution.
<br><br><b>ELI5:</b> Like giving all players same weight class before a race.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is column standardization,"Transforms features to mean=0, std=1:
\[ a_i' = \frac{a_i - \mu}{\sigma} \]
Combines mean centering + unit scaling. Data centered at origin with unit variance.
<br><br><b>ELI5:</b> Like centering a photo on origin and scaling to standard size.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How does standardization differ from normalization,"Normalization: maps to [0,1] range using min/max. Standardization: maps to mean=0, std=1 using mean/std. Standardization preserves outliers; normalization compresses them.
<br><br><b>ELI5:</b> Normalization squeezes everything between 0-1. Standardization centers at 0 with equal spread.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
Geometric interpretation of data mean,"Project all points onto each feature axis. The mean vector is the central point of these projections:
\[ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i \]
Represents centroid of point cloud.
<br><br><b>ELI5:</b> Like finding the balance point of a cloud of dots.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is a covariance matrix,"A square matrix S of size d x d where each element Sij represents the covariance between the i-th and j-th features. Diagonal elements Sii are variances of individual features, and off-diagonal elements capture feature relationships.<br><br>\[ S_{ij} = \text{cov}(f_i, f_j) \]<br>\[ \text{cov}(x,y) = \frac{1}{n} \sum_{k=1}^{n} (x_k - \bar{x})(y_k - \bar{y}) \]<br><br><b>ELI5:</b> Like a relationship scorecard for features - diagonal shows how much each feature varies alone, off-diagonal shows how features move together.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How does column standardization simplify covariance,"When features are column standardized (mean=0, std=1), the covariance matrix becomes S = (X^T X)/n. Each element Sij = (f_i · f_j)/n, which is just the dot product of normalized feature vectors divided by sample count.<br><br>\[ S(X) = \frac{X^T X}{n} \]<br><br><b>ELI5:</b> Standardization removes scaling differences, so covariance becomes pure geometric alignment between features.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How is MNIST data structured for dimensionality reduction,MNIST contains 60k images where each x_i is a 28x28 pixel image. Flattening converts this to a 784-dimensional column vector by stacking rows. PCA or t-SNE can then reduce 784D to 2D for visualization.<br><br>\[ \text{Image}(28 \times 28) \rightarrow \text{Vector}(784 \times 1) \]<br><br><b>ELI5:</b> Each image is a grid of pixels; we unroll it into a long list of numbers so math can find the most important patterns.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is the alternative formulation of PCA?,"PCA can be formulated as minimizing the sum of squared distances from data points to the projection line, which is mathematically equivalent to maximizing the variance of the projected data.

Distance minimization objective:
\[ \min_{\mathbf{u}_1} \sum_{i=1}^{n} (\|\mathbf{x}_i\|^2 - (\mathbf{u}_1^T \mathbf{x}_i)^2) \]

Variance maximization objective:
\[ \max_{\mathbf{u}_1} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{u}_1^T \mathbf{x}_i)^2 \]

Both share the constraint: \( \mathbf{u}_1^T \mathbf{u}_1 = 1 \)

<br><br><b>ELI5:</b> Imagine pinning a sheet to a wall with thumbtacks. Finding the best line to stretch the sheet tight (minimize distance) is the same as finding the line that captures the most spread of the tacks (maximize variance).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA use eigenvalues and eigenvectors?,"The eigenvectors of the covariance matrix \( S = X^T X \) provide the directions of maximum variance. The corresponding eigenvalues \( \lambda \) quantify the amount of variance along those directions.

Covariance matrix:
\[ S = X^T X \]

Eigenvalue equation:
\[ S \mathbf{v}_1 = \lambda_1 \mathbf{v}_1 \]

The first principal component \( \mathbf{u}_1 \) is the eigenvector \( \mathbf{v}_1 \) corresponding to the largest eigenvalue \( \lambda_1 \).

<br><br><b>ELI5:</b> If your data is a cloud of points, eigenvectors are the arrows pointing through the thickest parts of the cloud. Eigenvalues tell you how thick the cloud is along each arrow.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How do eigenvectors relate to variance preservation?,"Eigenvectors provide the directions of maximum variance, and eigenvalues provide the percentage of variance preserved along each direction. The percentage of variance preserved by the first \( d' \) components is:

\[ \text{Variance Preserved} = \frac{\sum_{i=1}^{d'} \lambda_i}{\sum_{i=1}^{d} \lambda_i} \]

where \( \lambda_1 > \lambda_2 > \dots > \lambda_d \).

<br><br><b>ELI5:</b> The total spread of data is the sum of all eigenvalues. The ratio of a specific eigenvalue to the total tells you how much of the data's story is told by that specific direction.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA perform dimensionality reduction?,"PCA transforms data by projecting it onto the subspace spanned by the top \( k \) eigenvectors. The original \( d \)-dimensional data point \( \mathbf{x}_i \) is reduced to a \( k \)-dimensional representation \( \mathbf{x}_i' \):

\[ \mathbf{x}_i' = [\mathbf{x}_i^T \mathbf{v}_1, \mathbf{x}_i^T \mathbf{v}_2, \dots, \mathbf{x}_i^T \mathbf{v}_k] \]

This is done by dropping the components with the smallest eigenvalues (least variance).

<br><br><b>ELI5:</b> To flatten a 3D object into 2D, you squash it along its thinnest dimension. PCA identifies the thinnest dimensions and discards them, keeping only the thickest ones.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
What are the steps to compute the first principal component?,"1. **Standardize**: Ensure each column of \( X \) has mean 0 and standard deviation 1.
2. **Covariance**: Compute \( S = X^T X \).
3. **Eigendecomposition**: Find eigenvalues \( \lambda \) and eigenvectors \( \mathbf{v} \) of \( S \).
4. **Select**: The first principal component \( \mathbf{u}_1 \) is the eigenvector corresponding to the largest eigenvalue \( \lambda_1 \).

<br><br><b>ELI5:</b> Center your data, find the matrix that describes its shape, find the directions that stretch that shape the most, and pick the direction with the biggest stretch.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
What is PCA,Principal Component Analysis is a dimensionality reduction technique that projects data onto orthogonal axes (principal components) that maximize variance.<br><br>\[ \text{maximize } \text{Var}(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S} \mathbf{w}}{\mathbf{w}^T \mathbf{w}} \]<br>where \( \mathbf{S} \) is the covariance matrix and \( \mathbf{w} \) is the projection vector.<br><br><b>ELI5:</b> Like finding the best camera angle to see the most variation in a 3D object before taking a 2D photo.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA work,"PCA computes eigenvectors and eigenvalues of the covariance matrix. The eigenvectors define the principal components (directions), and eigenvalues define the variance along each component.<br><br>\[ \mathbf{S} \mathbf{v}_i = \lambda_i \mathbf{v}_i \]<br>\[ \mathbf{X}_{\text{new}} = \mathbf{X} \mathbf{V}_k \]<br>where \( \mathbf{V}_k \) contains the top \( k \) eigenvectors.<br><br><b>ELI5:</b> Rotate the data so the axes align with the directions of maximum spread, then keep only the most spread-out dimensions.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA affect information loss,"PCA discards low-variance components, which can cause inseparable data to become crowded and overlapping in reduced dimensions.<br><br>\[ \text{Reconstruction Error} = \| \mathbf{X} - \hat{\mathbf{X}} \|^2 \]<br>where \( \hat{\mathbf{X}} \) is the projection onto the top \( k \) components.<br><br><b>ELI5:</b> Flattening a 3D object into 2D loses depth; distinct shapes might look identical when squashed.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
What is the effect of PCA standardization,"StandardScaler() mean-centers and scales features to unit variance before PCA, ensuring features with larger scales don't dominate the principal components.<br><br>\[ \mathbf{X}_{\text{scaled}} = \frac{\mathbf{X} - \mu}{\sigma} \]<br><br><b>ELI5:</b> Equalizes all measurements so no single feature shouts louder than others.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
What is PCA for dimensionality reduction,"PCA projects data onto a lower-dimensional subspace spanned by the top eigenvectors of the covariance matrix. It maximizes the retained variance of the data.<br><br>\[ X' = X V_{d'} \]<br>where \(X\) is the centered data matrix, \(V_{d'}\) contains the top \(d'\) eigenvectors, and \(X'\) is the reduced data.<br><br><b>ELI5:</b> Like compressing a photo by keeping only the most important colors and shapes, discarding the rest to save space while keeping the main image recognizable.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA maximize variance,"PCA finds orthogonal directions (eigenvectors) in the data that have the largest spread (variance). It solves the eigenvalue problem for the covariance matrix, sorting eigenvectors by their eigenvalues (variance).<br><br>\[ \text{Cov}(X) v = \lambda v \]<br>\(\lambda\) is the variance along direction \(v\). We pick the \(d'\) largest \(\lambda\).<br><br><b>ELI5:</b> Imagine a 3D cloud of points. PCA finds the longest axis of the cloud, then the next longest axis perpendicular to it. These axes capture the most information about where the points are.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does d prime affect PCA,"Increasing \(d'\) (number of components) increases retained variance and reduces reconstruction error (low bias), but keeps more dimensions (higher variance). Decreasing \(d'\) aggressively compresses data, potentially losing signal (high bias).<br><br>\[ \text{Retained Variance} = \frac{\sum_{i=1}^{d'} \lambda_i}{\sum_{i=1}^{d} \lambda_i} \]<br><br><b>ELI5:</b> Choosing a small \(d'\) is like summarizing a book in one sentence (fast but loses detail). A large \(d'\) is like a 10-page summary (accurate but long).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
How does PCA relate to t-SNE,PCA is linear and global; it preserves global structure and variance. t-SNE is non-linear and local; it preserves local neighborhoods to reveal clusters. PCA is often used to reduce dimensions before applying t-SNE for speed.<br><br><b>ELI5:</b> PCA flattens a 3D object onto a 2D sheet mathematically. t-SNE tears the object apart and rearranges the pieces on the sheet so that pieces that were close together in 3D stay close in 2D.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::pca_principal_component_analysis
What is t-SNE,"t-SNE is a nonlinear technique for visualizing high-dimensional data by embedding it into 2D or 3D space. Unlike PCA, it prioritizes preserving local neighborhood structures rather than global distances.<br><br>\[ \text{High-dim} \xrightarrow{\text{t-SNE}} \text{Low-dim (visualization)} \]<br><br><b>ELI5:</b> Like taking a crowded city map and folding it so that neighbors stay close, but far-away cities can move arbitrarily far apart.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is embedding in t-SNE,"Embedding is the process of projecting every point from the original high-dimensional dataset into a new, lower-dimensional representation space. Each original point gets a corresponding coordinate in the low-dimensional map.<br><br>\[ x_i \in \mathbb{R}^d \rightarrow x_i' \in \mathbb{R}^{k} \quad (k \ll d) \]<br><br><b>ELI5:</b> Taking every person in a 3D crowd photo and placing them as 2D dots on a piece of paper while keeping their relationships.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does t-SNE preserve structure,"t-SNE preserves distances between points that are close neighbors in high-dimensional space. Distances between non-neighbor points are ignored and can be distorted.<br><br>\[ \text{dist}(x_1, x_2) \approx \text{dist}(x_1', x_2') \quad \text{if } x_1, x_2 \text{ are neighbors} \]<br>\[ \text{dist}(x_1, x_4) \text{ is not preserved} \]<br><br><b>ELI5:</b> If two people were whispering in the crowd, they must stay whispering distance apart on the map. People on opposite sides of the room can end up anywhere.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is the role of KL divergence in t-SNE,"KL divergence measures the mismatch between neighborhood probabilities in high-dim (p) and low-dim (q). It is asymmetric: it penalizes heavily if nearby points in high-dim become far apart in low-dim.<br><br>\[ \text{Cost} = \sum_i \text{KL}(p_i \| q_i) \]<br><br><b>ELI5:</b> A strict teacher that screams if neighbors on the original map are separated on the new map, but doesn't care if strangers get closer.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does perplexity affect t-SNE,Perplexity controls the number of neighbors considered when calculating local probabilities. Low perplexity = very local focus (tiny neighborhoods). High perplexity = broader context (more neighbors).<br><br>\[ \text{Perplexity} \approx \text{Effective number of neighbors} \]<br><br><b>ELI5:</b> How many people you consider 'your neighborhood'. 2 friends vs 20 neighbors changes how you draw the map.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does t-SNE differ from PCA,"PCA preserves global structure (large distances) but discards local structure. t-SNE preserves local structure (neighborhoods) but discards global distances.<br><br>\[ \text{PCA: } \text{maximize variance} \quad \text{t-SNE: } \text{minimize local distortion} \]<br><br><b>ELI5:</b> PCA keeps the overall shape of the country. t-SNE keeps the cities and their suburbs intact, but might fold the country map.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is the crowding problem in t-SNE,"Non-neighbor data points can appear crowded with neighborhood points because t-SNE preserves local distances but cannot maintain all neighborhoods simultaneously.<br><br><b>ELI5:</b> Imagine a crowded subway where people from different groups are forced close together, even if they don't know each other.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does t-SNE preserve distances,"t-SNE preserves distances within local neighborhoods but does not maintain global distances between clusters. It optimizes for local similarity.<br><br><b>ELI5:</b> Like a city map where nearby streets are accurate, but distant landmarks may be distorted.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is the role of perplexity in t-SNE,"Perplexity loosely defines the number of neighbors whose distances are preserved. Low perplexity focuses on tight clusters, while high perplexity merges more points into the same neighborhood.<br><br><b>ELI5:</b> Like adjusting a spotlight: narrow (low perplexity) highlights a few friends, wide (high perplexity) includes strangers.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does perplexity affect t-SNE clusters,"As perplexity increases, clusters initially form clearly but eventually merge into a messy profile as more points are considered neighbors.<br><br><b>ELI5:</b> Like a party: small groups (low perplexity) are distinct, but as more people join (high perplexity), groups blur together.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
Why are t-SNE results stochastic,"The probabilistic embedding in t-SNE introduces randomness, so results vary even with identical parameters and data.<br><br><b>ELI5:</b> Like shuffling a deck of cards—each shuffle gives a different order, even with the same deck.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does t-SNE handle cluster sizes,"t-SNE expands dense clusters and shrinks sparse ones, so cluster sizes cannot be directly compared.<br><br><b>ELI5:</b> Like a magnifying glass: crowded areas (dense clusters) appear larger, while sparse areas shrink.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
Why should t-SNE not be relied upon blindly,"t-SNE results are sensitive to parameters and stochasticity. Multiple runs with varying parameters are needed for reliable interpretation.<br><br><b>ELI5:</b> Like a mood ring—colors change based on conditions, so you check it multiple times to trust the result.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does t-SNE perform on MNIST,"t-SNE groups MNIST digits based on visual similarity, reducing 784-dimensional data into a 2D/3D embedding.<br><br><b>ELI5:</b> Like sorting handwritten notes by how similar they look, even if the numbers are different.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is t-SNE,t-SNE (t-distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique that preserves local structure by minimizing the divergence between high-dimensional and low-dimensional pairwise similarities.<br><br><b>ELI5:</b> Imagine crumpling a paper map and then unfolding it; t-SNE tries to keep nearby points close while allowing distant points to spread out.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does perplexity affect t-SNE,Perplexity balances the number of effective neighbors considered. Higher perplexity captures global structure but may blur local clusters; lower perplexity focuses on local structure but may fragment clusters.<br><br><b>ELI5:</b> Like adjusting a microscope lens: too narrow (low perplexity) and you miss the big picture; too wide (high perplexity) and details blur.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is the objective of t-SNE,"Minimize the Kullback-Leibler divergence between high-dimensional pairwise similarities (Gaussian) and low-dimensional similarities (t-distribution).<br><br><b>ELI5:</b> Like rearranging puzzle pieces so nearby pieces in 3D stay close in 2D, even if far pieces move apart.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
How does iteration count affect t-SNE,More iterations refine the embedding but risk overfitting. Fewer iterations may leave the embedding under-optimized.<br><br><b>ELI5:</b> Like baking a cake: too little time and it's raw; too long and it burns.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::t_sne_t_distributed_stochastic_neighbourhood_embedding
What is the Amazon Fine Food Reviews task,Predict if a review is positive or negative based on text content. Used to determine if product improvements are needed.<br><br>\[ \text{Input: Review Text} \rightarrow \text{Output: Positive/Negative} \]<br><br><b>ELI5:</b> Like reading customer letters to decide if a toy needs fixing or if kids are happy with it.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
Why is deduplication critical in review data,"Single users posting identical reviews across multiple products at the same timestamp creates artificial bias. Use `pd.DataFrame.drop_duplicates()` on feature columns.<br><br><b>ELI5:</b> If one kid writes the same complaint letter to 10 different toy companies, counting it 10 times makes the problem seem 10x bigger than it is.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
Why convert text to vectors,Linear algebra requires numerical input. Text strings must become d-dimensional vectors to apply distance metrics and separation planes.<br><br>\[ \text{text} \in \mathbb{R}^d \]<br><br><b>ELI5:</b> Computers only understand numbers. We turn words into coordinate points so we can measure how close similar reviews are to each other.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does vector geometry separate reviews,"Positive reviews lie on one side of a separating plane, negatives on the other. Decision rule: \( w^T x > 0 \) implies positive.<br><br>\[ w^T x_i + b > 0 \implies \text{positive} \]<br><br><b>ELI5:</b> Imagine a wall dividing a room. Happy reviews are on the right side, sad reviews on the left. The math checks which side a review falls on.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does similarity relate to distance,"Higher similarity between reviews \( r_1 \) and \( r_2 \) means smaller Euclidean distance in vector space. Similar texts cluster geometrically.<br><br>\[ \text{sim}(r_1, r_2) > \text{sim}(r_1, r_3) \implies \|r_1 - r_2\| < \|r_1 - r_3\| \]<br><br><b>ELI5:</b> Two reviews about 'spicy taste' are like two points close together on a map. A review about 'packaging' is far away.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is Bag of Words (BOW)?,"A representation where text is converted into a vector of word counts, with each dimension corresponding to a unique word in the vocabulary. The vector is sparse due to the high dimensionality of the vocabulary.<br><br><b>ELI5:</b> Imagine a grocery list where each item is a word, and the count is how many times it appears in a review. Most items will have zero counts.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does BOW handle semantic meaning?,"BOW discards semantic meaning by relying solely on word counts. Reviews with opposite meanings can appear similar if they share many words, leading to poor performance for nuanced text analysis.<br><br><b>ELI5:</b> Two reviews saying 'tasty' and 'not tasty' might look identical to BOW if other words are the same.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is the objective of text preprocessing?,"To reduce noise and sparsity in text data by removing stop words, converting to lowercase, stemming, and lemmatization. This improves the efficiency and accuracy of text representations like BOW.<br><br><b>ELI5:</b> Cleaning up a messy room by removing clutter (stop words) and organizing similar items (stemming).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does stemming differ from lemmatization?,"Stemming reduces words to their root form using heuristic rules (e.g., 'tasty' → 'tast'), while lemmatization uses vocabulary and context to return the base form (e.g., 'tasty' → 'taste').<br><br><b>ELI5:</b> Stemming is like chopping off word endings, while lemmatization is like looking up the word in a dictionary.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What are n-grams?,"Sequences of n consecutive words in a text. Uni-grams are single words, bi-grams are pairs, etc. They capture local word order and context, unlike BOW which ignores sequence.<br><br><b>ELI5:</b> Think of n-grams as sliding a window over text to capture phrases like 'very tasty' instead of just 'very' and 'tasty' separately.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does n-gram size affect dimensionality?,"Higher n-gram sizes (e.g., bi-grams) increase the dimensionality of the feature space exponentially, as all possible word sequences are treated as unique features.<br><br><b>ELI5:</b> More n-grams mean more 'slots' in your grocery list, making it longer and harder to manage.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
Why is BOW sparse?,"Most words in the vocabulary do not appear in any given document, resulting in vectors with many zero entries. This sparsity is a computational challenge.<br><br><b>ELI5:</b> Like a library where most books are never checked out by a single reader.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is TF-IDF,"TF-IDF is a numerical statistic that reflects how important a word is to a document in a corpus. It is the product of Term Frequency (TF) and Inverse Document Frequency (IDF).<br><br>TF(Wi, rj) = (# of times Wi occurs in rj) / (total words in rj)<br><br>IDF(Wi, Dc) = log(N/ni)<br><br>Where:<br>- Wi: ith word<br>- rj: jth review<br>- Dc: Document corpus<br>- N: # of documents<br>- ni: # of documents containing Wi<br><br>TF-IDF = tf(Wi, rj) * idf(Wi, Dc)<br><br><b>ELI5:</b> TF-IDF is like a popularity contest for words. It gives high scores to words that are rare in the whole collection but frequent in a specific document.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does IDF work,"IDF measures how common or rare a word is across all documents. It is calculated as log(N/ni), where N is the total number of documents and ni is the number of documents containing the word Wi. The log function ensures that the IDF value is manageable and doesn't dominate the TF-IDF score.<br><br><b>ELI5:</b> IDF is like a filter that reduces the importance of common words like 'the' or 'and' and highlights rare words that might be more meaningful.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
Why use log in IDF,"The log function in IDF is used to handle the power law distribution of word frequencies (Zipf's law). Without the log, IDF values would be too large and dominate the TF-IDF score. The log function compresses these values, making them more manageable.<br><br><b>ELI5:</b> Using log in IDF is like using a magnifying glass to focus on the important details while blurring out the overwhelming background noise.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is Word2Vec,"Word2Vec is a technique that transforms words into dense vectors in a high-dimensional space. These vectors capture semantic meanings and relationships between words. Words with similar meanings are closer in this space.<br><br><b>ELI5:</b> Word2Vec is like a map where words are points. Words with similar meanings are close to each other, and relationships between words are preserved as directions on the map.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does Word2Vec work,"Word2Vec learns word vectors by analyzing the context in which words appear. It uses a large text corpus and builds vectors for each word based on the words in its neighborhood. The dimensionality of these vectors is specified as an input.<br><br><b>ELI5:</b> Word2Vec is like a detective that learns about words by looking at the company they keep. If two words often appear together, they are likely to have similar meanings.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does TF-IDF relate to Word2Vec,"TF-IDF and Word2Vec are both techniques for representing text data, but they differ in their approach. TF-IDF is based on word frequency and rarity, while Word2Vec captures semantic meanings and relationships between words.<br><br><b>ELI5:</b> TF-IDF is like counting how many times a word appears and how rare it is, while Word2Vec is like understanding the meaning and relationships between words.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
"What is CountVectorizer(ngram_range=(1,2))?","A scikit-learn tool that converts text into a matrix of token counts, including both unigrams (single words) and bigrams (pairs of adjacent words).<br><br><b>ELI5:</b> Like counting how many times each word and each word-pair appears in a document.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
"What is TfidfVectorizer(ngram_range=(1,2))?","A scikit-learn tool that converts text into a matrix of TF-IDF features, weighting terms by their importance in the document relative to the entire corpus, including unigrams and bigrams.<br><br><b>ELI5:</b> Like counting words and word-pairs but giving more weight to rare, meaningful terms.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does stemming affect Word2Vec?,"Stemming may produce words that don't exist in pre-trained Word2Vec models, leading to missing vectors. Pre-trained models rely on exact word forms.<br><br><b>ELI5:</b> Like cutting off the end of a word (e.g., 'running' → 'run') but the model only knows the full word.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is AVG-WORD2VEC,A method to compute document vectors by averaging the Word2Vec embeddings of all words in the document.<br><br><b>ELI5:</b> Like blending all word flavors in a review to get a single taste representing the whole review.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
What is TFIDF-WORD2VEC,"A weighted average of Word2Vec embeddings where weights are TF-IDF scores, emphasizing important words.<br><br><b>ELI5:</b> Like blending word flavors but giving more weight to rare, meaningful words in the review.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does AVG-WORD2VEC work,"For each word in the document, retrieve its Word2Vec embedding and compute the element-wise average of all embeddings.<br><br><b>ELI5:</b> Like taking the average height of all players on a team to represent the team's height.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does TFIDF-WORD2VEC work,"Compute TF-IDF weights for each word, then take a weighted average of their Word2Vec embeddings.<br><br><b>ELI5:</b> Like averaging player heights but giving more weight to star players who contribute more to the team.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does TFIDF affect WORD2VEC,"TF-IDF weights emphasize rare, meaningful words, reducing the influence of common words in the final vector.<br><br><b>ELI5:</b> Like turning up the volume on important voices in a crowd while muting the background noise.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::real_world_problem_predict_rating_given_product_reviews_on_amazon
How does K-Nearest Neighbors work,1. Find k points closest to query point xq in dataset D<br>2. Take class labels of k neighbors<br>3. Assign class via majority vote<br><br>\[ \text{class}(x_q) = \arg\max_{c} \sum_{i=1}^{k} \mathbb{I}(y_i = c) \]<br>where \(y_i\) are labels of neighbors and \(\mathbb{I}\) is indicator function<br><br><b>ELI5:</b> Ask your 3 closest neighbors which class they belong to; vote on the majority answer.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does k affect KNN bias and variance,"Small k (low bias, high variance): decision boundary is jagged, sensitive to noise<br>Large k (high bias, low variance): decision boundary is smooth, ignores local patterns<br><br>\[ \lim_{k \to 1} \text{boundary} \approx \text{training points} \]<br>\[ \lim_{k \to N} \text{boundary} \approx \text{global majority} \]<br><br><b>ELI5:</b> k=1 is like memorizing every detail (overfits); k=N is like guessing the average (underfits).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is Euclidean distance L2 norm,"Shortest straight-line distance between two points in space<br><br>\[ d(x_1, x_2) = \|x_1 - x_2\|_2 = \sqrt{\sum_{i=1}^{d} (x_{1i} - x_{2i})^2} \]<br>\(x_1, x_2 \in \mathbb{R}^d\) are vectors, \(d\) is dimensionality<br><br><b>ELI5:</b> Use a ruler to measure the straight line between two points on a map.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is Manhattan distance L1 norm,"Sum of absolute differences along each dimension (taxicab geometry)<br><br>\[ d(x_1, x_2) = \|x_1 - x_2\|_1 = \sum_{i=1}^{d} |x_{1i} - x_{2i}| \]<br>Movement restricted to grid lines, no diagonals<br><br><b>ELI5:</b> Distance driving through city blocks; you must follow the grid.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is Minkowski distance,"Generalized distance family that includes L1 and L2 as special cases<br><br>\[ d(x_1, x_2) = \|x_1 - x_2\|_p = \left( \sum_{i=1}^{d} |x_{1i} - x_{2i}|^p \right)^{1/p} \]<br>For p=1: Manhattan, p=2: Euclidean, p→∞: Chebyshev<br><br><b>ELI5:</b> A dial that lets you choose how strictly you measure distance along each axis.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is Hamming distance,"Count of positions where two vectors differ<br><br>\[ H(x_1, x_2) = \sum_{i=1}^{d} \mathbb{I}(x_{1i} \neq x_{2i}) \]<br>Used for binary or categorical vectors (BOW, gene sequences)<br><br>Example: \(x_1 = [0,1,1,0]\), \(x_2 = [1,0,1,0]\) → \(H = 2\)<br><br><b>ELI5:</b> Count how many switches are flipped differently between two binary strings.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
When does KNN fail,"1. Outliers: neighbors are too far, vote is unreliable<br>2. Random noise: no local structure to exploit<br><br>\[ \text{If } \min_{i} d(x_q, x_i) \gg \text{typical spacing} \Rightarrow \text{unstable prediction} \]<br><br><b>ELI5:</b> KNN fails when the query is isolated in empty space or when the data is just random static.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does distance measure affect KNN,"Choice of norm changes shape of neighborhood<br><br>Euclidean (L2): circular neighborhood, sensitive to outliers<br>Manhattan (L1): diamond neighborhood, robust to small perturbations<br>Minkowski (p): interpolates between shapes<br><br>\[ \text{Decision boundary shape} \propto \text{norm geometry} \]<br><br><b>ELI5:</b> L2 draws circles, L1 draws diamonds; the shape changes which points count as neighbors.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the space complexity limitation of kNN,kNN requires large RAM at deployment due to storing the entire training dataset.<br><br><b>ELI5:</b> Like carrying a whole library instead of just the books you need.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How do kd trees and LSH improve kNN performance,"kd trees and LSH reduce search time by organizing data spatially or using hashing, improving latency.<br><br><b>ELI5:</b> Like using a map or shortcuts to find things faster.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does k affect the decision surface in kNN,"As k increases, the decision surface becomes smoother. For k=1, it is jagged; for k=n, all points are classified as the majority class.<br><br><b>ELI5:</b> Like averaging more opinions to get a smoother consensus.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is overfitting in kNN,"Overfitting occurs when kNN generates extremely non-smooth surfaces to achieve high training accuracy, making it prone to noise.<br><br><b>ELI5:</b> Like memorizing answers instead of understanding the problem.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is underfitting in kNN,Underfitting occurs when kNN fails to generate a decision surface and classifies all points as the majority class.<br><br><b>ELI5:</b> Like guessing the same answer for every question.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does kNN avoid overfitting and underfitting,By choosing an appropriate k to generate smooth decision surfaces that balance accuracy and noise resistance.<br><br><b>ELI5:</b> Like finding the right number of opinions to make a balanced decision.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the purpose of cross validation in kNN,"To determine the optimal k hyperparameter without using the test set, ensuring the model generalizes well to unseen data.<br><br><b>ELI5:</b> Like practicing for a test with different study methods to find the best one before the real exam.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does k-fold cross validation work,"1. Split data into train and test sets.<br>2. Divide train set into k' equal parts.<br>3. For each k, train on k'-1 parts and validate on the remaining part.<br>4. Average the k' accuracies for each k.<br>5. Choose the k with the best average accuracy.<br><br><b>ELI5:</b> Like rotating study groups to test different methods and picking the best one.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
Why is k-fold cross validation better than a single validation set,"It uses all training data for validation by rotating subsets, reducing information loss and providing a more robust estimate of model performance.<br><br><b>ELI5:</b> Like getting feedback from all your friends instead of just one to make a better decision.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does the choice of k affect kNN performance,"Small k: High variance, sensitive to noise.<br>Large k: High bias, smoother decision boundaries.<br>Optimal k balances bias and variance for best generalization.<br><br><b>ELI5:</b> Like choosing how many neighbors to ask for advice—too few might be unreliable, too many might be too generic.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the role of the test set in kNN,The test set is used only once to evaluate the final model's performance after selecting the best hyperparameters via cross-validation.<br><br><b>ELI5:</b> Like the final exam that tests how well you learned after practicing with study guides.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is overfitting in K-NN,"Overfitting occurs when the model fits the training data too closely, resulting in low training error but high validation error. This typically happens when k is too small, causing the model to be overly sensitive to noise in the training data.<br><br><b>ELI5:</b> Imagine memorizing every detail of a single book but failing to understand the general plot—you can't summarize it for someone else.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is underfitting in K-NN,"Underfitting occurs when the model is too simple to capture the underlying pattern, resulting in high training and validation error. This typically happens when k is too large, making the model too rigid.<br><br><b>ELI5:</b> Imagine trying to describe a complex movie with just one word—you miss all the important details.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does k affect K-NN performance,"Small k: Low bias, high variance (overfitting). Large k: High bias, low variance (underfitting). Optimal k balances training and validation error.<br><br><b>ELI5:</b> A small k is like asking only your closest friends for advice (too specific), while a large k is like asking everyone in the city (too general).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How to detect overfitting in K-NN,"Plot training and validation error vs. k. Overfitting is indicated when training error is low but validation error is high.<br><br><b>ELI5:</b> If you ace your homework but fail the exam, you probably memorized instead of learning.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How to detect underfitting in K-NN,"Plot training and validation error vs. k. Underfitting is indicated when both training and validation errors are high.<br><br><b>ELI5:</b> If you fail both homework and the exam, you didn’t study enough.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is time-based splitting,"Time-based splitting sorts data by timestamp and assigns earlier data to training, middle to validation, and latest to testing. This prevents future data from influencing past predictions.<br><br><b>ELI5:</b> It’s like studying history in order—you don’t use future events to explain the past.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
Why use time-based splitting for reviews,"Reviews and products evolve over time. Time-based splitting ensures the model is tested on future data, not past data, avoiding unrealistic predictions.<br><br><b>ELI5:</b> Predicting 2020 trends using 2025 data is like cheating on a test with the answer key.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does K-NN work for regression,"For regression, K-NN predicts the output as the mean or median of the k nearest neighbors' y-values. Unlike classification, which uses majority vote, regression averages continuous values.<br><br><b>ELI5:</b> Instead of voting for the most popular answer (classification), you average all your neighbors' opinions (regression).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is weighted k-NN,"Weighted k-NN assigns weights to nearest neighbors based on their distance. Closer points have higher weights, and labels are multiplied by the reciprocal of the distance instead of using a simple majority vote.<br><br><b>ELI5:</b> Imagine voting where your closest friends' opinions count more than distant acquaintances'—their influence is stronger because they're nearer to you.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does weighted k-NN work,"Instead of a simple majority vote, weighted k-NN multiplies each neighbor's label by the reciprocal of its distance. This emphasizes closer neighbors more strongly in the final prediction.<br><br><b>ELI5:</b> Like giving louder microphones to people standing closer to you—their voices dominate the decision.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is a Voronoi diagram,A Voronoi diagram partitions the data space into regions where each region contains all points closest to a specific neighbor. It visually represents the decision boundaries of k-NN when k=1.<br><br><b>ELI5:</b> Imagine drawing lines between you and your friends so every spot on the map is assigned to the nearest friend—like claiming territory.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does a kd-tree improve k-NN,"A kd-tree reduces k-NN's time complexity from O(n) to O(log n) by organizing data in a binary tree structure optimized for nearest-neighbor searches in low-dimensional spaces.<br><br><b>ELI5:</b> Instead of checking every person in a crowd, you use a phonebook to quickly find the closest match.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does a binary search tree work,"A binary search tree (BST) organizes sorted data so that each comparison halves the search space, achieving O(log n) time complexity for lookups. The depth of a balanced BST is O(log n).<br><br><b>ELI5:</b> Like guessing a number between 1-100 by asking 'higher or lower?'—each guess cuts the problem in half.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is a kd-tree,"A kd-tree is a space-partitioning data structure that recursively divides the data space using axis-parallel hyperplanes. It organizes points in a k-dimensional space by alternating the splitting axis at each level.<br><br><b>ELI5:</b> Imagine a tree that splits a room into smaller rooms by drawing lines parallel to the walls, alternating between length and width at each split.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How is a kd-tree constructed,"1. Pick an axis (e.g., x-axis) and project all points onto it.<br>2. Choose the median point and draw a hyperplane through it, splitting the space into two halves.<br>3. Recursively repeat the process on each half, alternating the splitting axis (e.g., y-axis next).<br><br><b>ELI5:</b> Like sorting a deck of cards by alternating between sorting by suit and then by number at each step.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does a kd-tree find nearest neighbors,"1. Traverse the tree to find the closest point in the leaf node.<br>2. Draw a hypersphere around the query point with radius equal to the distance to the closest point.<br>3. Backtrack to check if the hypersphere intersects any splitting planes, indicating potential closer points in other branches.<br><br><b>ELI5:</b> Like searching for the nearest gas station by first checking the closest one, then expanding your search radius if a better option might exist just around the corner.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the time complexity of nearest neighbor search in a kd-tree,"Best case: O(log n)<br>Worst case: O(n)<br><br><b>ELI5:</b> In the best case, it's like finding a name in a phonebook (fast). In the worst case, it's like searching every page (slow).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does dimensionality affect kd-tree performance,"As dimensionality increases, the number of backtracks grows exponentially (O(2^d)), worsening time complexity. High dimensions make the tree less efficient due to the 'curse of dimensionality.'<br><br><b>ELI5:</b> Like trying to organize a library with too many categories—eventually, it becomes harder to find anything quickly.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does data distribution affect kd-tree performance,"For uniform distributions, the time complexity is O(log n). For clustered or real-world distributions, it degrades toward O(n).<br><br><b>ELI5:</b> Like searching for a book in a neatly organized library (fast) vs. a messy one (slow).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the time complexity for k-nearest neighbors in a kd-tree,"Best case: O(k log n)<br>Worst case: O(kn)<br><br><b>ELI5:</b> Finding one neighbor is fast, but finding multiple neighbors takes longer as you check more options.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is locality sensitive hashing,A hashing technique that maps similar data points to the same bucket with high probability. It reduces search space by grouping nearby points together.<br><br><b>ELI5:</b> Like sorting books by genre so you only check the mystery section for mystery books.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does LSH work for nearest neighbors,"It uses randomized hash functions to place similar points in the same bucket. For a query, only the relevant bucket is searched instead of the entire dataset.<br><br><b>ELI5:</b> Like putting all red marbles in one box so you only search that box for red marbles.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does cosine similarity relate to angular distance,"Cosine similarity decreases as angular distance increases. When θ = 0°, cos(θ) = 1 (identical vectors). When θ = 90°, cos(θ) = 0 (orthogonal vectors).<br><br><b>ELI5:</b> Like two arrows pointing in the same direction are more similar than arrows pointing in different directions.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does LSH use random hyperplanes,"It generates random hyperplanes with normal vectors w_i ~ N(0,1). For each point x, it computes sign(w_i^T x) to create a binary hash key. Points with the same key go to the same bucket.<br><br><b>ELI5:</b> Like drawing random lines on a map and grouping cities that fall on the same side of each line.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the time complexity of LSH construction,"O(mdn) where m is the number of hyperplanes, d is dimensionality, and n is the number of data points.<br><br><b>ELI5:</b> Like painting m walls in a d-dimensional room with n colors.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is the time complexity of LSH query,O(md + n'd) where n' is the number of points in the bucket. m is typically set to log(n).<br><br><b>ELI5:</b> Like checking m doors and then searching through n' items in the correct room.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does m affect LSH performance,Higher m increases collision probability for similar points but also increases computation. Lower m may miss nearest neighbors.<br><br><b>ELI5:</b> Like using more filters to catch fish but risking catching too many or too few.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
Why might LSH miss nearest neighbors,"If a nearest neighbor falls on the opposite side of any hyperplane, it may be placed in a different bucket and not found during query.<br><br><b>ELI5:</b> Like two friends sitting on opposite sides of a fence and not being grouped together.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is LSH for Euclidean distance,LSH for Euclidean distance divides each axis into units and projects data points onto these axes. Each point is hashed into a vector where each cell represents the segment of the axis the point lies on.<br><br><b>ELI5:</b> Imagine a grid where each point is assigned a unique address based on which grid lines it falls between.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does LSH for cosine similarity work,"LSH for cosine similarity encodes each feature as either +1 or -1, creating a binary hash vector.<br><br><b>ELI5:</b> Think of it as flipping a coin for each feature to decide if it's +1 or -1, then using that sequence as a fingerprint.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does k affect the decision boundary,"As k increases, the decision boundary becomes smoother because the model averages over more neighbors, reducing variance but potentially increasing bias.<br><br><b>ELI5:</b> With fewer neighbors, the boundary is jagged like a puzzle piece. With more neighbors, it smooths out like a blurred photo.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is probabilistic class label in KNN,"Probabilistic class label quantifies prediction confidence by reporting the proportion of neighbors belonging to each class (e.g., 4/7 positive vs. 7/7 positive).<br><br><b>ELI5:</b> Instead of just saying 'yes' or 'no,' it says 'yes, but only 57% sure' or 'yes, 100% sure.'",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is cross-validation,A technique to evaluate model performance by partitioning data into training and test sets to avoid overfitting.<br><br><b>ELI5:</b> Like studying from a textbook and then taking a quiz on unseen material to check if you really learned.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
How does train-test split work,"The dataset is divided into two subsets: one for training the model and another for testing its performance. Typically, 70-80% is used for training and 20-30% for testing.<br><br><b>ELI5:</b> Like practicing basketball with most of your time spent training and a small portion reserved for a game to see how well you perform.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_and_regression_models_k_nearest_neighbors
What is K-Nearest Neighbors,K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that classifies or predicts based on the majority vote or average of the k nearest data points in the feature space.<br><br><b>ELI5:</b> Imagine you're at a party and want to guess someone's favorite music genre. You ask the 5 people closest to them (k=5) and go with the most popular answer.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does KNN work,"KNN works by calculating the distance between the query point and all training points, selecting the k nearest neighbors, and aggregating their labels (classification) or values (regression).<br><br><b>ELI5:</b> Like finding the closest friends in a room to decide what game to play next.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Euclidean distance,"Euclidean distance (L2 norm) is the straight-line distance between two points in Euclidean space, calculated as the square root of the sum of squared differences between coordinates.<br><br><b>ELI5:</b> The shortest path between two points, like measuring with a ruler.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Manhattan distance,"Manhattan distance (L1 norm) is the sum of absolute differences between coordinates, representing grid-like movement.<br><br><b>ELI5:</b> Like walking city blocks where you can only move north-south or east-west.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Cosine Similarity,"Cosine similarity measures the angle between two vectors, ignoring magnitude. It is calculated as the dot product divided by the product of magnitudes.<br><br><b>ELI5:</b> Like comparing two arrows' directions, not their lengths.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does k affect KNN,"Small k increases variance (overfitting), large k increases bias (underfitting). Optimal k balances the trade-off.<br><br><b>ELI5:</b> Asking too few friends (small k) gives noisy answers, asking too many (large k) dilutes the opinion.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is K-fold cross validation,"K-fold cross validation splits data into k equal parts, trains on k-1 folds, and validates on the remaining fold, repeating k times.<br><br><b>ELI5:</b> Like testing a recipe by cooking it k times, each time leaving out one ingredient to see how it affects the dish.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Time-based splitting,"Time-based splitting divides data chronologically, training on past data and validating on future data to preserve temporal order.<br><br><b>ELI5:</b> Like predicting tomorrow's weather using only past weather data, not future data.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a kd-tree,"A kd-tree is a space-partitioning data structure for organizing points in k-dimensional space, enabling efficient nearest neighbor searches.<br><br><b>ELI5:</b> Like a phone book that splits names by first letter, then second letter, etc., to find contacts faster.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does a kd-tree find nearest neighbors,"A kd-tree recursively partitions space along alternating axes, pruning branches that cannot contain closer points than the current best candidate.<br><br><b>ELI5:</b> Like searching a library by narrowing down sections, shelves, and books to find the closest match.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Locality Sensitive Hashing,Locality Sensitive Hashing (LSH) is a technique to approximate nearest neighbors by hashing similar items to the same bucket with high probability.<br><br><b>ELI5:</b> Like grouping similar books by color so you can quickly find books with similar covers.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does LSH work for cosine similarity,"LSH for cosine similarity uses random hyperplanes to hash vectors into buckets, where vectors with similar angles are likely to collide.<br><br><b>ELI5:</b> Like tossing arrows into bins based on their direction, so arrows pointing similarly land in the same bin.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Accuracy,Accuracy is the proportion of correct predictions (true positives + true negatives) over total predictions.<br><br><b>ELI5:</b> Like counting how many questions you got right on a test out of all questions.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is a Confusion Matrix,"A confusion matrix is a table summarizing classification performance with true positives, true negatives, false positives, and false negatives.<br><br><b>ELI5:</b> Like a scorecard showing how many times you guessed correctly or incorrectly.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Precision and Recall,Precision is the ratio of true positives to all predicted positives. Recall is the ratio of true positives to all actual positives.<br><br><b>ELI5:</b> Precision: How many of your 'yes' answers were correct. Recall: How many correct 'yes' answers you found.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the ROC Curve,The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds.<br><br><b>ELI5:</b> Like adjusting a spam filter's sensitivity and seeing how many real emails get caught vs. how many spams slip through.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is AUC,"AUC (Area Under the ROC Curve) measures the overall performance of a classifier across all thresholds, with 1.0 being perfect.<br><br><b>ELI5:</b> Like grading a test where 100% means you got every question right, no matter the difficulty.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Conditional Probability,"Conditional probability is the probability of an event occurring given that another event has already occurred, denoted as P(A|B).<br><br><b>ELI5:</b> Like the chance it's raining given that you see dark clouds.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Bayes Theorem,Bayes Theorem updates the probability of a hypothesis based on new evidence: P(A|B) = P(B|A)P(A)/P(B).<br><br><b>ELI5:</b> Like updating your guess about a disease after seeing test results.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Laplace Smoothing,"Laplace smoothing adds a small constant to all counts to prevent zero probabilities in Naive Bayes, especially for unseen data.<br><br><b>ELI5:</b> Like assuming every word has been seen at least once to avoid saying a word is impossible.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Imbalanced Data,"Imbalanced data occurs when one class significantly outnumbers others, causing classifiers to favor the majority class.<br><br><b>ELI5:</b> Like having 99 healthy patients and 1 sick patient, making it hard to detect the sick one.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How to handle Outliers,"Outliers can be handled by removal, transformation, or using robust algorithms like median-based methods or tree-based models.<br><br><b>ELI5:</b> Like ignoring a single extremely high score in a test to get a fair average.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Multiclass Classification,"Multiclass classification involves predicting one of three or more classes, often using techniques like one-vs-rest or softmax regression.<br><br><b>ELI5:</b> Like deciding whether an animal is a cat, dog, or bird, not just cat or not-cat.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is logistic regression,"Logistic regression models the probability of a binary outcome using a linear decision boundary in feature space. It maps the linear combination of features to a probability between 0 and 1 using the sigmoid function.<br><br>\[ P(y=1|x) = \frac{1}{1 + e^{-(w^T x + b)}} \]<br><br><b>ELI5:</b> Like a weighted voting system where each feature casts a vote, and the total votes are converted to a probability score between 0 and 1.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the sigmoid function and squashing,"The sigmoid function \( \sigma(z) = \frac{1}{1 + e^{-z}} \) squashes any real-valued input to the range (0, 1). Squashing compresses extreme values toward the boundaries while preserving the monotonic relationship.<br><br><b>ELI5:</b> Like a rubber band that stretches infinitely in the middle but always snaps back to fixed endpoints at 0 and 1.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the optimization objective in logistic regression,"Maximize the log-likelihood of observed data, equivalent to minimizing the negative log-likelihood (cross-entropy loss).<br><br>\[ \min_w \sum_{i=1}^n \left[ y_i \log(\sigma(w^T x_i)) + (1-y_i) \log(1-\sigma(w^T x_i)) \right] \]<br><br><b>ELI5:</b> Adjust weights to make correct predictions have high probability and incorrect predictions have low probability.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does the weight vector affect logistic regression,The weight vector \( w \) determines the orientation and magnitude of the decision boundary. Larger magnitude weights create sharper transitions and higher confidence predictions. Each weight scales the influence of its corresponding feature.<br><br><b>ELI5:</b> Like adjusting the volume knobs for each ingredient in a recipe—higher volume means that ingredient matters more to the final taste.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does L2 regularization affect logistic regression,"L2 regularization adds \( \lambda \|w\|^2_2 \) penalty to the loss function. It shrinks all weights toward zero, reducing model complexity and preventing overfitting. All weights are reduced proportionally, maintaining feature relevance.<br><br>\[ \min_w \text{Loss} + \lambda \sum_{j=1}^d w_j^2 \]<br><br><b>ELI5:</b> Like adding a tax on large weights that forces the model to use many small contributions instead of relying heavily on any single feature.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How does L1 regularization create sparsity,"L1 regularization adds \( \lambda \|w\|_1 \) penalty which creates a diamond-shaped constraint region. At the optimum, many weights become exactly zero, performing automatic feature selection.<br><br>\[ \min_w \text{Loss} + \lambda \sum_{j=1}^d |w_j| \]<br><br><b>ELI5:</b> Like a budget that forces you to completely eliminate some expenses rather than reducing all expenses slightly.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the probabilistic interpretation of Gaussian Naive Bayes,"Gaussian Naive Bayes assumes features are conditionally independent given the class label, with each feature following a Gaussian distribution. The decision boundary is quadratic when classes have different variances, but linear when variances are equal.<br><br>\[ P(y|x) \propto P(y) \prod_{j=1}^d \mathcal{N}(x_j | \mu_{j,y}, \sigma_{j,y}^2) \]<br><br><b>ELI5:</b> Like classifying fruits by assuming weight and color are independent measurements, each following a bell curve for each fruit type.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the difference between grid search and random search,Grid search exhaustively evaluates all combinations in a predefined hyperparameter grid. Random search samples random combinations from distributions. Random search is more efficient when some hyperparameters have low impact on performance.<br><br><b>ELI5:</b> Grid search is like checking every house on a grid map. Random search is like randomly knocking on doors in a neighborhood.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is column standardization,Column standardization transforms each feature to have zero mean and unit variance: \( x' = \frac{x - \mu}{\sigma} \). This ensures features are on comparable scales and improves convergence of gradient-based optimization.<br><br><b>ELI5:</b> Like converting all measurements to a common ruler so no single feature dominates due to its units.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is feature collinearity,"Collinearity occurs when features are highly correlated, making the weight matrix nearly singular. This causes instability in weight estimates and poor generalization. The model cannot uniquely determine which feature is responsible for the effect.<br><br><b>ELI5:</b> Like having two witnesses who always say the same thing—you can't tell which one is actually providing the information.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is the time complexity of logistic regression,"Training: \( O(n \cdot d \cdot k) \) where \( n \) is samples, \( d \) is features, \( k \) is iterations. Prediction: \( O(d) \). Space complexity is \( O(d) \) for storing weights.<br><br><b>ELI5:</b> Training scales linearly with data size, features, and training passes. Prediction is just a quick weighted sum.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Why use Euclidean distance in k-means or kNN,"Euclidean distance is preferred because it preserves geometric intuition in isotropic spaces and penalizes larger deviations more heavily than Manhattan distance. It aligns with the L2 norm, which is differentiable and works well with gradient-based optimization.<br><br><b>ELI5:</b> Think of Euclidean distance as measuring 'as the crow flies'—it's the straight-line distance that naturally fits how we visualize clusters or neighbors in space.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to detect overfitting,Overfitting is detected when the model performs well on training data but poorly on unseen validation/test data. Key indicators include high training accuracy with low validation accuracy or a large gap between training and validation loss.<br><br><b>ELI5:</b> Like memorizing answers for a test but failing when questions are slightly rephrased—your model 'memorized' the training data instead of learning general patterns.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How does kNN differ from k-means,"kNN is a supervised classification algorithm that assigns labels based on the majority vote of k nearest neighbors. k-means is an unsupervised clustering algorithm that partitions data into k clusters by minimizing within-cluster variance. kNN uses labeled data; k-means does not.<br><br><b>ELI5:</b> kNN is like asking your neighbors for advice (they have labels), while k-means is grouping people by how close they stand together (no labels needed).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Difference between test set and validation set,"The validation set is used during model development to tune hyperparameters and evaluate performance iteratively. The test set is used only once at the end to assess the final model's generalization performance. Both are held-out subsets of the data.<br><br><b>ELI5:</b> Validation is your practice exam (adjust study methods), test is the final exam (grade your performance).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to avoid overfitting in kNN,"Avoid overfitting in kNN by increasing k (reduces variance but may increase bias), using cross-validation to select k, or applying feature selection/dimensionality reduction to remove noise. Larger k smooths decision boundaries.<br><br><b>ELI5:</b> Asking more neighbors (higher k) averages out noisy opinions, making decisions more stable.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
When is false positive more important than false negative,"False positives are prioritized in scenarios where incorrect alarms are costly or harmful, e.g., spam detection (flagging legitimate emails as spam) or medical screening (unnecessary treatments).<br><br><b>ELI5:</b> Better to double-check a 'false alarm' fire drill than miss a real fire.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
When is false negative more important than false positive,"False negatives are critical when missing a positive case has severe consequences, e.g., cancer diagnosis (missing a tumor) or fraud detection (missing fraudulent transactions).<br><br><b>ELI5:</b> Missing a single bomb at airport security is worse than a few extra bag checks.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
When are false positives and negatives equally important,"Both errors are equally important when their costs are balanced, e.g., email classification (misclassifying work vs personal) or product recommendations (wrong suggestions hurt engagement equally).<br><br><b>ELI5:</b> Like guessing heads/tails—wrong either way is equally bad.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Most frequent metric for classification accuracy,"Accuracy is the most common metric, calculated as (TP + TN) / (TP + TN + FP + FN). However, it can be misleading for imbalanced datasets.<br><br><b>ELI5:</b> Percentage of correct guesses, but useless if 99% of data is one class.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Why is AUROC better than raw accuracy,"AUROC measures the model's ability to distinguish classes across all thresholds, invariant to class imbalance. It evaluates the trade-off between true positive rate (TPR) and false positive rate (FPR) via the ROC curve.<br><br><b>ELI5:</b> Like judging a chef by how well they balance flavors (sweet vs spicy) at every bite, not just one dish.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to check for multicollinearity,"Check multicollinearity using Variance Inflation Factor (VIF) > 5–10 or correlation matrices. If detected, use PCA, ridge regression, or remove redundant features to stabilize coefficients without losing information.<br><br><b>ELI5:</b> Like two friends always giving the same advice—you only need one to avoid confusion.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Basic assumptions for linear regression,"Assumptions: 1) Linear relationship between features and target, 2) No multicollinearity, 3) Homoscedasticity (constant error variance), 4) Normality of residuals, 5) Independence of errors.<br><br><b>ELI5:</b> Like assuming a straight road (linear), no detours (multicollinearity), and consistent speed limits (homoscedasticity).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Difference between SGD and gradient descent,"Gradient Descent (GD) uses the entire dataset for each update, while Stochastic Gradient Descent (SGD) uses one random sample per update. SGD is faster but noisier; GD is precise but computationally expensive.<br><br><b>ELI5:</b> GD is reading the whole book before adjusting; SGD is adjusting after each sentence.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
When to use GD over SGD,"Use GD for small datasets where computational cost is low and precise convergence is needed. Use SGD for large datasets or online learning where speed and scalability matter more than exactness.<br><br><b>ELI5:</b> GD for a tiny garden (careful pruning), SGD for a forest (quick trimming).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to assess linear regression fit,"Assess fit using R-squared (proportion of variance explained), residual analysis (patterns indicate poor fit), and hypothesis tests (F-test for significance, t-tests for coefficients).<br><br><b>ELI5:</b> Like checking if a glove fits: R-squared is how much hand it covers, residuals are wrinkles, and tests confirm it's not just luck.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Why isn't logistic regression called logistic classification,"Logistic regression models the probability of a binary outcome using a logistic function (sigmoid), but it's a classification algorithm. The name 'regression' stems from its statistical roots in modeling probabilities.<br><br><b>ELI5:</b> It's like calling a light switch 'probability dial'—it predicts on/off but uses a smooth curve.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is a balanced dataset,"A dataset where the number of positive and negative points are approximately equal (e.g., n1:n2 = 58:42).<br><br><b>ELI5:</b> Like a fair coin toss where heads and tails are almost equal.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is an imbalanced dataset,"A dataset where one class significantly outnumbers the other (e.g., n1:n2 = 5:95). Models trained on such data may favor the majority class.<br><br><b>ELI5:</b> Like a rigged coin that lands on heads 95% of the time.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does undersampling work,"Reduces the majority class to match the minority class size by randomly discarding data points. Discards 80% of the dataset in the example (n1=100, n2=900 → n1=100, n2=100).<br><br><b>ELI5:</b> Like throwing away most of the apples to match the number of oranges.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does oversampling work,"Increases the minority class by repeating or synthesizing points (e.g., n1=100 → 900). Preserves all data but may overfit. Can also use class weights to emphasize the minority class.<br><br><b>ELI5:</b> Like photocopying the oranges to match the number of apples.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is the One vs Rest technique,"Converts a multi-class problem into multiple binary classifiers. For c classes, train c classifiers where each distinguishes one class from all others.<br><br><b>ELI5:</b> Like training a separate guard dog for each type of intruder.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does kNN work with a similarity matrix,"Converts similarity to distance (e.g., distance = 1/similarity) and applies kNN using these distances. Works even when data cannot be vectorized.<br><br><b>ELI5:</b> Like finding neighbors based on how similar they are, not just their coordinates.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does imbalanced data affect model accuracy,"A dumb model predicting the majority class can achieve high accuracy (e.g., 95% if n1:n2 = 5:95) but fails to generalize.<br><br><b>ELI5:</b> Like always guessing 'heads' in a rigged coin toss and being right most of the time.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does kNN extend to multi-class classification,Uses majority voting among the k nearest neighbors. No modification needed for multi-class problems.<br><br><b>ELI5:</b> Like asking your k closest friends to vote on the answer.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is train-test distribution drift,"When the distribution of train and test data changes over time, causing high test error despite low train/CV error. This can happen due to new/removed products or temporal shifts.<br><br><b>ELI5:</b> Like studying for an exam using old questions, but the actual test has completely different topics.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How to detect train-test distribution mismatch,"Train a binary classifier to distinguish between train (label=1) and test (label=0) data. High accuracy indicates dissimilar distributions; low accuracy suggests similar distributions.<br><br><b>ELI5:</b> If a simple test can easily tell your training data apart from test data, they're probably too different.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does k affect kNN sensitivity to outliers,"Small k makes kNN highly sensitive to outliers, creating unstable decision boundaries. Larger k smooths the decision surface by averaging more neighbors.<br><br><b>ELI5:</b> Asking 1 neighbor for directions might get you lost if they're confused, but asking 10 neighbors gives a more reliable answer.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does Local Outlier Factor detect outliers,"For each point, compute the average distance to its k nearest neighbors. Points with large average distances are global outliers. Local density comparisons identify local outliers within clusters.<br><br><b>ELI5:</b> Like finding people who are far from their friends (global outliers) or who have very few friends in their neighborhood (local outliers).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does outlier removal improve kNN,"Removing outliers stabilizes decision boundaries, especially for small k values. This reduces variance in predictions without increasing bias.<br><br><b>ELI5:</b> Removing confusing signposts makes navigation more reliable, especially when you're only looking at nearby signs.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is K-distance,K-distance(xi) is the distance of the kth nearest neighbor of xi from xi.<br><br><b>ELI5:</b> Imagine you're in a crowd; K-distance is how far the kth closest person is from you.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is reachability-distance,"Reachability-distance(xi, xj) = max(k-distance(xj), dist(xi, xj)).<br><br>If xi is in N(xj), it equals k-distance(xj); else, it equals dist(xi, xj).<br><br><b>ELI5:</b> It's like measuring how far you can reach to touch someone, considering their own personal space.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is local reachability-density,LRD(xi) = 1 / (average reachability-distance of xi from its neighbors).<br><br>It measures how dense the neighborhood around xi is.<br><br><b>ELI5:</b> Think of it as how crowded your immediate area is; higher LRD means more people are close by.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does K-distance relate to reachability-distance,"K-distance defines the boundary for reachability-distance. If a point is within the k-neighborhood, reachability-distance equals k-distance; otherwise, it's the actual distance.<br><br><b>ELI5:</b> K-distance sets a 'personal space' radius; reachability-distance adjusts based on whether you're inside or outside that space.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does reachability-distance affect local reachability-density,"Local reachability-density is the inverse of the average reachability-distance. Lower reachability-distances increase LRD, indicating denser neighborhoods.<br><br><b>ELI5:</b> If people are closer (lower reachability-distance), your area feels more crowded (higher LRD).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is Local Outlier Factor (LOF)?,"LOF measures the local density deviation of a point relative to its neighbors. It is defined as the average local reachability density (lrd) of neighbors divided by the lrd of the point itself. A high LOF indicates an outlier.<br><br><b>ELI5:</b> Imagine a crowded room where most people are close together. If someone is standing far away from everyone else, they are an outlier. LOF calculates how isolated a point is compared to its neighbors.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does LOF identify outliers?,"LOF identifies outliers by comparing the local density of a point to its neighbors. If the point's density is significantly lower than its neighbors, LOF is large, indicating an outlier.<br><br><b>ELI5:</b> Think of a traffic jam where most cars are close together. If one car is far away from the others, it stands out as unusual.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
Why is column standardization important for distance measures?,"Column standardization ensures that features with different scales do not dominate distance calculations. Without it, features with larger scales can disproportionately influence results.<br><br><b>ELI5:</b> Imagine comparing two people's heights in inches and centimeters. Without converting to the same unit, the comparison is meaningless.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does scale affect Euclidean distance?,"Euclidean distance is sensitive to scale. Features with larger scales can make distances appear larger even if the relative difference is small.<br><br><b>ELI5:</b> Comparing a 5-inch difference in height to a 0.8-inch difference in shoe size. The height difference seems larger, but it might not be as significant in context.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is an interpretable model?,"An interpretable model provides predictions along with explanations for why those predictions were made. This makes decisions reliable and understandable.<br><br><b>ELI5:</b> A doctor uses test results to explain a diagnosis, not just saying 'you have cancer' without any reasoning.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does kNN provide interpretability?,"kNN is interpretable because it shows the nearest data points used for prediction. This allows users to understand the basis of the prediction.<br><br><b>ELI5:</b> If you want to know if a new fruit is an apple, you compare it to known apples and oranges. Seeing the closest matches helps you decide.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is a black box model?,A black box model provides predictions without explaining how or why those predictions were made. This lack of transparency can make decisions unreliable.<br><br><b>ELI5:</b> A magic 8-ball gives answers but doesn't explain how it knows. You can't trust it for important decisions.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is feature importance,"Features that significantly contribute to a model's predictive performance, improving interpretability and understanding of the model's decision-making process.<br><br><b>ELI5:</b> Like highlighting the most useful ingredients in a recipe to understand why the dish tastes good.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does forward feature selection work,"1. Train model with each feature individually, select best-performing feature (fs1).<br>2. Retrain with fs1 + each remaining feature, select best combo (fs2).<br>3. Repeat until all features are evaluated or performance plateaus.<br><br><b>ELI5:</b> Like building a team by adding one player at a time, always picking the one who improves the team the most.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
Why is kNN not suitable for feature importance,kNN relies on distance metrics across all features equally and does not inherently rank or weight features by their predictive contribution.<br><br><b>ELI5:</b> Like judging a book by its cover without knowing which parts of the cover matter most.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does one-hot encoding solve ordinal issues,"Converts categorical values into binary columns (one per category), avoiding artificial ordinal relationships while increasing dimensionality.<br><br><b>ELI5:</b> Like turning a multiple-choice question into a series of yes/no questions, one for each option.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is the trade-off of one-hot encoding,"Eliminates ordinal bias but increases dimensionality, creating sparse datasets where most values are zero.<br><br><b>ELI5:</b> Like giving every student their own locker—organized but takes up a lot of space.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does mean replacement use domain knowledge,"Replaces categorical values (e.g., country) with domain-relevant numerical proxies, such as geographic coordinates or derived metrics (e.g., average height).<br><br><b>ELI5:</b> Like replacing 'France' with its latitude to hint at climate-related patterns.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does class-based imputation work,"Fills missing values using statistics (mean/median/mode) computed separately for each class label, preserving class-specific distributions.<br><br><b>ELI5:</b> Like filling in a student's missing test score using the average of their classmates in the same grade.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is the purpose of a missing value indicator,"Adds a binary feature flagging whether a value was originally missing, allowing the model to learn patterns associated with missingness itself.<br><br><b>ELI5:</b> Like adding a 'data missing' sticker to a file so the system knows to handle it differently.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does model-based imputation use kNN,"Treats missing values as a prediction task: uses non-missing features to train a kNN model, then predicts missing values based on similar instances (neighbors).<br><br><b>ELI5:</b> Like asking your neighbors to guess your age based on who you hang out with.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does forward feature selection relate to PCA,"Forward selection optimizes for classification performance by iteratively adding features, while PCA reduces dimensionality by maximizing variance without considering class labels.<br><br><b>ELI5:</b> PCA picks the most 'spread-out' features; forward selection picks the most 'useful' ones for the task.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is the best case for kNN,"kNN performs well when dimensionality is small and the right distance measure is known. It excels in similarity or distance matrix applications.<br><br><b>ELI5:</b> kNN works best when data has few features and you can measure distances accurately, like finding similar books in a small library.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is the worst case for kNN,"kNN struggles with high dimensionality due to the curse of dimensionality, high runtime complexity, and lack of interpretability. It is unsuitable for low-latency applications.<br><br><b>ELI5:</b> kNN fails when data has too many features, like trying to find similar books in a library with millions of categories.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does α affect Naïve Bayes,"When α = 0, small changes in training data cause large model changes (high variance, overfitting). When α is very large, all likelihoods become 0.5 (high bias, underfitting).<br><br><b>ELI5:</b> α acts like a stabilizer: too low makes the model overly sensitive, too high makes it ignore data details.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How are k and α determined,"Both k (for kNN) and α (for Naïve Bayes) are determined using cross-validation to balance bias and variance.<br><br><b>ELI5:</b> We test different values of k and α to find the best fit, like trying different keys to unlock a door.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How are missing values handled in text data,"Text data has no missing values. For categorical data, NaN is treated as another category. For numerical data, imputation is used.<br><br><b>ELI5:</b> Text data is complete, but missing categories are treated as a new label, and missing numbers are filled in.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is kNN's requirement for implementation,kNN requires a distance matrix to measure similarity between data points.<br><br><b>ELI5:</b> Like measuring how close neighbors are on a map to find similar houses.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
Why can't Naïve Bayes use distance matrices,"Naïve Bayes requires probability values, not distances, as it models feature independence via probabilities.<br><br><b>ELI5:</b> Like needing percentages (probabilities) instead of just distances to predict outcomes.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does conditional independence affect Naïve Bayes,"If features are not conditionally independent, Naïve Bayes performance deteriorates due to incorrect assumptions.<br><br><b>ELI5:</b> Like assuming two symptoms of a disease are unrelated when they actually influence each other.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
Why is Naïve Bayes used for text classification,"Naïve Bayes works well with categorical features and is interpretable, with low computational complexity.<br><br><b>ELI5:</b> Like counting word frequencies in emails to classify spam vs. not spam.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does Naïve Bayes handle real-valued features,Naïve Bayes struggles with real-valued features as real-world distributions often deviate from Gaussian assumptions.<br><br><b>ELI5:</b> Like trying to fit a bell curve to data that doesn't follow a normal pattern.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does Laplace smoothing help Naïve Bayes,"Laplace smoothing prevents overfitting by adding a small constant to zero probabilities, ensuring no feature is ignored.<br><br><b>ELI5:</b> Like adding a tiny buffer to avoid division by zero in calculations.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
What is accuracy in ML,"Accuracy is the ratio of correctly classified points to total points. It fails for imbalanced datasets and cannot distinguish between models with near-0.5 probability predictions.<br><br><b>ELI5:</b> Like grading a test where most answers are 'A'—a dumb model guessing 'A' always gets high marks, but it's not actually smart.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How does a confusion matrix work,"A confusion matrix maps actual vs predicted labels. For binary classification, it's a 2x2 matrix with True Negatives (a), False Positives (c), False Negatives (b), and True Positives (d). A good model has high diagonal values.<br><br><b>ELI5:</b> Like a scorecard comparing your guesses (predicted) to the real answers (actual).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What are TPR and FPR,TPR (True Positive Rate) = TP / (TP + FN). FPR (False Positive Rate) = FP / (FP + TN). High TPR and low FPR indicate a good model.<br><br><b>ELI5:</b> TPR is catching all the bad guys (high is good). FPR is arresting innocents (low is good).,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How do precision and recall differ,"Precision = TP / (TP + FP): Of predicted positives, how many are correct. Recall = TP / (TP + FN): Of actual positives, how many are caught.<br><br><b>ELI5:</b> Precision is 'how many of my catches are fish?' Recall is 'how many fish did I catch?'",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is the F1-score,"F1-score is the harmonic mean of precision and recall: 2 * (Precision * Recall) / (Precision + Recall). It balances both metrics.<br><br><b>ELI5:</b> Like a report card that averages your math and science grades, but punishes imbalance (e.g., 100 in one, 0 in the other).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
Why is accuracy misleading for imbalanced data,A dumb model can achieve high accuracy by always predicting the majority class. It ignores minority class performance.<br><br><b>ELI5:</b> Like a weather app that always says 'sunny'—it's right 90% of the time but useless on rainy days.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How does probability confidence affect accuracy,"Accuracy treats all predictions equally, even if one model predicts 0.51 (low confidence) and another 0.99 (high confidence). Both may be 'correct' but the latter is better.<br><br><b>ELI5:</b> Like two students guessing 'B'—one is sure, the other just lucky. Accuracy can't tell the difference.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
When is TPR prioritized over TNR,"In domains like medicine, high TPR (low FNR) is critical to avoid missing true positives (e.g., diseases). False positives (low TNR) are less harmful.<br><br><b>ELI5:</b> Better to test 10 healthy people than miss one sick person.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is ROC curve,A plot of True Positive Rate vs False Positive Rate across all possible classification thresholds derived from predicted probabilities.<br><br>\[ \text{TPR} = \frac{TP}{TP + FN} \]<br>\[ \text{FPR} = \frac{FP}{FP + TN} \]<br><br><b>ELI5:</b> A graph showing the trade-off between catching all true positives and avoiding false alarms.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What does AUC measure,"Area Under the ROC Curve. Ranges from 0 to 1, where 1 is perfect separation and 0.5 is random guessing.<br><br>AUC depends only on the ranking order of predictions, not their absolute values. Models with identical prediction orderings have the same AUC.<br><br>AUC = 0.5 for random models. AUC < 0.5 implies predictions are reversed (1 - AUC gives the correct score).<br><br><b>ELI5:</b> Probability that the model ranks a random positive instance higher than a random negative one.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How does AUC behave with imbalanced data,"AUC can remain high even for trivial models when classes are imbalanced because it uses ranking, not absolute probabilities.<br><br>\[ \text{AUC} \text{ is independent of class distribution} \]<br><br><b>ELI5:</b> AUC can be misleadingly high on imbalanced data because it only cares about ordering, not actual prediction quality.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is Log Loss,Binary: \[ \text{Log Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right] \]<br>where \( y_i \) is the true label (0 or 1) and \( p_i \) is the predicted probability.<br><br>Penalizes confident wrong predictions heavily.<br><br><b>ELI5:</b> Punishes the model more for being confidently wrong than for being uncertain.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is R-squared coefficient,"Proportion of variance explained by the model.<br><br>\[ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} \]<br>\[ SS_{res} = \sum (y_i - \hat{y}_i)^2 \]<br>\[ SS_{tot} = \sum (y_i - \bar{y})^2 \]<br><br>\( SS_{res} \) is sum of squared residuals, \( SS_{tot} \) is sum of squared errors of the mean model.<br><br><b>ELI5:</b> How much better your model is compared to just guessing the average.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How to interpret R-squared values,"R² = 1: Perfect fit (SS_res = 0)<br>R² ∈ [0, 1]: Better than mean model (SS_res < SS_tot)<br>R² = 0: Equivalent to mean model (SS_res = SS_tot)<br>R² < 0: Worse than mean model (SS_res > SS_tot)<br><br><b>ELI5:</b> Negative R² means your model is worse than just predicting the average every time.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is the distribution of errors in models,"Errors (ei) follow a distribution where few errors are large. The PDF shows error frequency, and the CDF shows cumulative error percentage. A model with a higher CDF is better.<br><br><b>ELI5:</b> Imagine errors as raindrops—most are small, but a few are big. The CDF is like a bucket filling up; the faster it fills, the better the model.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
How does CDF compare two models,"If model M1's CDF is above M2's CDF, M1 has fewer large errors and is better. The CDF shows the percentage of data points with errors below a threshold.<br><br><b>ELI5:</b> Think of two runners on a track. The one ahead (higher CDF) finishes faster, meaning fewer mistakes.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::performance_measurement_of_models
What is Median Absolute Deviation (MAD)?,"MAD is a robust measure of standard deviation calculated as the median of absolute deviations from the median of errors: Median(|ei – Median(ei)|). It is less sensitive to outliers compared to traditional standard deviation.<br><br><b>ELI5:</b> Imagine measuring how spread out your friends' heights are. Instead of using the average (which can be skewed by one very tall person), you find the middle height and then see how far each person is from that middle height, ignoring the direction (just the distance). Then you take the middle of those distances. This gives you a better sense of spread without being thrown off by outliers.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
How does MAD compare to standard deviation?,"MAD is robust to outliers because it uses the median instead of the mean, making it less sensitive to extreme values. Standard deviation (R²) is highly impacted by large errors, as it squares them (SSres = sum(ei²)).<br><br><b>ELI5:</b> If you have a group of people and one is a giant, the average height will be misleading, but the median height (middle person) won't be affected. MAD uses this idea to measure spread without being fooled by the giant.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
Why is MAD robust to outliers?,"MAD uses the median of absolute deviations, which is less influenced by extreme values. Squared errors (used in standard deviation) amplify outliers, while absolute deviations and medians dampen their effect.<br><br><b>ELI5:</b> Think of a seesaw. If one side has a heavy weight (outlier), the seesaw tips dramatically. MAD is like using a sturdy table instead—it stays balanced even with heavy weights.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::plotting_for_exploratory_data_analysis_eda
What is conditional probability,"The probability of an event A occurring given that event B has occurred, defined as P(A|B) = P(A ∩ B) / P(B).<br><br><b>ELI5:</b> Like calculating the chance of rolling a 2 on the first die if the sum of two dice is 5 or less.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Bayes' theorem work,It updates the probability of a hypothesis based on evidence: P(A|B) = P(B|A) * P(A) / P(B).<br><br><b>ELI5:</b> Like adjusting your guess about which machine made a defective part after seeing the defect rate.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What are independent events,Events where the occurrence of one does not affect the other: P(A|B) = P(A) and P(B|A) = P(B).<br><br><b>ELI5:</b> Like rolling two dice—one die's outcome doesn't influence the other.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What are mutually exclusive events,Events that cannot occur simultaneously: P(A ∩ B) = 0.<br><br><b>ELI5:</b> Like rolling a die and getting both a 1 and a 2 at the same time—impossible.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does prior probability affect Bayes' theorem,"The prior P(A) represents initial belief, while the posterior P(A|B) updates it with evidence. Higher prior increases posterior if evidence supports it.<br><br><b>ELI5:</b> If a machine rarely fails (low prior), seeing a defect slightly increases your belief it's from that machine.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What is the Naive Bayes assumption,Features are conditionally independent given the class label. This simplifies joint probability calculations by factoring them into products of individual probabilities.<br><br><b>ELI5:</b> Like assuming a student's math and English grades don't affect each other when predicting if they'll pass the year.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Naive Bayes work,"It calculates posterior probabilities using Bayes' theorem with conditional independence assumption. For a query point xq, it computes P(class|xq) ∝ P(xq|class) * P(class) for each class and picks the maximum.<br><br><b>ELI5:</b> Like a doctor diagnosing a disease by multiplying how likely your symptoms are for each possible disease with how common that disease is.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What are the time complexities of Naive Bayes,"Training: O(ndc) optimized to O(n) by counting frequencies once. Testing: O(dc) per prediction where d=features, c=classes.<br><br><b>ELI5:</b> Training is like counting all the words in a book once, while testing is like checking a few words against your counts.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Naive Bayes compare to kNN in space,Naive Bayes uses O(dc) space (only stores probability tables) while kNN uses O(nd) (stores all training data). This makes Naive Bayes more space-efficient at runtime.<br><br><b>ELI5:</b> Like carrying a cheat sheet (Naive Bayes) vs carrying all your textbooks (kNN) to take a test.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What determines the prediction in Naive Bayes,The class with the highest posterior probability P(class|x) is chosen. This is proportional to P(x|class)*P(class) since P(x) is constant for all classes.<br><br><b>ELI5:</b> Like betting on the horse that has both the best track record (P(class)) and performs best in current weather conditions (P(x|class)).,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What is feature importance in Naive Bayes,Features with high likelihood probabilities are most important for classification. Sorting these probabilities in descending order reveals the most influential features for each class.<br><br><b>ELI5:</b> Like ranking ingredients by how much they contribute to a recipe's flavor.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Naive Bayes handle imbalanced data,"Class priors favor the dominating class. Solutions include upsampling, downsampling, dropping priors, or using different Laplace smoothing (α) values per class.<br><br><b>ELI5:</b> Like giving extra weight to rare opinions in a poll to balance the results.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Laplace smoothing affect minority classes,Higher α values disproportionately impact minority classes by reducing their probability estimates less than majority classes.<br><br><b>ELI5:</b> Like adding extra votes to small groups to prevent their voices from being drowned out.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Gaussian Naive Bayes work,"Assumes numerical features follow a Gaussian distribution. For each class, compute mean and standard deviation from class-specific data points, then use the Gaussian PDF to estimate probabilities.<br><br><b>ELI5:</b> Like fitting a bell curve to each class's data to predict how likely a new point belongs.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What is the key assumption of Naive Bayes,"All features are conditionally independent given the class label. This simplifies computation but may not hold in practice.<br><br><b>ELI5:</b> Like assuming ingredients don't interact when making a dish, even though they do.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does feature distribution affect Naive Bayes variants,"Gaussian NB for numerical features, Bernoulli NB for binary features, and Multinomial NB for discrete counts. Each assumes a different underlying distribution.<br><br><b>ELI5:</b> Like using different tools (ruler, scale, counter) depending on what you're measuring.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What is multiclass classification in Naïve Bayes,"Naïve Bayes inherently supports multiclass classification by directly computing class-based probabilities.<br><br><b>ELI5:</b> Like sorting fruits into multiple baskets (apples, oranges, bananas) based on their features (color, shape).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
How does Naïve Bayes handle large dimensionality,"For high-dimensional data (e.g., text), Naïve Bayes uses log probabilities to avoid underflow and improve numerical stability.<br><br><b>ELI5:</b> Instead of multiplying tiny probabilities (which can vanish), it adds their logs (like counting on a ruler).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::naive_bayes
What is the geometric intuition of logistic regression,"Logistic regression finds a hyperplane (WTx + b = 0) that separates data points into two classes. The distance of a point from the hyperplane is di = WTxi/||W||. If W is a unit normal vector, di = WTxi. A point is classified as positive if WTxi > 0 and negative if WTxi < 0.<br><br><b>ELI5:</b> Imagine a line dividing two groups of points. Logistic regression finds the best line to separate them.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does logistic regression classify data points,"A data point is correctly classified if yi WTxi > 0, where yi is the true label (+1 or -1). The goal is to find W that maximizes the sum of yi WTxi, minimizing misclassifications.<br><br><b>ELI5:</b> Think of a seesaw. If the product of the label and distance is positive, the point is on the correct side.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the role of the sigmoid function in logistic regression,"The sigmoid function squashes the signed distance (yi WTxi) from [-∞, +∞] to [0, 1], making the model less sensitive to outliers. It is defined as sigmoid(x) = 1/(1 + e^-x).<br><br><b>ELI5:</b> Like a volume knob, it turns loud signals down to a manageable level.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does the sigmoid function affect the objective function,"The objective changes from maximizing the sum of signed distances to maximizing the sum of sigmoid of signed distances. This makes the model outlier-resistant.<br><br><b>ELI5:</b> Instead of counting how far points are, we count how confident we are they're on the right side.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the mathematical formulation of logistic regression's objective,"The objective is to minimize the sum of log(1 + exp(-yi WTxi)) for yi ∈ {-1, +1}. This is equivalent to maximizing the sum of yi WTxi but is more robust.<br><br><b>ELI5:</b> It's like finding the line that best splits the data while ignoring extreme outliers.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does the probabilistic interpretation of logistic regression work,"The probability pi that a point belongs to the positive class is given by pi = sigmoid(WTx). The objective is to minimize the negative log-likelihood: -yi log(pi) - (1 - yi) log(1 - pi) for yi ∈ {0, 1}.<br><br><b>ELI5:</b> It's like betting on the right side of the line, where the bet size depends on how sure you are.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does the sigmoid function help with optimization,"The sigmoid function is differentiable and provides a probabilistic interpretation, making it easier to solve the optimization problem using gradient-based methods.<br><br><b>ELI5:</b> It smooths out the problem, making it easier to find the best line step by step.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the weight vector in logistic regression,The weight vector W is a d-dimensional vector normal to the optimal hyperplane that separates data points into classes. Positive data points lie in the direction of W.<br><br><b>ELI5:</b> Imagine W as a line that best splits two groups of points on a graph.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does logistic regression make predictions,"For a query point xq, compute WTxq. If WTxq > 0, predict yq = +1; if WTxq < 0, predict yq = -1. The probability P(yq = +1) is given by the sigmoid function: sigmoid(WTxq).<br><br><b>ELI5:</b> The model checks which side of the line (hyperplane) the point is on and assigns a class based on that.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does a positive weight Wi affect predictions,"If Wi is positive, increasing xqi increases WTxq, which increases the sigmoid output and thus P(yq = +1).<br><br><b>ELI5:</b> A positive weight means the feature pushes the prediction toward the positive class as its value increases.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does a negative weight Wi affect predictions,"If Wi is negative, increasing xqi decreases WTxq, which decreases the sigmoid output and thus P(yq = +1), increasing P(yq = -1).<br><br><b>ELI5:</b> A negative weight means the feature pushes the prediction toward the negative class as its value increases.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the objective of L2 regularization,"L2 regularization adds a penalty term λ||W||2 to the loss function to prevent weights from becoming too large, reducing overfitting.<br><br><b>ELI5:</b> It’s like adding a spring to the weights to keep them from stretching too far.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does λ affect model performance,"If λ = 0, no regularization occurs, leading to overfitting (high variance). If λ is large, the model underfits (high bias) as the regularization term dominates.<br><br><b>ELI5:</b> λ acts like a dial: too low, the model memorizes noise; too high, it ignores the data.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does L1 regularization differ from L2,"L1 regularization uses λ||W||1, which induces sparsity by driving less important weights to zero. L2 regularization shrinks weights but keeps them non-zero.<br><br><b>ELI5:</b> L1 prunes unimportant features like cutting dead branches, while L2 just trims them.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is elastic net regularization,Elastic net combines L1 and L2 regularization: λ1||W||1 + λ2||W||2. It balances sparsity and smoothness.<br><br><b>ELI5:</b> It’s a mix of pruning (L1) and trimming (L2) to get the best of both worlds.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is Gaussian Naive Bayes,"A probabilistic model where real-valued features are Gaussian distributed and class labels are Bernoulli random variables. Assumes conditional independence of features given the class.<br><br><b>ELI5:</b> Imagine each feature (like height or weight) is a bell curve for each class, and the class itself is a coin flip.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does logistic regression relate to Gaussian Naive Bayes,Logistic regression can be derived as Gaussian Naive Bayes with a Bernoulli class label. It models P(Y=1|X) = 1/(1+exp(WTx)).<br><br><b>ELI5:</b> It's like combining the bell curves of features with a coin flip to predict probabilities.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the objective of logistic regression,"To minimize the logistic loss, which approximates the 0-1 loss (step loss) while being differentiable. The optimal weights are found via W* = argmin of the logistic loss.<br><br><b>ELI5:</b> It tries to reduce misclassifications smoothly, unlike a harsh step function.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does hinge loss relate to logistic loss,"Hinge loss is used in Support Vector Machines (SVMs), while logistic loss is used in logistic regression. Both approximate the 0-1 loss but with different geometric properties.<br><br><b>ELI5:</b> Hinge loss is like a ramp, while logistic loss is like a smooth curve—both try to avoid misclassifications.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is grid search,"A brute-force hyperparameter tuning technique that evaluates all combinations of predefined hyperparameter values (e.g., λ in [0.001, 0.01, 0.1, 1, 10, 100]) using cross-validation.<br><br><b>ELI5:</b> It's like testing every possible setting on a dial to find the best one.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does random search differ from grid search,"Random search samples hyperparameter values randomly, reducing computational cost compared to grid search's exhaustive evaluation. It often finds good hyperparameters faster.<br><br><b>ELI5:</b> Instead of testing every setting, it picks random ones, saving time while still finding good options.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does λ affect logistic regression,"λ (regularization strength) controls the trade-off between bias and variance. Higher λ increases bias (simpler model), while lower λ increases variance (more complex model).<br><br><b>ELI5:</b> It's like a dial that adjusts how much the model trusts the training data versus keeping things simple.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is Elastic Net,A regularization method combining L1 (λ1) and L2 (λ2) penalties. It balances feature selection (sparsity) and smoothness.<br><br><b>ELI5:</b> It's like using both a lasso (to pick important features) and a rubber band (to keep weights small).,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is feature importance in Logistic Regression,"Feature importance is determined by the absolute value of weights in the W vector. Larger absolute weights indicate higher feature importance.<br><br><b>ELI5:</b> Imagine weights as votes for each feature. The more votes (larger weight), the more important the feature is in making decisions.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does collinearity affect feature importance,"Collinearity distorts weight vectors, making feature importance interpretation unreliable. If features are collinear, weights cannot be directly used to infer importance.<br><br><b>ELI5:</b> If two features are almost identical, the model can't decide which one is more important, so the weights become misleading.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How to detect multicollinear features,"Add small noise (perturbation) to features. If the weight vector changes significantly after retraining, the features are multicollinear.<br><br><b>ELI5:</b> If shaking a feature slightly causes big changes in the model, it's likely tangled with other features.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the time complexity of training Logistic Regression,"Training time complexity is O(nd), where n is the number of samples and d is the number of features.<br><br><b>ELI5:</b> The time to train grows with both the number of data points and the number of features.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the run time complexity of Logistic Regression,"Run time complexity is O(d), where d is the number of features. Space complexity is also O(d).<br><br><b>ELI5:</b> The time and memory needed to make a prediction depend only on the number of features.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does L1 regularization affect Logistic Regression,"L1 regularization induces sparsity, reducing run time complexity. As λ increases, sparsity increases, bias increases, and latency decreases.<br><br><b>ELI5:</b> L1 regularization is like pruning a tree. It removes less important features, making the model faster but potentially less accurate.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does forward feature selection relate to Logistic Regression,"Forward feature selection is an algorithm-independent method to determine feature importance. It can be applied to Logistic Regression to identify important features.<br><br><b>ELI5:</b> It's like building a team one player at a time, testing each addition to see if it improves performance.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the decision surface of Logistic Regression,"A linear hyperplane that separates classes in the feature space.<br><br>\[ \mathbf{w}^T \mathbf{x} + b = 0 \]<br>Where \( \mathbf{w} \) is the weight vector and \( b \) is the bias term. Points on one side belong to class A, points on the other side belong to class B.<br><br><b>ELI5:</b> Like a flat wall dividing a room into two sides. Everything on the left is red, everything on the right is blue.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does imbalanced data affect Logistic Regression,"The model may create a degenerate hyperplane where all points lie on one side, optimizing for the majority class only.<br><br>The decision boundary shifts heavily toward the minority class, ignoring it completely to minimize overall loss.<br><br><b>ELI5:</b> If you have 99 red balls and 1 blue ball, the laziest guess is ""everything is red"". You get 99% accuracy but miss the blue ball entirely.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does Logistic Regression handle outliers,"The sigmoid function \( \sigma(z) = \frac{1}{1 + e^{-z}} \) saturates, limiting the gradient for extreme values. Outliers have diminishing influence on weight updates.<br><br>\[ \frac{d\sigma}{dz} = \sigma(z)(1 - \sigma(z)) \]<br>Maximum gradient occurs at \( z = 0 \). Far from the boundary, gradients approach zero.<br><br><b>ELI5:</b> Outliers are pushed so far away that the model stops listening to them. They become ""background noise"".",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How to detect and remove outliers in Logistic Regression,"Compute the distance from the decision boundary: \( d_i = |\mathbf{w}^T \mathbf{x}_i| \).<br><br>Points with large \( d_i \) are far from the hyperplane and considered outliers. Remove them and retrain.<br><br><b>ELI5:</b> Measure how far each point is from the dividing wall. If it's way out in left field, it's probably a mistake. Kick it out and redraw the wall.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How to handle non-linearly separable data with Logistic Regression,"Transform features into a higher-dimensional space where they become linearly separable.<br><br>Example: Circular data \( F_1^2 + F_2^2 = R^2 \) is non-linear in \( (F_1, F_2) \) but linear in \( (F_1^2, F_2^2) \).<br><br>Use polynomial, trigonometric, exponential, or logarithmic transformations.<br><br><b>ELI5:</b> If you can't draw a straight line in 2D, project the points into 3D where a flat plane can separate them.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the bias-variance tradeoff in Logistic Regression,"Low regularization (small C) → high variance, overfitting to noise.<br>High regularization (large C) → low variance, but may underfit (high bias).<br><br>\[ C = \frac{1}{\lambda} \]<br>Small C = strong regularization = simple boundary.<br>Large C = weak regularization = complex boundary.<br><br><b>ELI5:</b> Tight rules (strong regularization) make you consistent but rigid. Loose rules (weak regularization) let you adapt but you might overreact to random details.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
How does C parameter affect Logistic Regression,"C is the inverse regularization strength.<br><br>\[ \text{Loss} = C \sum \text{log loss} + \|\mathbf{w}\|^2 \]<br><br>Small C → Strong penalty on large weights → Smoother decision boundary → Underfitting risk.<br>Large C → Weak penalty → Complex boundary → Overfitting risk.<br><br><b>ELI5:</b> C controls how much the model cares about fitting every single point. Small C = chill, keep it simple. Large C = obsessive, fit everything perfectly.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is the difference between GridSearchCV and RandomSearchCV,GridSearchCV exhaustively tries all combinations of hyperparameters in a grid.<br>RandomSearchCV samples random combinations from distributions.<br><br>GridSearch is thorough but slow. RandomSearch is faster and often finds good parameters with fewer trials.<br><br><b>ELI5:</b> GridSearch tries every possible key on the keyring. RandomSearch grabs random keys until one works. Random is usually faster.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::logistic_regression
What is column standardization,Mean centering and scaling features to unit variance. Ensures equal contribution of features in distance-based algorithms like kNN and logistic regression.<br><br><b>ELI5:</b> Like measuring ingredients in grams instead of cups and teaspoons so they can be compared fairly.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
Why standardize columns in logistic regression,"Because logistic regression uses distances to compute the weight vector. Without standardization, features with larger scales dominate the distance calculation, biasing the model.<br><br><b>ELI5:</b> Like comparing apples and oranges by weight instead of size—you need a common scale.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
How does column standardization affect kNN,"Ensures all features contribute equally to distance calculations. Without it, features with larger scales disproportionately influence neighbor selection.<br><br><b>ELI5:</b> Like giving equal voting power to all citizens regardless of their income.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::dimensionality_reduction_and_visualization
What is Linear Regression,"Linear Regression is a supervised learning algorithm that finds a hyperplane minimizing the sum of squared errors between true and predicted continuous values.<br><br>\[ \text{minimize} \sum_{i=1}^{n} (y_i - (W^T x_i + b))^2 \]<br>\(y_i\) is the true value, \(W^T x_i + b\) is the predicted value.<br><br><b>ELI5:</b> Like stretching a rubber sheet over pins to minimize the distance to each pin.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::linear_regression
How does Linear Regression work,It optimizes weights \(W\) and bias \(b\) to position the hyperplane \(\pi\) such that the sum of vertical errors (residuals) is minimized.<br><br>\[ \text{error}_i = y_i - (W^T x_i + b) \]<br><br><b>ELI5:</b> Adjusting a ruler's position and angle until the total distance to all data points is smallest.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::linear_regression
How does Logistic Regression differ,"Logistic Regression is a two-class classification technique, whereas Linear Regression fits a hyperplane to continuous data.<br><br><b>ELI5:</b> Linear Regression predicts numbers (height), Logistic Regression predicts categories (tall vs short).",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::linear_regression
What is differentiation,"Differentiation measures the rate of change of a function y = f(x) with respect to x, represented as dy/dx or f'. It calculates the slope of the tangent to the curve at any point.<br><br><b>ELI5:</b> Like measuring how steep a hill is at a specific spot.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does slope relate to maxima and minima,"At maxima, minima, or saddle points, the slope (df/dx) is zero. These are critical points where the function's tangent is flat.<br><br><b>ELI5:</b> Like the peak of a hill (maxima) or bottom of a valley (minima) where the ground is momentarily flat.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is the derivative of F(x) = log(1 + exp(ax)),The derivative is F'(x) = a exp(ax)/(1 + exp(ax)). This is the sigmoid function scaled by 'a'.<br><br><b>ELI5:</b> Like calculating how fast a smooth S-shaped curve rises at any point.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
Why use SGD for optimization,"Most functions cannot be solved analytically, so Stochastic Gradient Descent (SGD) is used to iteratively approximate solutions by adjusting parameters based on gradients.<br><br><b>ELI5:</b> Like climbing a mountain blindfolded by feeling the slope under your feet and taking small steps.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is vector differentiation in this context,Differentiation of vectors results in a vector. We apply element-wise partial differentiation to each component.<br><br>\[ \nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} \]<br><br><b>ELI5:</b> Finding the slope of a mountain in every compass direction at once.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does gradient descent work geometrically,Start at random point. Update position by subtracting the gradient scaled by step size. Repeat until position stops changing. Gradient magnitude decreases as you approach the optimum.<br><br>\[ \mathbf{x}_{\text{new}} = \mathbf{x}_{\text{old}} - r \cdot \nabla f(\mathbf{x}_{\text{old}}) \]<br><br><b>ELI5:</b> Walking downhill blindfolded by taking small steps in the steepest direction until the ground flattens out.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does learning rate affect gradient descent,"If learning rate is too large, the algorithm jumps over the optimum and oscillates without converging. Reducing step size guarantees convergence by preventing overshoot.<br><br><b>ELI5:</b> Taking huge steps down a hill might vault you over the valley floor into the opposite slope; smaller steps ensure you settle at the bottom.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is Stochastic Gradient Descent,"SGD approximates the full gradient using a random subset of data (batch size k) instead of all n points. This introduces noise but drastically reduces per-iteration cost.<br><br>\[ \mathbf{w}_{j+1} = \mathbf{w}_j - \eta \sum_{i=1}^{k} (-2\mathbf{x}_i)(y_i - \mathbf{w}_j^T \mathbf{x}_i) \]<br><br><b>ELI5:</b> Instead of asking every person in a city for directions (expensive), you ask a random small group. The direction is noisier, but you get an answer much faster.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does batch size affect SGD,"There is a trade-off between noise and speed. Smaller batch sizes (k → 1) increase variance (noise) per step but require less computation per step. Larger batches are more stable but computationally heavier per step. To reach the optimum, smaller batches generally require more total iterations.<br><br><b>ELI5:</b> Asking 1 person for directions is fast but risky (might be wrong). Asking 100 people is accurate but takes forever. You balance speed and accuracy.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is the objective of Constrained Optimization,"We seek to minimize a function subject to equality or inequality constraints. This is solved using the Lagrangian function, introducing multipliers (λ, μ) for each constraint.<br><br>\[ \mathcal{L}(\mathbf{x}, \lambda, \mu) = f(\mathbf{x}) + \lambda g(\mathbf{x}) + \mu h(\mathbf{x}) \]<br>\( \lambda, \mu \geq 0 \)<br><br><b>ELI5:</b> Finding the lowest point on a mountain while staying on a specific trail. You add the trail's rules into your map calculation.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How are Lagrange Multipliers used,"Solve the optimization by setting partial derivatives of the Lagrangian to zero. This yields the optimal \( \mathbf{x} \) along with the multiplier values.<br><br>\[ \nabla_{\mathbf{x}} \mathcal{L} = 0, \quad \frac{\partial \mathcal{L}}{\partial \lambda} = 0, \quad \frac{\partial \mathcal{L}}{\partial \mu} = 0 \]<br><br><b>ELI5:</b> You flatten the 3D landscape (including the constraints) into a set of equations and solve them to find the exact meeting point.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How is PCA related to Eigenvalues,"PCA finds the principal components by solving the eigenvalue equation of the covariance matrix \( S \). The eigenvector \( \mathbf{u} \) represents the direction of the component, and the eigenvalue \( \lambda \) represents its variance.<br><br>\[ S\mathbf{u} = \lambda\mathbf{u} \]<br><br><b>ELI5:</b> The covariance matrix is a machine that shakes data. The eigenvectors are the directions it shakes, and the eigenvalues are how hard it shakes in those directions.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is the Lagrangian for logistic regression with regularization,"The Lagrangian combines logistic loss and a penalty term:
\[
L = \text{logistic loss} - \lambda(1 - W^TW)
\]
This formulation treats regularization as an equality constraint on the weight magnitude.<br><br><b>ELI5:</b> Like balancing a budget: the loss is your spending error, and the penalty forces you to keep your total spending (weights) under a strict limit.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What does L1 regularization induce into the weight vector,"L1 regularization induces sparsity, meaning most elements of the weight vector become exactly zero.<br><br><b>ELI5:</b> Like packing a suitcase with strict weight limits - you throw out most items completely rather than keeping small amounts of everything.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does the L2 regularization objective function look,"L2 regularization minimizes:
\[
\min_W (W_1^2 + W_2^2 + \dots + W_d^2)
\]
This creates a parabolic (bowl-shaped) optimization surface.<br><br><b>ELI5:</b> Like rolling a ball down a smooth curved bowl - it gently settles near the bottom.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does the L1 regularization objective function look,"L1 regularization minimizes:
\[
\min_W (|W_1| + |W_2| + \dots + |W_d|)
\]
This creates a V-shaped (diamond) optimization surface with sharp corners.<br><br><b>ELI5:</b> Like a pyramid with flat sides - the ball can get stuck exactly at the sharp pointy corners.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is the derivative of L2 regularization,"For L2: \(L_2(W_1) = W_1^2\), so \(\frac{dL_2}{dW_1} = 2W_1\)<br>The derivative is a straight line proportional to the weight value.<br><br><b>ELI5:</b> Like a dimmer switch - the push gets weaker as you approach zero.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is the derivative of L1 regularization,"For L1: \(L_1(W_1) = |W_1|\), so \(\frac{dL_1}{dW_1} = \pm 1\)<br>The derivative is a constant step function independent of weight magnitude.<br><br><b>ELI5:</b> Like an on/off switch - it always pushes with the same constant force.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does L2 weight update change as weights approach zero,"L2 update: \(W_{j+1} = W_j - r(2W_j)\)<br>As \(W_j \to 0\), the update \(2rW_j \to 0\). Convergence rate decreases because the derivative shrinks.<br><br><b>ELI5:</b> Like braking a car - you slow down gently as you approach the stop line.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
How does L1 weight update behave near zero,"L1 update: \(W_{j+1} = W_j - r(\pm 1)\)<br>The update remains constant at \(r\) regardless of weight magnitude, driving weights to exactly zero.<br><br><b>ELI5:</b> Like chopping wood - each swing has the same force, eventually cutting through completely.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
Why does L1 regularization create sparsity,"L1 derivative is constant (\(\pm 1\)) and independent of weight value, so weights get pushed to exactly zero. L2 derivative decreases as \(W \to 0\), so weights approach but rarely reach zero.<br><br><b>ELI5:</b> L1 is like a bulldozer that keeps pushing until the weight hits zero. L2 is like a gentle breeze that loses strength as it gets close.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::solving_optimization_problems
What is moving window featurization,Simplest featurization for time series data; takes a time split of a specific width.<br><br><b>ELI5:</b> Like looking at a small section of a long video at a time to understand what's happening.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is window width hyperparameter,The chosen time duration for a moving window in time series featurization.<br><br><b>ELI5:</b> Deciding how long each 'snapshot' of the video should be.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is Fourier decomposition,Method to represent time series data by decomposing a composite wave into multiple sine waves of different periods and peaks; transforms data to frequency domain.<br><br><b>ELI5:</b> Breaking down a complex song into its individual musical notes.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is frequency in Fourier decomposition,"The number of oscillations per unit of time, calculated as 1/T where T is the time taken for one oscillation.<br><br><b>ELI5:</b> How fast a musical note repeats.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is phase in Fourier decomposition,The difference in the starting angle of different waves in a composite signal.<br><br><b>ELI5:</b> When different musical notes start playing relative to each other.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How does Fourier decomposition work,Transforms time series data from the time domain to the frequency domain by identifying constituent sine waves.<br><br><b>ELI5:</b> Analyzing a song by looking at all the individual notes and their loudness instead of just the combined sound.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
When is Fourier decomposition useful,"Useful for time series data that exhibits repeating patterns.<br><br><b>ELI5:</b> When you expect to see a recurring rhythm or cycle, like daily sales.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How does moving window affect dataset,"Each data point in the generated dataset corresponds to a different time window, allowing for feature extraction from segments.<br><br><b>ELI5:</b> Creating many small summaries from different parts of a long document.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How does window width affect featurization,"Determines the granularity of analysis; a smaller window captures short-term patterns, while a larger window captures long-term trends.<br><br><b>ELI5:</b> Looking at a video clip frame-by-frame (small window) versus watching a whole scene (large window).",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How does Fourier decomposition affect vectorization,"Allows for vectorization using important frequencies and their corresponding amplitudes, representing the signal's spectral content.<br><br><b>ELI5:</b> Describing a song by its main instruments and how loud each one is.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What are deep learnt features,"Features automatically learned by deep learning models like LSTMs, often specific to the problem and not transferable.<br><br><b>ELI5:</b> Skills a specific AI learns for one game that it can't use for another.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is color histogram,"A plot of pixel color values (0-255) for each color channel (R, G, B) vectorized.<br><br><b>ELI5:</b> Like counting how many red, green, and blue crayons are in a box of crayons.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is edge histogram,A histogram representing edge orientation at color separation interfaces.<br><br><b>ELI5:</b> Like a map showing where the lines and borders are in a drawing.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is relational data,Data stored across multiple tables linked by defined relationships.<br><br><b>ELI5:</b> Like a set of interconnected address books where one book lists people and another lists their jobs.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is CNN for images,Convolutional Neural Networks are designed to process grid-like data such as images.<br><br><b>ELI5:</b> A special type of computer vision system that looks at pictures by breaking them down into smaller parts.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is LSTM for time series,Long Short-Term Memory networks are a type of recurrent neural network suitable for sequential data like time series.<br><br><b>ELI5:</b> A computer memory system that can remember important past events to understand what's happening now in a sequence.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is graph data example,"Facebook user social graph for recommending new friends.<br><br><b>ELI5:</b> Like a map of who knows whom, used to suggest new people to meet.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is indicator variable,"A binary variable (0 or 1) representing a condition. Example: 1 if height > 150cm, 0 otherwise.<br><br><b>ELI5:</b> A simple yes/no switch for a specific characteristic.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How to convert numerical to indicator variable,"Define a threshold. If the numerical value meets the condition, assign 1, else 0.<br><br><b>ELI5:</b> Setting a rule, like 'if you're taller than this line, you get a gold star'.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How to convert categorical to indicator variable,"Assign 1 if the category matches a specific value, 0 otherwise. Example: 1 if country is India or USA, 0 else.<br><br><b>ELI5:</b> Creating a flag for specific groups, like 'is this person from India or the US?'.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is feature binning,Extending indicator variables to multi-class encoding by grouping numerical data into bins or ranges.<br><br><b>ELI5:</b> Sorting items into different sized boxes instead of just 'big' or 'small'.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How to perform feature binning,"Define ranges for a numerical feature. Assign a unique integer to each range.<br>Example: Height bins: 1 if H<120, 2 if 120<=H<150, 3 if 150<=H<180, 4 if H>=180.<br><br><b>ELI5:</b> Grouping people by age into 'child', 'teenager', 'adult', 'senior'.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How decision trees help feature binning,Decision trees can automatically find optimal thresholds for binning by using criteria like Information Gain.<br><br><b>ELI5:</b> A smart game of '20 questions' that helps decide the best cut-off points for sorting data.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How does feature binning affect model,"Transforms continuous numerical data into discrete categorical bins, potentially simplifying relationships for models like decision trees.<br><br><b>ELI5:</b> Making complex measurements easier to understand by categorizing them, like turning exact temperatures into 'hot', 'warm', 'cold'.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is interaction variable,"New features created by combining existing features through mathematical operations or logical conditions.<br><br><b>ELI5:</b> Imagine combining two ingredients like flour and sugar to make a cake batter, creating something new.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How to find interaction features using decision trees,"Create new features by using all leaf nodes of a decision tree.<br><br><b>ELI5:</b> Think of a decision tree as a flowchart. Each endpoint (leaf) represents a unique combination of conditions, which can be a new interaction feature.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is feature orthogonality,Features are orthogonal when they are very different and have no linear relationship. More orthogonality generally leads to better model performance.<br><br><b>ELI5:</b> Orthogonal features are like independent tools in a toolbox; each does a different job without interfering with others.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How feature correlation affects model performance,"High correlation (low orthogonality) among features leads to poor overall prediction performance on the target.<br><br><b>ELI5:</b> If two tools in your toolbox do the exact same thing, having both is redundant and doesn't improve your ability to build something.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
How feature slicing works,"Split data based on feature values and build separate models for each split.<br><br><b>ELI5:</b> Imagine sorting your tools by type (e.g., wrenches, screwdrivers) and then learning how to use each type best for specific tasks.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is split criteria for feature slicing,"Feature values must be different across splits and sufficient data points must be available for each split.<br><br><b>ELI5:</b> You only sort your tools if there are distinct groups (e.g., metric vs. imperial wrenches) and you have enough of each to practice with.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::featurization_and_feature_engineering
What is Isotonic Regression,"A non-parametric regression method that fits a piecewise linear function to data, allowing for non-monotonic relationships.<br><br><b>ELI5:</b> Imagine fitting a wiggly line to data points instead of a straight one, where the line can go up and down.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
How does Isotonic Regression work,"It solves an optimization problem to minimize the difference between the model's predictions and actual values, creating a piecewise linear fit.<br><br><b>ELI5:</b> It's like drawing a series of connected straight line segments to best follow a scattered set of points.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
When to use Isotonic Regression,"Used when calibration plots are not sigmoid-shaped and a simple sigmoid fit would have large errors, especially with large datasets.<br><br><b>ELI5:</b> Use this when your data's trend isn't a simple S-shape and you have plenty of data to map out the complex trend.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
How does data size affect Isotonic Regression choice,"Large data favors Isotonic Regression for calibration, while small data favors simpler methods like Platt Scaling.<br><br><b>ELI5:</b> With lots of data, you can afford a complex tool like Isotonic Regression; with little data, a simpler tool like Platt Scaling is better.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is RANSAC,"Random Sampling Consensus, a method to build robust models in the presence of outliers by iteratively sampling data, fitting models, and removing outliers.<br><br><b>ELI5:</b> It's like trying to draw a line through a bunch of dots, but some dots are way off; RANSAC repeatedly tries to draw lines using only good dots and throws away the bad ones.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
How does RANSAC work,"It repeatedly samples subsets of data, fits a model, computes loss, identifies outliers based on loss, and removes them, continuing until satisfied.<br><br><b>ELI5:</b> Pick a few points, draw a line, see which points are far from the line, remove those far points, repeat until the line looks good for most points.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
Why retrain models periodically,"Models need retraining to capture current trends, adapt to new data (e.g., new companies), and counteract performance degradation over time.<br><br><b>ELI5:</b> Like updating a map because roads change, models need updates to stay relevant as the world they represent evolves.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
When is model retraining necessary,"Retraining is necessary when dealing with time-series data where trends change or when model performance drops due to dataset drift.<br><br><b>ELI5:</b> If your stock predictor starts losing money, it's time to teach it about today's stock market, not yesterday's.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is A/B testing,Controlled experiment comparing two versions A and B to determine which performs better<br><br><b>ELI5:</b> Testing two different flavors of ice cream to see which one people like more.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is control group in A/B testing,Group receiving the standard or existing treatment/product<br><br><b>ELI5:</b> The group of people who get the regular medicine while testing a new one.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is treatment group in A/B testing,Group receiving the new or experimental treatment/product<br><br><b>ELI5:</b> The group of people who get the new experimental medicine.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 1,Understand business requirements define problem and customer<br><br><b>ELI5:</b> Figuring out what the boss actually wants the project to achieve.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 2,Data acquisition ETL extract transform load SQL<br><br><b>ELI5:</b> Gathering all the necessary information from different places and putting it in one spot.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 3,Data preparation cleaning and pre-processing<br><br><b>ELI5:</b> Tidying up the collected information so it's ready to be used.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 4,EDA plots visualization hypothesis testing data slicing<br><br><b>ELI5:</b> Looking at the data with charts and tests to understand its patterns.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 5,Modeling evaluation and interpretation<br><br><b>ELI5:</b> Building a predictive tool and checking how well it works.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 6,Communicate results clear and simple 1-pager and 6 pagers<br><br><b>ELI5:</b> Explaining the findings of the project in an easy-to-understand way.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 7,Deployment<br><br><b>ELI5:</b> Making the finished project available for use.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 8,Real-world testing A/B testing<br><br><b>ELI5:</b> Trying out the project in a live environment to see how it performs.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 9,Customer business buy-in<br><br><b>ELI5:</b> Getting approval and agreement from the people who will use or benefit from the project.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 10,"Operations retrain models handle failures process<br><br><b>ELI5:</b> Keeping the project running smoothly, fixing issues, and updating it.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is data science life cycle step 11,Optimization improve models more data more features optimize code<br><br><b>ELI5:</b> Making the project even better by adding improvements and fine-tuning it.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is VC dimension,Measure of the model class potential.<br><br><b>ELI5:</b> How many distinct patterns a simple tool can draw on a piece of paper.,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
VC dimension of linear model,Maximum points shattered by linear model for all configurations not collinear is 3.<br><br><b>ELI5:</b> A straight line can perfectly separate up to 3 points in any arrangement (unless they are on the same line).,aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
VC dimension RBF SVM,"Theoretically infinite, practically limited by assumptions.<br><br><b>ELI5:</b> An RBF SVM is like a super flexible drawing tool that can theoretically create any shape, but in practice, it's constrained by the paper size and how you hold it.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
VC dimension practical use,"Mostly for research, not general applied work.<br><br><b>ELI5:</b> It's a theoretical concept like calculating the exact speed of light, interesting for scientists but not something a mechanic uses daily.",aaic::module_5_feature_engineering_productionization_and_deployment_of_ml_models::miscellaneous_topics
What is clustering,Grouping similar data points without target values<br><br><b>ELI5:</b> Sorting socks into piles of similar colors.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
What is intra cluster distance,The distance between data points within the same cluster<br><br><b>ELI5:</b> How close together your friends are at a party.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
What is inter cluster distance,The distance between data points in different clusters<br><br><b>ELI5:</b> How far apart different groups of people are at a party.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
Objective of clustering,Minimize intra-cluster distance and maximize inter-cluster distance<br><br><b>ELI5:</b> Making groups of friends close together and separate from other groups.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
How does Dunn index measure clustering,"Ratio of minimum inter-cluster distance to maximum intra-cluster distance<br><br>D = min d(i,j) / max d’(k)<br>where d(i,j) is the distance between clusters i and j, and d’(k) is the distance within cluster k.<br><br><b>ELI5:</b> A score where higher is better, indicating well-separated and compact groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
What is unsupervised learning,Learning from data without predefined target variables<br><br><b>ELI5:</b> Learning by observing and finding patterns without being told the right answers.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
How does clustering relate to unsupervised learning,Clustering is a task within unsupervised learning used to group data<br><br><b>ELI5:</b> Clustering is like sorting items into categories when you don't know the categories beforehand.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
What is elbow method for determining k,Plot loss function versus number of clusters k<br><br>Select k at the 'elbow' point where the rate of decrease sharply changes<br><br><b>ELI5:</b> Imagine bending a pipe; the elbow is the sharpest bend.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::unsupervised_learning_clustering
K-Means time complexity,"O(n*k*d*i) where n is points, k is clusters, d is dimensionality, i is iterations. Simplified to O(nd).<br><br><b>ELI5:</b> Imagine sorting a huge pile of mixed-up socks. The more socks (n) and the more colors (d), the longer it takes. The number of times you have to re-sort (i) also adds time.",aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
K-Means space complexity,"O(nd + kd) where n is points, d is dimensionality, k is clusters. Simplified to O(nd).<br><br><b>ELI5:</b> To store your sock collection, you need space for each sock (n*d) and space for labels for each color group (k*d).",aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
Hierarchical clustering space complexity,"O(n^2) where n is the number of data points.<br><br><b>ELI5:</b> Imagine drawing lines connecting every single person in a room to every other person. The more people (n), the more lines you need to draw.",aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
Hierarchical clustering time complexity,"O(n^3) where n is the number of data points.<br><br><b>ELI5:</b> Imagine trying to find the closest pair of people in a room by comparing everyone to everyone else, then repeating this process many times. The more people (n), the more comparisons and the more complex the process becomes.",aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
Algorithm time complexity O(n log n),"Efficient for sorting large datasets.<br><br><b>ELI5:</b> Like sorting a deck of cards by dividing them into piles, sorting each pile, and then merging them back. It's faster than sorting one card at a time.",aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
Algorithm space complexity O(n),Requires space proportional to input size.<br><br><b>ELI5:</b> You need a notebook that can hold one entry for each item you are processing.,aaic::module_1_fundamentals_of_programming::python_for_data_science_computational_complexity
What is agglomerative clustering,"Starts with each data point as a cluster and merges the two nearest clusters iteratively until a desired number of clusters k is reached.<br><br><b>ELI5:</b> Imagine each person is their own group, and then the closest two people join forces, then the next closest pair, and so on, until everyone is in one big group.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
What is divisive clustering,"Starts with all data points in a single cluster and recursively splits clusters until a desired number of clusters k is reached. The splitting criterion is complex.<br><br><b>ELI5:</b> Imagine everyone starts in one giant room, and then the room is split into two, then one of those rooms is split again, and so on, until you have the right number of smaller rooms.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
What is a dendrogram,"A tree-like diagram that visualizes the hierarchical clustering process, showing the sequence of merges or splits and the distances at which they occur.<br><br><b>ELI5:</b> It's like a family tree for your data points, showing how they gradually group together or split apart.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does min inter cluster similarity work,Defines the similarity between two clusters as the minimum distance between any point in one cluster and any point in the other cluster.<br><br><b>ELI5:</b> The clusters are considered close if even one point from each cluster is very near each other.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does max inter cluster similarity work,Defines the similarity between two clusters as the maximum distance between any point in one cluster and any point in the other cluster.<br><br><b>ELI5:</b> The clusters are considered close if at least one point from each cluster is very far from each other.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does group average inter cluster similarity work,"Defines the similarity between two clusters as the average similarity between all pairs of points, where one point is from the first cluster and the other is from the second.<br><br><b>ELI5:</b> It's like finding the average closeness between every possible pair of people, one from each group.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does distance between centroids work,Defines the similarity between two clusters based on the distance between their calculated centroids (mean points).<br><br><b>ELI5:</b> We measure how far apart the 'centers of mass' of two groups are.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does Ward's method work,"Merges clusters to minimize the increase in total within-cluster variance. It uses squared distances in its calculations, similar to Group Average.<br><br><b>ELI5:</b> It tries to merge groups in a way that keeps the groups as 'tight' and compact as possible after the merge.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does min inter cluster similarity affect clustering,Can handle non-globular distributions but is sensitive to noise and outliers.<br><br><b>ELI5:</b> It's good at finding oddly shaped groups but can be easily fooled by a few stray points.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does max inter cluster similarity affect clustering,"Less prone to outliers and noise but tends to break large clusters and is biased towards globular clusters.<br><br><b>ELI5:</b> It's good at ignoring weird points and keeping big groups together, but it might split a large, spread-out group into smaller, rounder ones.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does group average similarity affect clustering,"Less prone to outliers and noise but biased towards globular clusters.<br><br><b>ELI5:</b> It's good at ignoring weird points and prefers to form round, compact groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does distance between centroids affect clustering,"Less prone to outliers and noise but biased towards globular clusters.<br><br><b>ELI5:</b> It's good at ignoring weird points and prefers to form round, compact groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
How does Ward's method affect clustering,"Less prone to outliers and noise but biased towards globular clusters.<br><br><b>ELI5:</b> It's good at ignoring weird points and prefers to form round, compact groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::hierarchical_clustering_technique
What is DBSCAN clustering,Clusters dense regions separates sparse regions into noise<br><br><b>ELI5:</b> Imagine finding islands (clusters) in an ocean (dataset) by looking for areas with many people (dense regions) and separating them from empty spaces (sparse regions).,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is density at point P,Number of points within radius eps of point P<br><br><b>ELI5:</b> How many friends are within arm's reach (radius eps) of you (point P).,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is a dense region,A region with at least MinPts points within radius eps<br><br><b>ELI5:</b> A spot where at least a certain number of people (MinPts) are gathered closely together (within radius eps).,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is a core point,A point with at least MinPts points in its eps radius<br><br><b>ELI5:</b> Someone who is in a crowded area (dense region) and has many people nearby.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is a border point,A point within eps radius of a core point but not a core point itself<br><br><b>ELI5:</b> Someone standing just outside the main crowd but still close enough to be considered part of the group.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is a noise point,"A point that is neither a core point nor a border point<br><br><b>ELI5:</b> Someone standing all alone, far away from any groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does DBSCAN work objective,Identify dense regions as clusters and sparse regions as noise<br><br><b>ELI5:</b> To find groups of closely packed items and separate them from isolated items.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is density edge,Connection between two core points within eps radius<br><br><b>ELI5:</b> Two friends are close enough to easily talk to each other.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is density connected points,Two core points are density connected if a path of density edges exists between them.<br><br><b>ELI5:</b> A chain of friends where each friend knows the next one in line.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is objective of DBSCAN algorithm,"To identify clusters based on point density and connect density-reachable points.<br><br><b>ELI5:</b> Finding groups of friends who are all close to each other, directly or indirectly.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does DBSCAN label points,"Points are labeled core, border, or noise based on MinPts and Eps using Range Query.<br><br><b>ELI5:</b> Deciding who is a central figure, a friend of a central figure, or an outsider.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does DBSCAN form clusters,"New clusters are formed from unassigned core points, adding all density-connected points.<br><br><b>ELI5:</b> Starting a new social circle with a key person and inviting all their connected friends.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does DBSCAN handle border points,Border points are assigned to the nearest cluster they are density-connected to.<br><br><b>ELI5:</b> People on the edge of a group joining the closest existing group.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is RangeQuery in DBSCAN,Returns all data points within a specified epsilon (eps) radius of a given point.<br><br><b>ELI5:</b> Checking everyone within shouting distance.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does MinPts affect DBSCAN,"Higher MinPts makes it harder to be a core point, reducing sensitivity to noise and potentially merging smaller clusters.<br><br><b>ELI5:</b> Requiring more friends to be considered popular, making it harder for casual acquaintances to form groups.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does Eps affect DBSCAN,"Larger Eps allows points further apart to be considered neighbors, potentially merging clusters and including more points.<br><br><b>ELI5:</b> Increasing the distance for friends to be considered 'close', making groups larger and more inclusive.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How to choose Eps for DBSCAN,"Calculate distance to kth nearest point (k=MinPts), sort distances, and find the 'elbow' point on a plot.<br><br><b>ELI5:</b> Finding the sweet spot for 'closeness' by seeing where the distances to friends start to jump significantly.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is DBSCAN resistant to,"Noise and varying cluster sizes<br><br><b>ELI5:</b> Like finding groups of friends at a party, DBSCAN can ignore people standing alone (noise) and find both small huddles and large crowds.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What does DBSCAN not require,Specification of the number of clusters (n_clusters)<br><br><b>ELI5:</b> You don't need to guess how many friend groups there are beforehand; DBSCAN finds them naturally.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What does DBSCAN have trouble with,"Varying densities and high dimensionality data<br><br><b>ELI5:</b> It struggles if friend groups have vastly different sizes (some tiny, some huge) or if there are too many factors to consider when grouping people.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is DBSCAN sensitive to,"Hyperparameters and distance measure<br><br><b>ELI5:</b> The results depend heavily on how you define 'close' and the specific settings you choose, which can be tricky.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
How does distance measure affect DBSCAN,"Curse of dimensionality<br><br><b>ELI5:</b> In high-dimensional spaces, all points tend to appear equally far apart, making it hard for DBSCAN to find meaningful clusters.",aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::dbscan_density_based_clustering_technique
What is K-means clustering,"An unsupervised learning algorithm that partitions data into K clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids.<br><br><b>ELI5:</b> Imagine sorting toys into K boxes, trying to put similar toys in the same box and minimizing how far each toy is from the center of its box.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How to select K for K-means,"Common methods include the Elbow Method (plotting within-cluster sum of squares vs K and choosing the 'elbow' point) and Silhouette Analysis (measuring how similar a point is to its own cluster compared to other clusters).<br><br><b>ELI5:</b> Like deciding how many groups to divide your friends into for a game, you look for the point where adding more groups doesn't make the game much better.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How is KNN different from k-means clustering,KNN is a supervised classification/regression algorithm that predicts a new data point's label based on its K nearest neighbors. K-means is an unsupervised clustering algorithm that groups data points into K clusters without prior labels.<br><br><b>ELI5:</b> KNN is like asking your K closest friends for their opinion to decide something. K-means is like sorting your friends into K groups based on who they hang out with most.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Hierarchical clustering,"A clustering method that builds a hierarchy of clusters. Agglomerative (bottom-up) starts with individual points as clusters and merges them. Divisive (top-down) starts with one cluster and splits it.<br><br><b>ELI5:</b> Like building a family tree, starting with individuals and grouping them into families, then larger family groups, and so on.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Limitations of Hierarchical clustering,"Can be computationally expensive (O(n^3) or O(n^2 log n)), sensitive to noise and outliers, and once a merge or split is made, it cannot be undone.<br><br><b>ELI5:</b> Once you've decided two people are siblings, you can't easily change your mind later, and it can take a lot of effort to figure out all the family connections.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Time complexity of Hierarchical clustering,"Agglomerative hierarchical clustering typically has a time complexity of O(n^2 log n) or O(n^3) depending on the linkage method and implementation, where n is the number of data points.<br><br><b>ELI5:</b> For a large group of people, figuring out all possible family relationships can take a very long time, getting slower as more people are added.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is DBSCAN,"Density-Based Spatial Clustering of Applications with Noise. It groups together points that are closely packed together (points with many nearby neighbors), marking points that lie alone in low-density regions as outliers.<br><br><b>ELI5:</b> Imagine finding groups of people at a party. DBSCAN finds dense clusters of people talking together and identifies those standing alone as 'outliers'.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Advantages of DBSCAN,"Can find arbitrarily shaped clusters, robust to outliers, and does not require specifying the number of clusters beforehand.<br><br><b>ELI5:</b> It can find groups of people in any shape, not just circles, and it doesn't get confused by people who are just wandering around alone.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Limitations of DBSCAN,"Struggles with clusters of varying densities and sensitive to the choice of parameters (epsilon and min_samples).<br><br><b>ELI5:</b> It might have trouble if some groups at a party are very crowded and others are spread out, and it's tricky to pick the right 'closeness' and 'group size' settings.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Content based filtering,"A recommender system approach that recommends items similar to those a user liked in the past, based on item features and user profiles.<br><br><b>ELI5:</b> If you like action movies, it recommends more action movies based on their genre and actors.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Collaborative filtering,"A recommender system approach that recommends items based on the preferences of similar users. It finds users with similar tastes and recommends items they liked but the current user hasn't seen.<br><br><b>ELI5:</b> If you and your friend like the same music, and your friend recommends a new song, you might like it too.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is PCA,"Principal Component Analysis. A dimensionality reduction technique that transforms data into a new coordinate system such that the greatest variance by any projection of the data lies on the first coordinate (the first principal component), the second greatest variance on the second coordinate, and so on.<br><br><b>ELI5:</b> Like squashing a 3D object onto a 2D surface while keeping as much of its original shape and detail as possible.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is SVD,"Singular Value Decomposition. A matrix factorization technique that decomposes a matrix into three other matrices: U, Σ, and V^T. It's fundamental for many machine learning tasks including dimensionality reduction and recommender systems.<br><br><b>ELI5:</b> Breaking down a complex recipe into simpler base ingredients and cooking methods.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is NMF,"Non-negative Matrix Factorization. A matrix factorization technique that decomposes a non-negative matrix into two non-negative matrices. It's often used for topic modeling and feature extraction where interpretability is key.<br><br><b>ELI5:</b> Like breaking down a picture into its basic colors and shapes, where all parts are visible and understandable.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How to do MF for Collaborative filtering,"Decompose the user-item interaction matrix into two lower-dimensional matrices: user factors and item factors. The product of these matrices approximates the original matrix, predicting missing ratings.<br><br><b>ELI5:</b> Imagine a grid of users and movies they've rated. Matrix factorization tries to find hidden 'tastes' for users and 'qualities' for movies that explain these ratings.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How to do MF for feature engineering,"Use the latent factors learned from matrix factorization as new features for downstream machine learning models. These factors capture underlying patterns and relationships in the data.<br><br><b>ELI5:</b> Using the 'hidden tastes' and 'movie qualities' discovered by matrix factorization as new clues to help predict something else, like whether a user will like a new genre.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
Relation between Clustering and MF,"Clustering can be used to group similar users or items, and these clusters can inform matrix factorization by providing prior information or by being used as features. MF can also reveal latent clusters within the data.<br><br><b>ELI5:</b> Grouping similar people (clustering) can help understand their preferences better, which in turn helps in recommending things (MF), and vice versa.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Hyperparameter tuning,"The process of finding the optimal set of hyperparameters for a machine learning model. Hyperparameters are parameters whose values are set before the learning process begins.<br><br><b>ELI5:</b> Like adjusting the settings on a radio (volume, bass, treble) to get the best sound quality before you start listening.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is Cold Start problem,"The challenge in recommender systems where it's difficult to make recommendations for new users or new items due to a lack of historical interaction data.<br><br><b>ELI5:</b> Trying to recommend a movie to someone you just met, or recommending a brand new movie that no one has seen yet.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
How to solve Word Vectors using MF,"Treat word co-occurrence counts in a corpus as a matrix and apply matrix factorization to learn dense vector representations (word embeddings) that capture semantic relationships.<br><br><b>ELI5:</b> Like finding hidden 'meanings' or 'concepts' for words by looking at which words tend to appear together, and representing each word as a point in a space based on these concepts.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What are Eigen faces,"A set of eigenvectors derived from the covariance matrix of a set of images of faces. They represent the principal components of variation in facial appearance and can be used for face recognition.<br><br><b>ELI5:</b> Like finding the most 'typical' or 'defining' features of faces (e.g., general shape, common eye positions) that can be combined to represent any face.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::linear_algebra
What is cold start problem in recommender systems,New users or items lack rating data for recommendations.<br><br><b>ELI5:</b> Imagine a new restaurant opening; you don't know what dishes people like yet.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How to handle cold start problem,Recommend top items based on metadata like geo location browser and device.<br>Use content-based recommendation.<br><br><b>ELI5:</b> For a new restaurant suggest popular dishes from similar restaurants in the area.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
What is word vectors as matrix factorization,Techniques like Word2Vec LSA PLSA LDA GLOVE can be interpreted as matrix factorization methods.<br><br><b>ELI5:</b> Breaking down complex word relationships into simpler underlying components like ingredients in a recipe.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How is co-occurrence matrix constructed for word vectors,Xij = # of times word wj occurs in the context of word Wi.<br>X is an nxn matrix where n is the vocabulary size.<br><br><b>ELI5:</b> Counting how often words appear near each other in a book.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How is matrix factorization applied to co-occurrence matrix,Truncated Singular Value Decomposition (SVD) is applied to the co-occurrence matrix X.<br>Xnxn = U Σ VT.<br>Discarding (n-k) columns to get U of shape nxk.<br><br><b>ELI5:</b> Simplifying a large word-association table by focusing on the most important relationships.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
What is truncated SVD for word vectors,Keeps top k singular values left singular vectors and right singular vectors.<br>Discards eigenvalues with least information.<br>k is a hyperparameter.<br><br><b>ELI5:</b> Keeping only the most significant patterns in word co-occurrences.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How are word vectors obtained from matrix factorization,The U matrix from truncated SVD (nxk shape) provides vector representations for each word.<br>Each row is a k-dimensional vector for a word.<br><br><b>ELI5:</b> Each word gets a unique numerical fingerprint based on its common usage.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How do Eigen-Faces relate to matrix factorization,Eigen-faces use Principal Component Analysis (PCA) which is related to matrix factorization (SVD) on image data.<br><br><b>ELI5:</b> Finding the most important facial features that distinguish different people.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How is matrix constructed for Eigen-Faces,Images are stacked row-wise.<br>Each row is a flattened vector of an image.<br><br><b>ELI5:</b> Arranging all pixels from many photos into one long list for each photo.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How is dimensionality reduction applied for Eigen-Faces,A covariance matrix is computed from the image matrix.<br>Dimensionality reduction is applied via matrix factorization (SVD) on the covariance matrix.<br><br><b>ELI5:</b> Finding the core patterns in how pixel values vary across all faces.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How are Eigen Faces computed,Multiply the row-stacked image matrix with top k left singular vectors (from SVD) or column-stacked eigenvectors (from covariance matrix).<br><br><b>ELI5:</b> Combining the core facial patterns with the original images to create representative 'face components'.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
How to determine k for Eigen Faces,Use eigenvalues to compute the percentage of explained variance.<br>Ratio of eigenvalues helps select a good k.<br><br><b>ELI5:</b> Choosing how many key facial features are enough to represent most faces accurately.,aaic::module_7_data_mining_unsupervised_learning_and_recommender_systems_real_world_case_studies::recommender_systems_and_matrix_factorization
What is content based recommendation,"Recommends items similar to those a user liked in the past based on item features.<br><br><b>ELI5:</b> If you like action movies, it recommends more action movies.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is collaborative based recommendation,"Recommends items based on the preferences of similar users.<br><br><b>ELI5:</b> If people who like the same movies as you also like a new movie, it recommends that new movie to you.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is affinity analysis,Analyzes relationships between items to discover which items are frequently purchased together.<br><br><b>ELI5:</b> Finding out that people who buy bread often also buy butter.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is objective of recommendation system,To predict user preferences and suggest relevant items to increase engagement and sales.<br><br><b>ELI5:</b> Showing you things you're likely to want to buy or watch.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How does content based recommendation work,Matches user's past item interactions with item features to find similar items.<br><br><b>ELI5:</b> It looks at the genre and actors of movies you liked and finds others with similar characteristics.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How does collaborative filtering work,Identifies users with similar tastes and recommends items that those similar users liked.<br><br><b>ELI5:</b> It finds people who have similar movie tastes to you and suggests movies they enjoyed.,aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How does affinity analysis relate to recommendations,"Identifies items frequently bought together, enabling 'people who bought this also bought' recommendations.<br><br><b>ELI5:</b> If many people buy milk and cookies together, it recommends cookies when someone buys milk.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is MLP training objective,Minimize the loss function L by finding optimal weights W.<br><br><b>ELI5:</b> Adjusting knobs on a machine to get the best possible output.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is square loss,"A common loss function measuring the squared difference between the true value y_i and the predicted value y_hat_i.<br><br>L(y_i, y_i_hat) = (y_i - y_i_hat)^2<br><br><b>ELI5:</b> How far off your guess is from the actual number, squared.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is SGD update rule,"Update weights W by moving in the opposite direction of the gradient of the loss function, scaled by the learning rate η.<br><br>W_new = W_old - η * dL/dW<br><br><b>ELI5:</b> Taking small steps downhill on a mountain to reach the lowest point.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does backpropagation work,"It uses the chain rule to compute gradients of the loss function with respect to weights, starting from the output layer and moving backward through the network.<br><br><b>ELI5:</b> Tracing blame backward from a mistake to find who or what caused it.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is memoization in backpropagation,"Storing computed gradient values to avoid redundant calculations, significantly speeding up the training process.<br><br><b>ELI5:</b> Writing down an answer to a math problem so you don't have to solve it again if it appears later.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does chain rule help backpropagation,"It allows the gradient of the loss function to be computed efficiently with respect to each weight by breaking down the complex derivative into a product of simpler derivatives.<br><br><b>ELI5:</b> Like a chain reaction, where each link's effect depends on the previous one, allowing us to calculate the overall impact.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is forward propagation,The process of passing input data through the network to compute the output and the loss.<br><br><b>ELI5:</b> Feeding information into a machine and seeing what it produces.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is backward propagation,"The process of computing gradients of the loss with respect to weights, starting from the loss and moving backward through the network.<br><br><b>ELI5:</b> Figuring out how much each part of the machine contributed to the final error.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is an epoch in training,One complete pass of all the training data points through the neural network for both forward and backward propagation.<br><br><b>ELI5:</b> Reading through an entire textbook once.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
Why must activation function be differentiable,"Differentiability is required to compute the gradients of the loss function with respect to the weights using the chain rule during backpropagation.<br><br><b>ELI5:</b> You need a smooth path to follow when adjusting settings, not a jagged one.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does batch size affect training,Larger batch sizes can speed up training by leveraging hardware parallelism but require more memory. Smaller batch sizes offer more frequent updates but can be slower.<br><br><b>ELI5:</b> Processing a whole classroom of students at once versus one student at a time.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is sigmoid activation function,"Squashes values between 0 and 1. Used in logistic regression.<br><br><b>ELI5:</b> Like a dimmer switch for a light, it can turn the brightness up or down but never fully off or on.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is tanh activation function,Squashes values between -1 and 1.<br><br><b>ELI5:</b> Similar to a dimmer switch but can also turn the light off completely or make it glow negatively.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is vanishing gradient problem,"Gradients become very small during backpropagation due to repeated multiplication of values less than 1, hindering weight updates and network training.<br><br><b>ELI5:</b> Imagine whispering a secret down a long line of people; by the end, the message is too faint to understand.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is exploding gradient problem,"Gradients become very large during backpropagation, causing noisy and unstable weight updates that prevent convergence.<br><br><b>ELI5:</b> Imagine shouting a secret down a long line of people; by the end, the message is distorted and too loud to be useful.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does sigmoid tanh cause vanishing gradients,"Their derivatives are often less than 1, and repeated multiplication through many layers shrinks the gradient exponentially.<br><br><b>ELI5:</b> Each step in a deep network is like a small discount; many small discounts add up to a huge loss of value.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is objective of activation functions,"Introduce non-linearity, enable gradient computation, and be fast to compute.<br><br><b>ELI5:</b> To add interesting twists and turns to a simple straight path, making the journey more complex and interesting.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does sigmoid tanh affect training deep networks,"They can lead to vanishing gradients, preventing effective weight updates and thus hindering the training of deep networks.<br><br><b>ELI5:</b> Trying to teach a very tall building to stand by only adjusting the bottom floor; the top floors never get the message.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
How does sigmoid tanh affect gradient computation,"They are differentiable, allowing for gradient calculation via the chain rule, but their derivatives can be small, leading to vanishing gradients.<br><br><b>ELI5:</b> They provide a way to measure how much each part of the network contributed to the error, but sometimes that measurement is too quiet to be useful.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_neural_networks
What is bias variance tradeoff,Bias is error from wrong assumptions in learning algorithm. Variance is error from sensitivity to small fluctuations in training set.<br><br><b>ELI5:</b> Imagine guessing a target. Bias is consistently missing the target in one direction. Variance is scattering your guesses all over the place.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does number of layers affect bias variance,Increasing layers increases weights leading to overfitting (low bias high variance).<br><br><b>ELI5:</b> A very complex machine can perfectly mimic a specific task but fail on slightly different ones.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How does logistic regression affect bias variance,Logistic regression has fewer weights leading to underfitting (high bias low variance).<br><br><b>ELI5:</b> A simple tool can perform a basic task reliably but cannot handle complex variations.,aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
How do regularizers affect variance,"Regularizers (L1 L2 Dropout) reduce variance by penalizing complex models and preventing overfitting.<br><br><b>ELI5:</b> Like a leash on a dog, regularization keeps the model from straying too far and getting lost.",aaic::module_3_foundations_of_natural_language_processing_and_machine_learning::classification_algorithms_in_various_situations
Deep learning limitations until 2010,"Vanishing gradients<br>Limited data (overfitting)<br>Limited compute power<br><br><b>ELI5:</b> Like trying to build a tall tower with weak bricks, not enough building materials, and a slow crane.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Factors enabling modern deep learning,"Abundant labeled data (Internet)<br>Increased compute power (Gaming GPUs)<br>Algorithmic advancements<br><br><b>ELI5:</b> Having lots of good building materials, a powerful crane, and better blueprints for building tall towers.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Classical ML vs Deep Learning approach,"Classical ML: Theory first, then experiments.<br>Deep Learning: Experiment first, then develop theory.<br><br><b>ELI5:</b> Classical ML is like a scientist proving a theorem before building. Deep Learning is like an engineer trying out designs and figuring out the math later.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
When to use ML vs Deep Learning,"ML is sufficient for simple data with linear relationships.<br>DL may perform better with complex data.<br>Experimentation (EDA) is required to decide.<br><br><b>ELI5:</b> For a straight road, a simple car is fine. For a winding mountain path, a powerful off-road vehicle is better. You need to check the road first.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
MLP as a complex mathematical function,MLPs are complex functions mapping features to outputs.<br>Weights are coefficients adjusted to find optimal values.<br>Weights capture patterns in training data for prediction.<br><br><b>ELI5:</b> An MLP is like a super-complicated recipe. The weights are the exact amounts of each ingredient. You adjust the amounts to make the best dish.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Overfitting in small datasets,Increasing hidden units<br>Increasing layers<br>Increasing epochs<br><br><b>ELI5:</b> Memorizing answers for a test instead of understanding the material. You get perfect scores on practice questions but fail the real exam.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How neural networks handle outliers,"Batch training<br>Robust cost functions<br>Regularization<br><br><b>ELI5:</b> Ignoring a few extremely loud people in a crowd to understand the general conversation, or using a special filter to smooth out noisy signals.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Overfitting in deep neural networks,"Regularization techniques like L1, L2, and Dropout.<br>Dropout is a preferred equivalent technique.<br><br><b>ELI5:</b> Like forcing a student to only use a limited set of study notes to prevent them from memorizing the entire textbook.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Dropout regularization analogy,"Similar to sampling a subset of features in Random Forests.<br>Base learners (trees) have high variance, low bias.<br>Dropout reduces variance.<br><br><b>ELI5:</b> Imagine a team where not everyone shows up for work each day. This forces the remaining team members to be more versatile and less reliant on specific individuals.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Dropout mechanism at training time,Each neuron is present with probability 'p'.<br>This randomly drops neurons.<br><br><b>ELI5:</b> Flipping a coin for each student in a class to decide if they attend the lecture that day.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Dropout mechanism at test time,"All neurons are present.<br>Weights of dropped neurons are scaled by 'p'.<br>Accounts for the probability of presence during training.<br><br><b>ELI5:</b> After the class, you adjust everyone's grade to reflect the fact that some people missed lectures, making the overall performance comparable.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
High dropout rate effect,"Low keep probability.<br>Leads to underfitting.<br>Acts as large regularization.<br><br><b>ELI5:</b> Most students are absent. The few who attend struggle to cover all the material, leading to poor overall understanding.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Low dropout rate effect,"High keep probability.<br>Leads to overfitting.<br>Acts as small regularization.<br><br><b>ELI5:</b> Almost all students attend. The class might focus too much on specific students' strengths, neglecting others.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Problem with classical neural network activations,Vanishing gradients leading to small weight updates.<br><br><b>ELI5:</b> Like trying to push a heavy door that only budges a tiny bit with each push.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
ReLU activation function,"f(Z) = max(0, Z)<br>f'(Z) = 0 if Z < 0, 1 if Z > 0<br>Avoids vanishing/exploding gradients but can have dead activations.<br><br><b>ELI5:</b> A gate that only lets positive signals pass through. If the signal is negative, it's blocked completely.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
ReLU derivative properties,"Derivative is 0 for negative inputs, 1 for positive inputs.<br>No vanishing or exploding gradients for positive inputs.<br>Risk of 'dead activation' for negative inputs.<br><br><b>ELI5:</b> For positive signals, it's a full 'on' switch. For negative signals, it's a complete 'off' switch, which can sometimes be too permanent.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
ReLU non-differentiability,Not differentiable at Z=0.<br>Smooth approximations like Softplus can be used.<br><br><b>ELI5:</b> Like a sharp corner on a road. You can't define the exact slope at the corner point itself.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Softplus activation function,"f(x) = log(1 + exp(x))<br>f'(x) = sigmoid(x)<br>A smooth approximation for ReLU.<br><br><b>ELI5:</b> A slightly rounded corner instead of a sharp one, making the road smoother to navigate mathematically.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
ReLU advantages,Faster convergence<br>Easy to compute derivatives<br><br><b>ELI5:</b> It's quick to calculate and speeds up the learning process.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
ReLU limitations,"Non-differentiable at zero<br>Unbounded output<br>Dying ReLU problem<br>Not zero-centered<br><br><b>ELI5:</b> It has a sharp point, can output infinitely large numbers, neurons can get stuck 'off', and its outputs aren't balanced around zero.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Leaky ReLU activation function,"f(z) = 0.01z for z <= 0, f(z) = z for z > 0<br>f'(z) = 0.01 for z <= 0, 1 for z > 0<br>Addresses dead ReLU neurons.<br><br><b>ELI5:</b> A gate that mostly blocks negative signals but lets a tiny trickle through, preventing it from getting completely stuck.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Why use non-linear activation functions,"Linear activations result in a single-layer network.<br>Non-linearity allows approximation of complex relationships.<br><br><b>ELI5:</b> Without non-linearity, a deep network is just a shallow one, unable to solve complicated problems like recognizing faces.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
When to use sigmoid activation,"Output layer for binary classification.<br><br><b>ELI5:</b> For a yes/no question, the final answer is squeezed between 0 and 1.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
When to use softmax activation,"Output layer for multi-class classification.<br><br><b>ELI5:</b> For choosing one option out of many (e.g., cat, dog, bird), it gives probabilities for each choice.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
When to use linear activation,"Output layer for regression problems.<br><br><b>ELI5:</b> For predicting a continuous value like house price, the output can be any number.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is weight initialization problem,"Initializing all weights to zero or same value causes all neurons to compute same thing and receive same gradient updates leading to symmetry and no training.<br><br><b>ELI5:</b> If everyone in a team does the exact same task, they all learn the same thing and don't improve individually.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does weight initialization affect neuron learning,"Asymmetric weight distributions cause neurons to learn different aspects of input data.<br><br><b>ELI5:</b> If team members have different skills, they can tackle different parts of a project.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What happens if weights are large negative for ReLU,"ReLU(z) becomes 0 for large negative inputs, leading to vanishing gradients.<br><br><b>ELI5:</b> Imagine a light switch that's always off if the input is too low.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is good weight initialization,"Weights should be small random values from a distribution with good variance, not all zero.<br><br><b>ELI5:</b> Start with slightly different ideas for everyone, not the same idea or no idea.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is uniform weight initialization,"Weights are sampled from a uniform distribution Unif[-1/sqrt(fan_in), 1/sqrt(fan_in)].<br><br><b>ELI5:</b> Picking a number randomly between two specific small values.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is Xavier Glorot initialization,"Normal: mean 0 variance 2/(fan_in + fan_out). Uniform: ~Unif[-sqrt(6)/sqrt(fan_in+fan_out), sqrt(6)/sqrt(fan_in+fan_out)]. Useful for sigmoid.<br><br><b>ELI5:</b> A smart way to pick starting numbers for weights that works well for certain types of activation functions.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is He initialization,"Normal: mean 0 variance 2/(fan_in). Uniform: ~Unif[-sqrt(6)/sqrt(fan_in), sqrt(6)/sqrt(fan_in)]. Useful for ReLU.<br><br><b>ELI5:</b> A smart way to pick starting numbers for weights that works well for ReLU activation.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is internal co-variance shift,The change in the distribution of network activations due to parameter changes during training.<br><br><b>ELI5:</b> The internal 'rules' of how signals flow through the network keep changing as it learns.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is the objective of batch normalization,"To reduce internal co-variance shift by normalizing activations within a mini-batch and then scaling and shifting them.<br><br><b>ELI5:</b> Keeping the signal levels consistent throughout the network, like adjusting volume controls.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does batch normalization help training,"It leads to faster convergence, allows larger learning rates, and acts as a weak regularizer.<br><br><b>ELI5:</b> Makes learning faster and more stable, like having better shock absorbers on a car.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How is batch normalization used on test set,"Uses the population mean and variance learned during training, not the current batch's statistics.<br><br><b>ELI5:</b> Using the average knowledge gained from all previous lessons, not just the current one.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is the objective of optimization,To minimize the loss function L(w).<br><br><b>ELI5:</b> Finding the lowest point in a valley.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What does zero gradient imply,"Zero gradient implies a local minimum, local maximum, or saddle point.<br><br><b>ELI5:</b> Being at a flat spot on a map, which could be the bottom of a valley, top of a hill, or a plateau.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Why can SGD get stuck,"SGD can get stuck at saddle points where the gradient is zero but it's not a minimum or maximum.<br><br><b>ELI5:</b> A hiker might get stuck on a flat, featureless plain where it's hard to tell which way is downhill.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is the difference between convex and non-convex functions,"Convex functions have a single minimum or maximum, while non-convex functions can have multiple local minima or maxima.<br><br><b>ELI5:</b> A bowl is convex (one bottom), a lumpy landscape is non-convex (many dips and peaks).",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Why do deep learning loss functions tend to be non-convex,"Non-convex loss functions are often used because they can lead to better performance and depend on architecture and desired outputs.<br><br><b>ELI5:</b> Complex problems often have many possible 'good' solutions, not just one perfect one.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does weight initialization affect deep learning,"In non-convex loss landscapes, weight initialization determines which minimum or maximum the model converges to.<br><br><b>ELI5:</b> Where you start your hike in a lumpy landscape determines which valley you end up in.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is contour plot,"Projection of 3D loss function points at same height onto a plane indicating maxima and minima locations.<br><br><b>ELI5:</b> Imagine looking at a mountain range from directly above, seeing the lines that connect all points at the same elevation.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is gradient descent,"Optimizer that computes gradient using all training points to update model parameters.<br><br><b>ELI5:</b> Finding the lowest point in a valley by always taking steps in the steepest downward direction, using information from the entire landscape.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is stochastic gradient descent,"Optimizer that computes gradient using a single random training point to update model parameters.<br><br><b>ELI5:</b> Finding the lowest point in a valley by taking steps in the steepest downward direction, but only looking at a tiny patch of the landscape for each step.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is mini batch stochastic gradient descent,"Optimizer that computes gradient using a small random sample of training points to update model parameters. Preferred for efficiency and approximation to GD.<br><br><b>ELI5:</b> Finding the lowest point in a valley by taking steps in the steepest downward direction, using information from a small group of nearby landscape points.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does simple SGD work,"Updates parameters using gradients computed from single data points. Prone to getting stuck at saddle points, local minima, and local maxima.<br><br><b>ELI5:</b> Walking blindfolded down a hill, you might get stuck in a small dip or on a flat spot instead of reaching the bottom.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is the objective of batch SGD with momentum,"To denoise SGD updates by incorporating an exponentially weighted average of past gradients, leading to smoother convergence and faster travel towards the optimum.<br><br><b>ELI5:</b> Imagine pushing a heavy ball down a hill. Momentum helps it keep rolling in the same direction, overcoming small bumps and accelerating towards the bottom.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does momentum affect SGD updates,"Momentum adds a fraction of the previous update vector to the current gradient step. This helps accelerate convergence in consistent directions and dampens oscillations.<br><br><b>ELI5:</b> If you're running downhill and keep going in the same direction, you build up speed. Momentum is like carrying that speed forward to make your next step even bigger.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does gamma affect momentum,"Gamma controls the weighting of past gradients in the exponential average. Higher gamma means past gradients have more influence, leading to smoother but potentially slower initial movement. Lower gamma makes updates more responsive to recent gradients.<br><br><b>ELI5:</b> Gamma is like how much you remember your past steps. A high gamma means you remember many past steps and average them out for a smoother path. A low gamma means you only care about your very last step.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does mini batch SGD relate to GD,"Mini-batch SGD approximates the gradient of the full dataset (GD) using a small sample. It's computationally efficient and provides a noisy but generally correct descent direction.<br><br><b>ELI5:</b> Instead of surveying the entire mountain (GD), you survey a small neighborhood (mini-batch SGD) to decide which way is downhill. It's faster but might not be the absolute steepest path.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Nesterov accelerated gradient update rule,"W_t = W_{t-1} - gamma * V_{t-1} - eta * g'
where g' = grad(L, W)[W_{t-1} - gamma * V_{t-1}]
<br><br><b>ELI5:</b> Imagine rolling a ball down a hill. Instead of just looking at the steepest downhill path from your current spot, you first take a small step in the direction you were already rolling, and *then* check the steepest downhill path from that new spot.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Nesterov accelerated gradient objective,Achieve faster convergence by taking a step in the momentum direction before computing the gradient.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does Nesterov accelerated gradient compare to SGD with momentum,"NAG ends up slightly closer to the optimum, leading to faster convergence.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Adagrad adaptive learning rate formula,"eta'_t = eta_t / (sqrt(alpha_t + epsilon))
where alpha_t = sum_{i=1 to t-1} (g_i^2)
g_i = grad(L,W)[W_{t-1}]
<br><br><b>ELI5:</b> Think of it like adjusting the volume knob for each instrument in a band. Instruments that have played a lot (large alpha_t) get their volume turned down (smaller eta'_t), while instruments that haven't played much get their volume turned up.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Adagrad disadvantage,"alpha_t grows with t, making the learning rate eta'_t very small, leading to negligible updates and no training.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Why different learning rates for sparse features in Adagrad,"Sparse features have small gradients. To make meaningful updates, they require larger learning rates, while dense features require smaller ones. Adagrad adapts this per-weight.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Adadelta update rule,"W_t = W_{t-1} - eta'_t * g_t
where eta'_t = eta / (sqrt(eda_t + epsilon))
eda_t = gamma * eda_{t-1} + (1 - gamma) * (g_{t-1}^2)
<br><br><b>ELI5:</b> Adadelta is like Adagrad but instead of summing up all past squared gradients (which can get huge), it uses a 'rolling average' of them. This prevents the learning rate from shrinking too much and stopping training.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
How does Adadelta avoid Adagrad's slow convergence,"Adadelta uses an exponential weighted average of squared gradients instead of a simple sum, preventing alpha from becoming excessively large.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec CBOW objective,Predict focus word given context words<br><br><b>ELI5:</b> Guessing the main subject of a sentence based on the surrounding words.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec Skip-Gram objective,Predict context words given focus word<br><br><b>ELI5:</b> Guessing the surrounding words based on the main subject of a sentence.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec CBOW structure,Input: one-hot encoded context words<br>Hidden layer: linear activation<br>Output layer: softmax to predict focus word<br><br><b>ELI5:</b> A system that takes several clues and tries to identify the main answer.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec Skip-Gram structure,Input: one-hot encoded focus word<br>Hidden layer: linear activation<br>Output layers: multiple softmax layers to predict context words<br><br><b>ELI5:</b> A system that takes a main answer and tries to predict all the clues around it.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec CBOW weights,"Number of weights is (K+1) * (N*V)<br>K: number of context words<br>N: hidden layer dimension<br>V: vocabulary size<br><br><b>ELI5:</b> The complexity of the system depends on how many clues you consider, the internal detail level, and the total number of possible words.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec Skip-Gram weights,"Number of weights is (K+1) * (N*V)<br>K: number of context words<br>N: hidden layer dimension<br>V: vocabulary size<br><br><b>ELI5:</b> Similar to CBOW, the system's complexity depends on the number of words it tries to predict, internal detail, and total word count.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec CBOW vs Skip-Gram performance,"CBOW performs well for frequent words<br>Skip-Gram performs well for infrequent words<br><br><b>ELI5:</b> CBOW is good at understanding common words, while Skip-Gram is better at capturing the meaning of rare words.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec training optimization Hierarchical Softmax,"Uses binary trees to predict a word<br><br><b>ELI5:</b> Instead of checking every possible word, it uses a decision tree to find the target word faster.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
Word2Vec training optimization Negative Sampling,"Updates weights for target word and a sample of non-target words<br><br><b>ELI5:</b> Instead of adjusting all possible word connections, it focuses on the correct word and a few incorrect ones.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_deep_multi_layer_perceptrons
What is Tensorflow Keras,Toolkits or libraries for coding Deep Learning.<br><br><b>ELI5:</b> Think of them as specialized toolboxes for building AI models.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is a Tensor,"A mathematical term for vectors.<br>1D vector: 1D tensor<br>2D vector: 2D tensor<br>3D vector: 3D tensor<br><br>Deep Learning involves tensor operations.<br><br><b>ELI5:</b> Tensors are like containers for numbers, from simple lists to multi-dimensional grids.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does Tensorflow work,"Core written in C/C++ for speed, with interfaces in Java, Python, JavaScript. Tensorflow Lite runs on Android.<br><br><b>ELI5:</b> The engine is super fast C++, but you can talk to it using easier languages like Python.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does Keras work,"A high-level Neural Network library, acting as a frontend for backends like Tensorflow. It simplifies development and deployment.<br><br><b>ELI5:</b> Keras is like a user-friendly remote control for the powerful Tensorflow engine.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does Keras relate to Tensorflow,"Keras is the frontend, Tensorflow is the backend. Other backends include Theano, PyTorch, Caffe, MXNET.<br><br><b>ELI5:</b> Keras is the easy-to-use interface, and Tensorflow is the powerful system that does the actual work behind the scenes.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is GPU for Deep Learning,"GPUs are suitable for Deep Learning due to their architecture of many slower cores, enabling parallelizable tasks like matrix operations.<br><br><b>ELI5:</b> GPUs are like a huge team of workers, each a bit slower than a single expert (CPU), but together they can do many simple tasks simultaneously, which is perfect for AI math.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does GPU architecture differ from CPU,"CPUs have fewer, faster cores. GPUs have many, slower cores with distributed VRAM for parallel processing.<br><br><b>ELI5:</b> A CPU is like a few highly skilled chefs in a kitchen, while a GPU is like a massive cafeteria with many cooks, each doing a specific part of a large meal preparation.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is Google Colaboratory,"A cloud computing platform provided by Google for Machine Learning education and research, similar to Jupyter Notebook.<br><br><b>ELI5:</b> It's like a free online computer specifically designed for AI experiments, giving you powerful tools without needing your own hardware.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
MLP architecture for MNIST,"784 input neurons (flattened image) -> 512 fully connected neurons -> 128 fully connected neurons -> 10 output neurons (softmax for classification).<br><br><b>ELI5:</b> Imagine a series of filters, each processing information from the previous one to make a final decision.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
MLP weight initialization,Weights are initialized to small random values. This is crucial to break symmetry and allow neurons to learn different features.<br><br><b>ELI5:</b> Like giving each student a slightly different starting point in a race so they don't all follow the exact same path.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is batch normalization,Normalizes layer inputs by re-centering and re-scaling. This stabilizes training and allows higher learning rates.<br><br><b>ELI5:</b> Imagine adjusting everyone's height to a standard average before a group photo so the camera focuses better.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is dropout,"Randomly sets a fraction of neuron outputs to zero during training. This prevents co-adaptation of neurons and acts as regularization.<br><br><b>ELI5:</b> Like randomly telling some students in a study group to be quiet for a bit, forcing others to learn more independently.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does dropout affect training,"Reduces overfitting by preventing complex co-adaptations between neurons. It effectively trains an ensemble of smaller networks.<br><br><b>ELI5:</b> Training many slightly different teams for a game, so the main team doesn't rely too much on just a few star players.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is a hyperparameter,"A parameter of a learning algorithm that is not learned from the data. It remains constant during training.<br><br><b>ELI5:</b> Think of it as a setting you choose before baking a cake, like the oven temperature, not the ingredients you mix.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is hyperparameter tuning,Procedure to find a tuple of hyperparameters that yields an optimal model minimizing a predefined loss function on test data.<br><br><b>ELI5:</b> It's like trying different oven temperatures and baking times to find the perfect setting for your cake recipe.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does hyperparameter tuning work,"Try different combinations of hyperparameters, evaluate on a validation set, and select the combination that yields the best performance.<br><br><b>ELI5:</b> You bake several cakes with slightly different oven settings and pick the one that tastes best.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does number of hidden layers affect model,"Slowly add layers until overfitting. Above 50-100 layers, consider transfer learning. Too few layers limit representational power.<br><br><b>ELI5:</b> Adding too many layers is like adding too many steps to a recipe; it can make the cake too complex or even ruin it. Too few steps might not fully cook it.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does number of neurons per layer affect model,"Input/output layer neurons are data-determined. For hidden layers, pick powers of 2. Use early stopping and regularization. Prefer more layers over more neurons.<br><br><b>ELI5:</b> Neurons are like ingredients. Too many of one ingredient can overpower the flavor. It's often better to add more distinct ingredients (layers) than just more of one.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does learning rate affect loss function,"Change LR from 10^-5 to 10^1 at intervals. Select LR just behind the minimum loss value. Tune LR after other hyperparameters.<br><br><b>ELI5:</b> Learning rate is how fast you adjust the oven temperature. Too fast, you burn the cake. Too slow, it's undercooked. Find the sweet spot.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How does Keras model connect to Scikit-learn tuning,"Use wrappers like KerasClassifier or KerasRegressor to make Keras models compatible with Scikit-learn's hyperparameter tuning algorithms (e.g., GridSearchCV).<br><br><b>ELI5:</b> It's like putting a special adapter on your Keras oven so it can be controlled by a standard Scikit-learn timer.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is the objective of hyperparameter tuning,To derive a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given test data.<br><br><b>ELI5:</b> To find the perfect recipe settings that make the best possible cake.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
How to tune activation function type,"Use Scikit-learn's GridSearchCV with a Keras model wrapped by KerasClassifier, specifying activation function choices in the parameter grid.<br><br><b>ELI5:</b> Trying different flavorings (like vanilla vs. chocolate) for your cake by systematically testing each one.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
What is the role of n_jobs in GridSearchCV,"When using CPU, n_jobs=-1 utilizes all available CPU cores for faster parallel tuning. Ignore for GPU.<br><br><b>ELI5:</b> n_jobs is like hiring more bakers to help test cake recipes simultaneously. More bakers (CPU cores) mean faster testing.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_tensorflow_and_keras
RGB image pixel values,Each pixel in an RGB image contains a tuple of three values representing Red Green and Blue intensities.<br><br><b>ELI5:</b> Like a tiny color dot made of three primary color amounts.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
3D tensor representation of image,An RGB image can be viewed as three 2D arrays (channels) stacked together forming a 3D tensor of size n x m x c where c is the number of channels.<br><br><b>ELI5:</b> Imagine stacking three flat pictures (red green blue) to make one thicker image.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Convolution filter for 3D image,A convolution filter for a 3D image must also be a 3D tensor with the same number of channels as the image.<br><br><b>ELI5:</b> The tool you use to scan the stacked pictures must have the same number of layers as the pictures themselves.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Convolution output size 3D image,Convolution of an n x n x c image with a k x k x c filter results in a 2D output array of size (n-k+1) x (n-k+1).<br><br><b>ELI5:</b> When you slide a small square scanner over a larger grid the resulting scanned area gets smaller by the scanner size minus one.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Convolutional layer objective,To learn kernel matrices that act as feature detectors for images through back-propagation.<br><br><b>ELI5:</b> Training a network to find specific patterns like edges or corners in an image by adjusting its scanning tools.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Multiple kernels in convolution layer,Multiple kernels are used in a convolution layer each learning to detect different features resulting in multiple 2D output arrays.<br><br><b>ELI5:</b> Using several different specialized scanners each looking for a unique type of detail in the image.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Convolution layer output shape,The output of a convolution layer with m kernels is an n x n x m array where m is the number of kernels.<br><br><b>ELI5:</b> The final output is a stack of grids where each grid shows the result of one specialized scanner.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Convolution layer transformation,A convolution layer transforms an input array to an output array by padding convolving and applying an activation function.<br><br><b>ELI5:</b> The process of preparing the image adding detail scanning it and then processing the scanned results.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Feature extraction in CNN layers,Multiple convolution layers extract increasingly complex features from pixels to parts to objects.<br><br><b>ELI5:</b> Like building blocks starting with simple lines then shapes then recognizable objects.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
MLP vs Convolutional layer weights,MLPs use weights while convolutional layers use kernels which are also learned weights.<br><br><b>ELI5:</b> Both use adjustable numbers but convolutional layers use them in a sliding window pattern for spatial data.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Pooling layer benefits,Pooling introduces location scale and rotational invariance reduces computation memory and parameters limiting overfitting.<br><br><b>ELI5:</b> Making the network less sensitive to exactly where an object is or how big it is while simplifying calculations.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Pooling layer destructiveness,Pooling is destructive as it subsamples information which can be undesirable if invariance is not needed.<br><br><b>ELI5:</b> It's like summarizing a book by only keeping the main plot points you lose some of the finer details.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Invariance vs Equivariance,Invariance means output is unchanged by input change equivariance means output changes proportionally to input change.<br><br><b>ELI5:</b> Invariance is like a photo filter that always makes the cat look the same no matter the pose. Equivariance is like a shadow that changes shape as the object moves.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Global Average Pooling,Computes the mean of an entire feature map producing a single scalar value.<br><br><b>ELI5:</b> Taking the average value across a whole grid to get one representative number.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Max pooling backpropagation gradient,"Gradient propagates only to the maximum value element. Gradient is 1 for the max element and 0 elsewhere.<br><br><b>ELI5:</b> Imagine a group of friends voting for their favorite ice cream flavor. When you ask them to tell you the most popular flavor, only the person who voted for the winning flavor gets to speak.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is receptive field,"The region of pixels in an input image to which a filter is applied at a specific instant during convolution.<br><br><b>ELI5:</b> When you look at a small part of a picture through a magnifying glass, that small part is your receptive field.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is effective receptive field,"The region of the input image that a filter in a deeper layer of a CNN ultimately influences. It's the cumulative receptive field across all preceding layers.<br><br><b>ELI5:</b> If you're looking at a large mural through a series of increasingly powerful magnifying glasses, the effective receptive field is the entire mural that your final view is based on, even though each glass only saw a small part.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
LeNet architecture depth,"LeNet is characterized by its small depth, meaning it has fewer convolutional and pooling layers compared to modern deep CNNs.<br><br><b>ELI5:</b> LeNet is like a short staircase, while modern CNNs are like very tall skyscrapers.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is ImageNet dataset,Benchmark dataset for deep learning algorithms<br><br><b>ELI5:</b> A massive photo album used to test how well AI can recognize objects.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is data augmentation,Generating new training images through transformations like translation scaling mirror<br><br><b>ELI5:</b> Making slightly different copies of your training photos so the AI learns to recognize the object even if it's tilted or zoomed.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How does data augmentation help CNNs,Introduces invariance to input image changes creates larger datasets from smaller ones<br><br><b>ELI5:</b> Helps the AI see the same object in different ways making it more robust and less likely to need tons of examples.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is flatten layer,Converts a multi-dimensional array into a 1D vector<br><br><b>ELI5:</b> Takes a grid of information and stretches it out into a single line.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is LeNet architecture,2 Conv layers 2 Mean Pool layers 3 FC layers 1000s trainable parameters Sigmoid activation<br><br><b>ELI5:</b> A simple blueprint for recognizing handwritten digits.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is AlexNet architecture,5 Conv layers 3 Pool layers 3 FC layers 10 million trainable parameters ReLU activation Dropout<br><br><b>ELI5:</b> A more complex blueprint for recognizing images trained on a large dataset.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is VGGNet architecture,Stacked small convolutional filters (2 Conv + 1 MaxPool)x2 then (3 Conv + 1 MaxPool)x3 followed by 3 FC layers and Softmax<br><br><b>ELI5:</b> A deep blueprint using many small building blocks to recognize images.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How does Softmax classifier work,Uses softmax function to convert raw class scores into normalized positive values that sum to one enabling cross-entropy loss calculation<br><br><b>ELI5:</b> Turns raw guesses into probabilities that add up to 100% for each category.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is Residual Network ResNet,"Networks with skip connections where output of a previous layer is also an input to a later layer<br><br><b>ELI5:</b> A network that can remember previous steps to help learn better, especially when very deep.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How do ResNet skip connections help,"They ensure that even if intermediate layers are not useful they are bypassed preventing performance degradation as network depth increases<br><br><b>ELI5:</b> Like a shortcut on a long road, it helps avoid getting stuck if a part of the path isn't helpful.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is the objective of ResNet skip connections,To allow adding more layers without increasing training or test error by enabling the network to learn identity mappings<br><br><b>ELI5:</b> To make networks deeper without making them worse at their job.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is required for ResNet skip connections,Input array dimensions must match for skip connections to be used<br><br><b>ELI5:</b> The information being skipped must fit the destination without needing to be reshaped.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is Inception Network,Uses multiple filters of different kernel sizes at a layer. Stacks filter outputs for a single output. Reduces computation with 1x1 convolutions before filters.<br><br><b>ELI5:</b> Like using different sized magnifying glasses on a picture and combining what you see.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How does 1x1 convolution help Inception Network,Reduces the number of computations by creating an intermediate bottleneck layer before applying larger filters.<br><br><b>ELI5:</b> Like summarizing a long paragraph before asking detailed questions about it.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What is Transfer Learning,Utilizing a pre-trained neural network's performance on a new dataset instead of training from scratch.<br><br><b>ELI5:</b> Learning to ride a bike helps you learn to ride a motorcycle faster.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Why use Transfer Learning,"Avoids lengthy training from scratch, especially with limited data. Leverages features learned from large datasets.<br><br><b>ELI5:</b> Using a pre-made recipe instead of inventing one from scratch.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
ConvNet as fixed feature extractor,"Remove the last layer of a pre-trained ConvNet. Use remaining layers to extract features (CNN codes). Train a new classifier on these features.<br><br><b>ELI5:</b> Using a skilled photographer's eye to identify objects, then teaching a new person to categorize them.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Fine-tuning the ConvNet,Retrain the classifier and also update weights of the pre-trained network via backpropagation. Can fine-tune all or some layers.<br><br><b>ELI5:</b> Taking a skilled chef's recipe and slightly adjusting ingredients or cooking times for a new dish.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How does dataset size affect fine-tuning,Small dataset: Avoid fine-tuning to prevent overfitting. Large dataset: Fine-tuning is more feasible and beneficial.<br><br><b>ELI5:</b> Practicing a few times vs. practicing hundreds of times before a test.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How does dataset similarity affect fine-tuning,"Similar dataset: Higher layers are likely relevant, fine-tuning or linear classifier on CNN codes works. Different dataset: Earlier layers are more generic, may need to train classifier from earlier activations.<br><br><b>ELI5:</b> Learning about apples helps with pears (similar), but less so with bananas (different).",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
What are CNN codes,"Feature vectors (activations) extracted from a hidden layer of a pre-trained ConvNet, typically before the final classifier.<br><br><b>ELI5:</b> A summary of key visual elements detected by a pre-trained image recognition system.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
How do learning rates differ in transfer learning,Use smaller learning rates for fine-tuning pre-trained weights. Use larger learning rates for new classifier weights.<br><br><b>ELI5:</b> Gently nudging an experienced driver vs. giving clear instructions to a new driver.,aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Face recognition system from scratch dataset size,"Approximately 1000 images per person are needed for a face recognition system built from scratch.
<br><br>
<b>ELI5:</b> Imagine needing a thousand different photos of your friend to be sure you can recognize them anywhere.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Face recognition system small dataset approach,"If the dataset size is less than 1000 images per person, use pre-trained models with fine-tuning on around 100 images per person.
<br><br>
<b>ELI5:</b> If you only have a few pictures of your friend, it's better to use a general 'face expert' (pre-trained model) and show them just a handful of your friend's pictures to specialize them.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Face recognition data augmentation purpose,"Data augmentation is used to collect data in various conditions to improve model robustness.
<br><br>
<b>ELI5:</b> Taking pictures of your friend in bright sun, shade, and at night helps you recognize them no matter the lighting.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Face recognition model training strategy,"For face recognition, use transfer learning, then categorical log loss (cross-entropy with Softmax), and consider cloud APIs for faster training.
<br><br>
<b>ELI5:</b> Start with a general 'face expert', teach them a bit more with your specific photos, use a smart way to check their answers, and get help from a super-fast computer if needed.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_convolutional_neural_nets
Simple RNN problem long term dependency,"Simple RNNs struggle with long-term dependencies because vanishing gradients cause later outputs to depend less on earlier inputs.<br><br><b>ELI5:</b> Imagine trying to remember the first word of a very long sentence. By the time you reach the end, the memory of that first word has faded.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
Deep RNN structure,"Deep RNNs are created by stacking recurrent layers one over the other, similar to how Deep MLPs and Deep CNNs are built.<br><br><b>ELI5:</b> It's like building a taller tower by adding more floors on top of each other.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
Deep RNN backpropagation,"In Deep RNNs, backpropagation occurs not only across time steps but also across the different layers (depth).<br><br><b>ELI5:</b> When you make a mistake in a multi-story building, you need to figure out which floor and which part of that floor the error originated from.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
RNN units per layer hyperparameter,"The number of units in each layer of an RNN is a hyperparameter to be tuned and is independent of the number of words in a sentence.<br><br><b>ELI5:</b> The number of rooms in a house is a design choice, not determined by how many people live there.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
RNN padding vs embedding,"Padding is used to make sentences of different lengths uniform for batch processing, while embedding represents each word as a dense vector.<br><br><b>ELI5:</b> Padding is like adding blank spaces to make all lines of text the same length on a page. Embedding is like giving each word a unique, rich description.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
Bidirectional RNN input consideration,"Bidirectional RNNs process input sequences in both forward and backward directions, allowing each time step to consider past and future context.<br><br><b>ELI5:</b> Reading a sentence and understanding its meaning by looking at the words that came before and the words that come after.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
Bidirectional RNN usage,"Bidirectional RNNs are useful in tasks like Machine Translation where understanding future words is crucial for correctly interpreting current words.<br><br><b>ELI5:</b> To translate a sentence accurately, you need to know what words are coming next to grasp the full meaning of the current word.",aaic::module_8_neural_networks_computer_vision_and_deep_learning::deep_learning_long_short_term_memory_lstms
What is Bag of Words,"Represents text as a multiset of its words ignoring grammar and word order but keeping multiplicity.
<br><br>
<b>ELI5:</b> Imagine counting how many times each word appears in a recipe, without caring about the order of ingredients.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Tokenization,"Breaking down text into smaller units called tokens, typically words or punctuation.
<br><br>
<b>ELI5:</b> Chopping a sentence into individual words like pieces of a puzzle.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Stemming,"Reducing words to their root or base form by removing suffixes, often crudely.
<br><br>
<b>ELI5:</b> Shortening words like 'running' and 'ran' to 'run'.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Lemmatization,"Reducing words to their dictionary form (lemma) considering context and morphology.
<br><br>
<b>ELI5:</b> Changing 'better' to 'good' because 'good' is the base dictionary word.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Stop-word removal,"Eliminating common words (like 'the', 'a', 'is') that have little semantic value.
<br><br>
<b>ELI5:</b> Removing filler words from a sentence so you focus on the important words.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is uni-gram,"A sequence of one token.
<br><br>
<b>ELI5:</b> A single word in a sentence.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is bi-gram,"A sequence of two adjacent tokens.
<br><br>
<b>ELI5:</b> A pair of consecutive words in a sentence like 'New York'.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is n-gram,"A contiguous sequence of n tokens.
<br><br>
<b>ELI5:</b> A group of 'n' consecutive words.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is tf-idf,"Term Frequency-Inverse Document Frequency measures word importance in a document relative to a corpus.
<br><br>
<b>ELI5:</b> A score that tells you how important a word is to a specific document, considering how common it is across all documents.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Why use log in IDF,"To dampen the effect of very large document frequencies, preventing them from dominating the score.
<br><br>
<b>ELI5:</b> To make sure words that appear in a huge number of documents don't unfairly get a super low score.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Word2Vec,"A technique to learn word embeddings representing words as dense vectors in a continuous vector space.
<br><br>
<b>ELI5:</b> Turning words into numbers (vectors) so computers can understand their meaning and relationships.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Avg-Word2Vec,"A document representation created by averaging the Word2Vec vectors of all words in the document.
<br><br>
<b>ELI5:</b> Getting the 'average meaning' of a document by averaging the meanings of its words.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is tf-idf weighted Word2Vec,"A document representation where Word2Vec vectors are weighted by their tf-idf scores before averaging.
<br><br>
<b>ELI5:</b> Giving more importance to the meaning of words that are significant to a document when calculating its overall meaning.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Multi-Layered Perceptron,"A feedforward artificial neural network with at least three layers: input, hidden, and output.
<br><br>
<b>ELI5:</b> A layered network of artificial neurons that processes information step-by-step.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to train a single-neuron model,"Adjusting weights and bias based on the error between predicted and actual output using an optimization rule.
<br><br>
<b>ELI5:</b> Teaching a single light switch to turn on or off correctly by adjusting how sensitive it is to different inputs.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to train MLP using Chain Rule,"Applying the chain rule of calculus to compute gradients of the loss function with respect to weights and biases layer by layer.
<br><br>
<b>ELI5:</b> Breaking down a complex calculation of error into smaller, manageable steps to update the network's settings.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to train MLP using Memoization,"Storing intermediate results of computations to avoid redundant calculations during gradient computation.
<br><br>
<b>ELI5:</b> Remembering the results of previous calculations so you don't have to do them again, speeding things up.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Backpropagation algorithm,"An algorithm for training neural networks by propagating the error gradient backward through the network.
<br><br>
<b>ELI5:</b> The process of learning from mistakes by figuring out how much each part of the network contributed to the error and adjusting it.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Vanishing Gradient problem,"Gradients become extremely small during backpropagation, hindering learning in early layers.
<br><br>
<b>ELI5:</b> When the 'lesson' from the error gets weaker and weaker as it travels back, so the early parts of the network don't learn much.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Exploding Gradient problem,"Gradients become extremely large during backpropagation, causing unstable learning.
<br><br>
<b>ELI5:</b> When the 'lesson' from the error becomes overwhelmingly large, making the network's adjustments chaotic and unstable.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Bias-Variance tradeoff in neural networks,"Balancing model complexity to minimize errors from underfitting (high bias) and overfitting (high variance).
<br><br>
<b>ELI5:</b> Finding a sweet spot where the model is not too simple (missing patterns) nor too complex (memorizing noise).",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is sampled softmax,"An approximation of the softmax function used for large output layers, sampling a subset of negative classes.
<br><br>
<b>ELI5:</b> Instead of checking every single possible answer, you only check a few likely wrong answers to speed up learning.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Why train RNN with SGD difficult,"Due to vanishing or exploding gradients, especially with long sequences, making weight updates ineffective or unstable.
<br><br>
<b>ELI5:</b> The learning signal gets lost or becomes too wild over long chains of information, making it hard to learn.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to tackle exploding gradients,"By using gradient clipping, which limits the magnitude of gradients to a maximum value.
<br><br>
<b>ELI5:</b> Setting a ceiling on how big the 'lesson' from an error can be, preventing it from becoming too extreme.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
How to tackle vanishing gradients,"By using architectures like LSTMs or GRUs that are designed to preserve gradients over long sequences.
<br><br>
<b>ELI5:</b> Using special memory units that help the learning signal travel further back in time without disappearing.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is LSTM memory cell,"A component in LSTM that allows selective forgetting and long-term memory storage through gates.
<br><br>
<b>ELI5:</b> A special box that can decide what old information to keep, what to throw away, and what new information to remember.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What regularization in LSTM,"Techniques like dropout applied to gates or recurrent connections to prevent overfitting.
<br><br>
<b>ELI5:</b> Randomly turning off some connections during training to prevent the LSTM from relying too much on specific paths.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Problem with sigmoid during backprop,"Its gradient is small (between 0 and 0.25), contributing to the vanishing gradient problem.
<br><br>
<b>ELI5:</b> The signal for learning gets very weak when passed through a sigmoid function, making it hard for earlier layers to learn.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is transfer learning,"Reusing a pre-trained model on a new, related task, often fine-tuning it.
<br><br>
<b>ELI5:</b> Using knowledge learned from one task (like recognizing cats) to help learn a new, similar task (like recognizing dogs) faster.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is back propagation through time,"Backpropagation applied to recurrent neural networks, unrolling the network over time steps.
<br><br>
<b>ELI5:</b> Teaching a network that remembers things by going back through its entire memory of past events to correct errors.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Difference between LSTM and GRU,"GRU has fewer gates (update and reset) and merges cell and hidden state, making it simpler and faster than LSTM.
<br><br>
<b>ELI5:</b> GRU is a simpler, lighter version of LSTM with fewer controls, but still good at remembering things.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
What is Gradient Clipping,"A technique to prevent exploding gradients by capping the norm of the gradient vector.
<br><br>
<b>ELI5:</b> If the error signal is too strong, you cut it down to a manageable size so it doesn't break the learning process.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
Adam RMSProp gradient clipping,"They adapt learning rates based on past gradients but do not inherently perform gradient clipping.
<br><br>
<b>ELI5:</b> These methods adjust step sizes based on past steps, but they don't automatically prevent steps from becoming too huge; you still need to clip them if they do.",aaic::module_2_data_science_exploratory_data_analysis_and_data_visualization::interview_questions_on_probability_and_statistics
